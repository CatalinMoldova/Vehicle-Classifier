{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        " \"\"\"\n",
        "# Abu Dhabi Traffic Flow Vehicle Behavior Classification Project\n",
        "\n",
        "## Research Objective:\n",
        "Train a CNN-LSTM hybrid model using actual vehicle labels from Aimsun data:\n",
        "- HDV Aggressive ‚Üí Aggressive behavior\n",
        "- HDV Conventional Gipps Model ‚Üí Normal behavior\n",
        "- HDV Cooperative ‚Üí Cooperative behavior\n",
        "- CAV ‚Üí Autonomous vehicle (excluded from training)\n",
        "\n",
        "## Model Validation:\n",
        "Compare CNN-LSTM predictions with actual vehicle labels to evaluate accuracy and F1 scores. Split the data into 80% training and 20% testing. Get the data from BOX using BOX API\n",
        "\"\"\"\n",
        "print(\"üöó Abu Dhabi Traffic Flow Vehicle Behavior Classification System\")\n",
        "print(\"üìä Training with Actual Vehicle Labels from Data Files\")\n"
      ],
      "metadata": {
        "id": "19aUYFyUEL3B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "CHeck the GPU"
      ],
      "metadata": {
        "id": "ayA6YEbYEfTA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HXIpGdvyEE_T",
        "outputId": "3219074d-3ced-4839-972c-8122c0d09d66"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tue Jul 15 08:51:31 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   68C    P0             31W /   70W |     250MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from psutil import virtual_memory\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "if ram_gb < 20:\n",
        "  print('Not using a high-RAM runtime')\n",
        "else:\n",
        "  print('You are using a high-RAM runtime!')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jaOFC8HaEJpm",
        "outputId": "879c468f-8dd9-482d-edce-ada9af328028"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your runtime has 54.8 gigabytes of available RAM\n",
            "\n",
            "You are using a high-RAM runtime!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "flowchart TD\n",
        "\n",
        "    A[Raw CSV Data] --> B[Data Parsing & Cleaning]\n",
        "    B --> C[Feature Extraction]\n",
        "    C --> D[Data Preparation]\n",
        "    D --> E[Model Training (CNN-LSTM)]\n",
        "    E --> F[Prediction & Continuous Learning]\n",
        "    F --> G[Behavior Output: Aggressive/Cooperative/Normal]\n"
      ],
      "metadata": {
        "id": "iGLsituzEhKB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade tensorflow\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jI05iFqnYAti",
        "outputId": "534ba5b2-a5eb-4be5-a323-48b2ed796070"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.19.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (5.29.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.14.1)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.73.1)\n",
            "Requirement already satisfied: tensorboard~=2.19.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.19.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.8.0)\n",
            "Requirement already satisfied: numpy<2.2.0,>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.0.2)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.14.0)\n",
            "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.5.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.1.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.7.9)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.8.2)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard~=2.19.0->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install boxsdk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lDM7amQRbYUK",
        "outputId": "a1341ed8-b701-4476-b873-4899890ae7d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: boxsdk in /usr/local/lib/python3.11/dist-packages (3.14.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from boxsdk) (25.3.0)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.11/dist-packages (from boxsdk) (2.4.0)\n",
            "Requirement already satisfied: requests<3,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from boxsdk) (2.32.3)\n",
            "Requirement already satisfied: requests-toolbelt>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from boxsdk) (1.0.0)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.11/dist-packages (from boxsdk) (2.9.0.post0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.4.3->boxsdk) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.4.3->boxsdk) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.4.3->boxsdk) (2025.7.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil->boxsdk) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install \"boxsdk[jwt]\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nob-Ws-EbYxC",
        "outputId": "8d7e2d35-b0f6-41ad-fdc8-7fcae677d21e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: boxsdk[jwt] in /usr/local/lib/python3.11/dist-packages (3.14.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from boxsdk[jwt]) (25.3.0)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.11/dist-packages (from boxsdk[jwt]) (2.4.0)\n",
            "Requirement already satisfied: requests<3,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from boxsdk[jwt]) (2.32.3)\n",
            "Requirement already satisfied: requests-toolbelt>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from boxsdk[jwt]) (1.0.0)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.11/dist-packages (from boxsdk[jwt]) (2.9.0.post0)\n",
            "Requirement already satisfied: pyjwt>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from boxsdk[jwt]) (2.10.1)\n",
            "Requirement already satisfied: cryptography>=3 in /usr/local/lib/python3.11/dist-packages (from boxsdk[jwt]) (43.0.3)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography>=3->boxsdk[jwt]) (1.17.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.4.3->boxsdk[jwt]) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.4.3->boxsdk[jwt]) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.4.3->boxsdk[jwt]) (2025.7.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil->boxsdk[jwt]) (1.17.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography>=3->boxsdk[jwt]) (2.22)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, json, numpy as np, pandas as pd, warnings\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model, load_model\n",
        "from tensorflow.keras.layers import (Input, LSTM, Conv1D, Dense, Dropout, BatchNormalization, Concatenate, GlobalMaxPooling1D, Masking)\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "import joblib\n",
        "import matplotlib.pyplot as plt\n",
        "import io\n",
        "import json\n",
        "import hashlib\n",
        "from tensorflow.keras.layers import Lambda\n",
        "from sklearn.metrics import f1_score, precision_score, recall_score, classification_report\n",
        "from boxsdk import Client, JWTAuth\n",
        "import pandas as pd\n",
        "import random\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "auth = JWTAuth.from_settings_file('key.json')\n",
        "client = Client(auth)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "print(\"‚úÖ All libraries imported successfully\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z4oDAKdSElhK",
        "outputId": "c772224a-d6d7-4abe-81f7-f3436bc194de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ All libraries imported successfully\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Processing functions"
      ],
      "metadata": {
        "id": "Cg4LXHlyF9tG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_array_string(array_str):\n",
        "    \"\"\"Parse array strings with error handling\"\"\"\n",
        "    try:\n",
        "        if pd.isna(array_str): return []\n",
        "        cleaned = str(array_str).replace('inf', '0').replace('-inf', '0').replace('nan', 'None')\n",
        "        return eval(cleaned)\n",
        "    except:\n",
        "        return []\n",
        "\n",
        "def interpolate_missing_values(sequence):\n",
        "    \"\"\"Handle missing values using linear interpolation\"\"\"\n",
        "    \"\"\"\n",
        "    Interpolate only None or np.nan values in the sequence.\n",
        "    Leave -inf and +inf unchanged.\n",
        "    \"\"\"\n",
        "    if not sequence:\n",
        "        return []\n",
        "    arr = np.array(sequence, dtype=float)\n",
        "    # Identify missing values (None or np.nan)\n",
        "    missing_mask = np.isnan(arr)\n",
        "    # Valid values are finite and not missing\n",
        "    valid_mask = ~missing_mask & np.isfinite(arr)\n",
        "    valid_indices = np.where(valid_mask)[0]\n",
        "\n",
        "    # If all are missing, return zeros except for -inf/+inf\n",
        "    if np.all(missing_mask | ~np.isfinite(arr)):\n",
        "        return [v if np.isinf(v) else 0.0 for v in arr]\n",
        "\n",
        "    # If only one valid value, fill missing with that value (but keep infs)\n",
        "    if len(valid_indices) == 1:\n",
        "        fill_value = arr[valid_indices[0]]\n",
        "        return [\n",
        "            v if not np.isnan(v) else fill_value\n",
        "            for v in arr\n",
        "        ]\n",
        "\n",
        "    # Standard case: interpolate only missing values, leave infs untouched\n",
        "    interp_values = np.copy(arr)\n",
        "    # Indices to interpolate: missing and not inf\n",
        "    interp_indices = np.where(missing_mask & ~np.isinf(arr))[0]\n",
        "    if len(valid_indices) >= 2 and len(interp_indices) > 0:\n",
        "        # Interpolate only where needed\n",
        "        interp_result = np.interp(\n",
        "            interp_indices, valid_indices, arr[valid_mask]\n",
        "        )\n",
        "        interp_values[interp_indices] = interp_result\n",
        "\n",
        "    # Convert to list and return\n",
        "    return interp_values.tolist()\n",
        "\n",
        "def parse_coordinate_string(coord_str):\n",
        "    \"\"\"Parse coordinate data with interpolation\"\"\"\n",
        "    try:\n",
        "        if pd.isna(coord_str): return []\n",
        "        cleaned_str = str(coord_str).replace('inf', '0').replace('-inf', '0').replace('nan', 'None')\n",
        "        coords = eval(cleaned_str)\n",
        "        x_coords = [c[0] for c in coords]\n",
        "        y_coords = [c[1] for c in coords]\n",
        "        return list(zip(\n",
        "            interpolate_missing_values(x_coords),\n",
        "            interpolate_missing_values(y_coords)\n",
        "        ))\n",
        "    except:\n",
        "        return []\n",
        "\n",
        "def calculate_lateral_speeds(front_coords, rear_coords):\n",
        "    \"\"\"Calculate lateral movement speeds\"\"\"\n",
        "    if len(front_coords) < 2: return []\n",
        "    lateral_speeds = []\n",
        "    for i in range(1, len(front_coords)):\n",
        "        try:\n",
        "            dx = front_coords[i][0] - front_coords[i-1][0]\n",
        "            dy = front_coords[i][1] - front_coords[i-1][1]\n",
        "            lateral_speeds.append(np.sqrt(dx**2 + dy**2))\n",
        "        except:\n",
        "            lateral_speeds.append(0)\n",
        "    return interpolate_missing_values(lateral_speeds)\n"
      ],
      "metadata": {
        "id": "BjE3mvMzFv6C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_vehicle_labels(df):\n",
        "    \"\"\"Extract labels directly from VehTypeName column\"\"\"\n",
        "    def map_vehicle_type(veh_type_name):\n",
        "        veh_type = str(veh_type_name).strip()\n",
        "        if 'CAV' in veh_type:\n",
        "            return 'autonomous'  # Will be excluded from training\n",
        "        elif 'Aggressive' in veh_type:\n",
        "            return 'aggressive'\n",
        "        elif 'Cooperative' in veh_type:\n",
        "            return 'cooperative'\n",
        "        elif 'Conventional' in veh_type or 'Gipps' in veh_type:\n",
        "            return 'normal'\n",
        "        else:\n",
        "            return 'normal'  # Default classification\n",
        "\n",
        "    df['behavior_label'] = df['VehTypeName'].apply(map_vehicle_type)\n",
        "    return df"
      ],
      "metadata": {
        "id": "U_mUvROnQ6K7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Feature extraction and if any record is more than half empty then it is discarded"
      ],
      "metadata": {
        "id": "f0_t3CzdF_9n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_speed_position_features(vehicle_row, min_sequence_length=5):\n",
        "    \"\"\"\n",
        "    Extract only speed and position sequences from vehicle data row.\n",
        "    Returns None if data is insufficient.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Parse speed sequence\n",
        "        speeds = parse_array_string(vehicle_row['Speeds'])\n",
        "        # Parse front coordinates (positions)\n",
        "        front_coords = parse_coordinate_string(vehicle_row['VehFrontCoords'])\n",
        "\n",
        "        # Validation: Check for at least 30 invalid values in any sequence\n",
        "        def count_invalid(seq):\n",
        "            return sum(\n",
        "                (v is None) or\n",
        "                (isinstance(v, float) and (np.isnan(v) or np.isinf(v)))\n",
        "                for v in seq\n",
        "            )\n",
        "\n",
        "        if count_invalid(speeds) >= 30 or count_invalid(front_coords) >= 30:\n",
        "            return None\n",
        "\n",
        "        # Clean sequences\n",
        "        speeds_clean = interpolate_missing_values(speeds)\n",
        "        positions_clean = front_coords  # Already interpolated by parse_coordinate_string\n",
        "\n",
        "        # Ensure consistent length\n",
        "        min_length = min(len(speeds_clean), len(positions_clean))\n",
        "        if min_length < min_sequence_length:\n",
        "            return None\n",
        "\n",
        "        # Truncate to same length\n",
        "        speeds_clean = speeds_clean[:min_length]\n",
        "        positions_clean = positions_clean[:min_length]\n",
        "\n",
        "        # Convert to arrays\n",
        "        speeds_arr = np.array(speeds_clean).reshape(-1, 1)\n",
        "        positions_arr = np.array(positions_clean).reshape(-1, 2)\n",
        "\n",
        "        return {\n",
        "            'speeds': speeds_arr,          # shape (N, 1)\n",
        "            'positions': positions_arr,    # shape (N, 2)\n",
        "            'sequence_length': min_length\n",
        "        }\n",
        "    except Exception as e:\n",
        "        print(f\"Feature extraction error: {e}\")\n",
        "        return None\n"
      ],
      "metadata": {
        "id": "ngSBzjcLF8rM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data reading function"
      ],
      "metadata": {
        "id": "47gNKBafEzBN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def read_aimsun_data(file_path, column_map=None):\n",
        "    \"\"\"Read CSV with flexible column mapping\"\"\"\n",
        "    DEFAULT_MAP = {\n",
        "        'vehicle_id': 'VehNr', 'timestep': 'Timestep', 'speed': 'Speeds',\n",
        "        'acceleration': 'Accelerations', 'front_coords': 'VehFrontCoords',\n",
        "        'rear_coords': 'VehRearCoords', 'vehicle_type': 'VehTypeName', 'length': 'Length'\n",
        "    }\n",
        "    col_map = column_map or DEFAULT_MAP\n",
        "    try:\n",
        "        df = pd.read_csv(file_path)\n",
        "        return df.rename(columns={v: k for k, v in col_map.items()})\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading {file_path}: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "def extract_vehicle_table(df, vehicle_id):\n",
        "    \"\"\"Extract time-series table for specific vehicle\"\"\"\n",
        "    vehicle_data = df[df['vehicle_id'] == vehicle_id].copy()\n",
        "    vehicle_data['front_coords'] = vehicle_data['front_coords'].apply(parse_coordinate_string)\n",
        "    vehicle_data['rear_coords'] = vehicle_data['rear_coords'].apply(parse_coordinate_string)\n",
        "    vehicle_data['position'] = vehicle_data.apply(\n",
        "        lambda x: np.mean([x['front_coords'], x['rear_coords']], axis=0), axis=1)\n",
        "    return vehicle_data[['timestep', 'speed', 'acceleration', 'position']]\n",
        "\n",
        "print(\"‚úÖ Data reading functions ready\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jTs8hiznEo0o",
        "outputId": "b4016c9a-e327-4f5f-f9ce-3233b32ec33a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Data reading functions ready\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def process_labeled_data_speed_position(csv_files, folder_path, max_files=None):\n",
        "    \"\"\"\n",
        "    Process data files using only speed and position features.\n",
        "    \"\"\"\n",
        "    all_features = []\n",
        "    all_labels = []\n",
        "    vehicle_details = []\n",
        "    processed_count = 0\n",
        "\n",
        "    files_to_process = csv_files[:max_files] if max_files else csv_files\n",
        "\n",
        "    for filename in files_to_process:\n",
        "        try:\n",
        "            print(f\"üìÑ Processing {filename}...\")\n",
        "            file_path = os.path.join(folder_path, filename)\n",
        "            df = pd.read_csv(file_path)\n",
        "\n",
        "            # Extract labels from VehTypeName\n",
        "            df = extract_vehicle_labels(df)\n",
        "\n",
        "            print(f\"   Labels in {filename}:\")\n",
        "            file_labels = df['behavior_label'].value_counts()\n",
        "            for label, count in file_labels.items():\n",
        "                print(f\"     {label}: {count}\")\n",
        "\n",
        "            for idx, row in df.iterrows():\n",
        "                if row['behavior_label'] == 'autonomous':\n",
        "                    continue\n",
        "\n",
        "                features = extract_speed_position_features(row)\n",
        "                if features is not None:\n",
        "                    all_features.append(features)\n",
        "                    all_labels.append(row['behavior_label'])\n",
        "                    vehicle_details.append({\n",
        "                        'VehNr': row['VehNr'],\n",
        "                        'VehTypeName': row['VehTypeName'],\n",
        "                        'actual_label': row['behavior_label'],\n",
        "                        'file': filename\n",
        "                    })\n",
        "                    processed_count += 1\n",
        "\n",
        "            print(f\"   ‚úÖ Extracted {processed_count} valid vehicles so far\")\n",
        "        except Exception as e:\n",
        "            print(f\"   ‚ùå Error: {e}\")\n",
        "\n",
        "    return all_features, all_labels, vehicle_details\n"
      ],
      "metadata": {
        "id": "ms4Ms3QEKjEa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_speed_position_data(features_list, max_sequence=60):\n",
        "    \"\"\"\n",
        "    Prepare padded speed and position arrays for model input.\n",
        "    Returns:\n",
        "        X_speed: (num_samples, max_sequence, 1)\n",
        "        X_pos: (num_samples, max_sequence, 2)\n",
        "    \"\"\"\n",
        "    X_speed = []\n",
        "    X_pos = []\n",
        "    for features in features_list:\n",
        "        speeds = features['speeds'][:max_sequence]\n",
        "        positions = features['positions'][:max_sequence]\n",
        "        seq_len = min(len(speeds), max_sequence)\n",
        "\n",
        "        # Pad if needed\n",
        "        speed_padded = np.zeros((max_sequence, 1))\n",
        "        pos_padded = np.zeros((max_sequence, 2))\n",
        "        speed_padded[:seq_len] = speeds[:seq_len]\n",
        "        pos_padded[:seq_len] = positions[:seq_len]\n",
        "\n",
        "        X_speed.append(speed_padded)\n",
        "        X_pos.append(pos_padded)\n",
        "    return np.array(X_speed), np.array(X_pos)\n"
      ],
      "metadata": {
        "id": "NjHLMrcCWEjs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_speed_position_model(sequence_length=60, num_classes=3):\n",
        "    \"\"\"\n",
        "    Build a model that receives speed and position sequences,\n",
        "    computes acceleration and lateral movement internally,\n",
        "    and predicts vehicle behavior.\n",
        "    \"\"\"\n",
        "    # Inputs\n",
        "    speed_input = Input(shape=(sequence_length, 1), name='speed')\n",
        "    position_input = Input(shape=(sequence_length, 2), name='position')\n",
        "\n",
        "    # Compute acceleration (difference between consecutive speeds)\n",
        "    def compute_accel(x):\n",
        "        return tf.concat([tf.zeros_like(x[:, :1, :]), x[:, 1:, :] - x[:, :-1, :]], axis=1)\n",
        "    accel = Lambda(compute_accel, name='acceleration')(speed_input)\n",
        "\n",
        "    # Compute lateral movement (Euclidean distance between consecutive positions)\n",
        "    def compute_lateral(pos):\n",
        "        delta = pos[:, 1:, :] - pos[:, :-1, :]\n",
        "        lateral = tf.concat([tf.zeros_like(delta[:, :1, :]), delta], axis=1)\n",
        "        return tf.norm(lateral, axis=-1, keepdims=True)\n",
        "    lateral = Lambda(compute_lateral, name='lateral')(position_input)\n",
        "\n",
        "    # Concatenate all features\n",
        "    features = Concatenate(axis=-1)([speed_input, accel, lateral])  # shape: (batch, seq, 3)\n",
        "\n",
        "    # Masking for variable-length sequences\n",
        "    masked = Masking(mask_value=0.0)(features)\n",
        "\n",
        "    # CNN branch\n",
        "    cnn = Conv1D(64, 3, activation='relu')(masked)\n",
        "    cnn = BatchNormalization()(cnn)\n",
        "    cnn = Dropout(0.3)(cnn)\n",
        "    cnn = Conv1D(128, 3, activation='relu')(cnn)\n",
        "    cnn = BatchNormalization()(cnn)\n",
        "    cnn_out = GlobalMaxPooling1D()(cnn)\n",
        "\n",
        "    # LSTM branch\n",
        "    lstm = LSTM(128, return_sequences=True, unroll=True, use_cudnn=False)(masked)\n",
        "    lstm = BatchNormalization()(lstm)\n",
        "    lstm_out = LSTM(64)(lstm)\n",
        "\n",
        "    # Feature fusion\n",
        "    combined = Concatenate()([cnn_out, lstm_out])\n",
        "    x = Dense(256, activation='relu')(combined)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Dropout(0.5)(x)\n",
        "    x = Dense(128, activation='relu')(x)\n",
        "    output = Dense(num_classes, activation='softmax')(x)\n",
        "\n",
        "    model = Model(inputs=[speed_input, position_input], outputs=output)\n",
        "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "    print(\"üß† Model Architecture:\")\n",
        "    model.summary()\n",
        "    return model"
      ],
      "metadata": {
        "id": "1m9Vc8MLLr4w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "graph TD\n",
        "\n",
        "    TS[Time Series Input<br/>60 x 3] --> Mask[Masking Layer]\n",
        "    Mask --> CNN1[Conv1D 64<br/>Kernel=3]\n",
        "    CNN1 --> BN1[BatchNorm]\n",
        "    BN1 --> Drop1[Dropout 0.3]\n",
        "    Drop1 --> CNN2[Conv1D 128<br/>Kernel=3]\n",
        "    CNN2 --> BN2[BatchNorm]\n",
        "    BN2 --> GMP[GlobalMaxPooling1D]\n",
        "    \n",
        "    Mask --> LSTM1[LSTM 128<br/>return_sequences=True]\n",
        "    LSTM1 --> BN3[BatchNorm]\n",
        "    BN3 --> LSTM2[LSTM 64]\n",
        "    \n",
        "    S[Static Features<br/>13 dims] --> Dense1[Dense 64]\n",
        "    Dense1 --> BN4[BatchNorm]\n",
        "    \n",
        "    GMP --> Concat[Concatenate]\n",
        "    LSTM2 --> Concat\n",
        "    BN4 --> Concat\n",
        "    \n",
        "    Concat --> Dense2[Dense 256]\n",
        "    Dense2 --> BN5[BatchNorm]\n",
        "    BN5 --> Drop2[Dropout 0.5]\n",
        "    Drop2 --> Dense3[Dense 128]\n",
        "    Dense3 --> Output[Softmax Output<br/>3 Classes]\n"
      ],
      "metadata": {
        "id": "AELTfAu-LuCN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calculate the F1 score"
      ],
      "metadata": {
        "id": "_KFMVLbeKpxk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def classify_vehicle_behavior_speed_position(vehicle_data, model, max_sequence=60):\n",
        "    \"\"\"\n",
        "    Classify vehicle behavior using only speed and position sequences.\n",
        "    \"\"\"\n",
        "    features = extract_speed_position_features(vehicle_data)\n",
        "    if not features:\n",
        "        return \"unknown\"\n",
        "\n",
        "    speeds = features['speeds'][:max_sequence]\n",
        "    positions = features['positions'][:max_sequence]\n",
        "    seq_len = min(len(speeds), max_sequence)\n",
        "\n",
        "    # Pad\n",
        "    speed_padded = np.zeros((1, max_sequence, 1))\n",
        "    pos_padded = np.zeros((1, max_sequence, 2))\n",
        "    speed_padded[0, :seq_len] = speeds\n",
        "    pos_padded[0, :seq_len] = positions\n",
        "\n",
        "    prediction = model.predict([speed_padded, pos_padded])\n",
        "    class_id = np.argmax(prediction)\n",
        "    return {0: 'aggressive', 1: 'cooperative', 2: 'normal'}.get(class_id, 'unknown')\n"
      ],
      "metadata": {
        "id": "Xv_LU62xLhDy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def analyze_f1_performance(comparisons, actual_labels, predictions):\n",
        "    \"\"\"Detailed F1 score analysis with visualizations\"\"\"\n",
        "\n",
        "    # Classification report\n",
        "    print(\"\\nüìã Detailed Classification Report:\")\n",
        "    print(\"=\" * 60)\n",
        "    print(classification_report(actual_labels, predictions))\n",
        "\n",
        "    # F1 scores by confidence ranges\n",
        "    df_comp = pd.DataFrame(comparisons)\n",
        "    confidence_ranges = [(0.0, 0.5), (0.5, 0.7), (0.7, 0.9), (0.9, 1.0)]\n",
        "\n",
        "    print(\"\\nüìä F1 Score by Confidence Ranges:\")\n",
        "    print(\"-\" * 50)\n",
        "    for low, high in confidence_ranges:\n",
        "        mask = (df_comp['confidence'] >= low) & (df_comp['confidence'] < high)\n",
        "        subset = df_comp[mask]\n",
        "        if len(subset) > 0:\n",
        "            subset_f1 = f1_score(subset['actual'], subset['predicted'], average='macro')\n",
        "            print(f\"Confidence {low:.1f}-{high:.1f}: F1={subset_f1:.3f} (n={len(subset)})\")\n",
        "\n",
        "    # Misclassified vehicles analysis\n",
        "    print(f\"\\n‚ùå Misclassified Vehicles Analysis:\")\n",
        "    misclassified = df_comp[df_comp['correct'] == False]\n",
        "    if not misclassified.empty:\n",
        "        print(f\"Total misclassified: {len(misclassified)}\")\n",
        "        for behavior in ['aggressive', 'cooperative', 'normal']:\n",
        "            behavior_miss = misclassified[misclassified['actual'] == behavior]\n",
        "            if len(behavior_miss) > 0:\n",
        "                print(f\"  {behavior}: {len(behavior_miss)} vehicles\")\n",
        "\n",
        "def plot_f1_results(comparisons, actual_labels, predictions):\n",
        "    \"\"\"Visualize F1 score results\"\"\"\n",
        "\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
        "\n",
        "    # 1. F1 scores by class\n",
        "    classes = sorted(set(actual_labels + predictions))\n",
        "    if len(classes) > 1:\n",
        "        f1_per_class = f1_score(actual_labels, predictions, average=None, labels=classes)\n",
        "        axes[0,0].bar(classes, f1_per_class, color=['#ff9999','#66b3ff','#99ff99'])\n",
        "        axes[0,0].set_title('F1 Score by Vehicle Behavior Class')\n",
        "        axes[0,0].set_ylabel('F1 Score')\n",
        "        axes[0,0].set_ylim(0, 1.1)\n",
        "\n",
        "    # 2. Prediction confidence distribution\n",
        "    df_comp = pd.DataFrame(comparisons)\n",
        "    axes[0,1].hist([df_comp[df_comp['correct']]['confidence'],\n",
        "                   df_comp[~df_comp['correct']]['confidence']],\n",
        "                  bins=20, alpha=0.7, label=['Correct', 'Incorrect'])\n",
        "    axes[0,1].set_title('Prediction Confidence Distribution')\n",
        "    axes[0,1].set_xlabel('Confidence')\n",
        "    axes[0,1].legend()\n",
        "\n",
        "    # 3. Confusion matrix\n",
        "    if len(classes) > 1:\n",
        "        from sklearn.metrics import confusion_matrix\n",
        "        import seaborn as sns\n",
        "        cm = confusion_matrix(actual_labels, predictions, labels=classes)\n",
        "        sns.heatmap(cm, annot=True, fmt='d', xticklabels=classes, yticklabels=classes, ax=axes[1,0])\n",
        "        axes[1,0].set_title('Confusion Matrix')\n",
        "        axes[1,0].set_xlabel('Predicted')\n",
        "        axes[1,0].set_ylabel('Actual')\n",
        "\n",
        "    # 4. F1 vs Accuracy comparison\n",
        "    accuracy = df_comp['correct'].mean()\n",
        "    f1_macro = f1_score(actual_labels, predictions, average='macro') if len(classes) > 1 else accuracy\n",
        "\n",
        "    metrics = ['Accuracy', 'F1-Macro']\n",
        "    values = [accuracy, f1_macro]\n",
        "    axes[1,1].bar(metrics, values, color=['lightblue', 'lightcoral'])\n",
        "    axes[1,1].set_title('Accuracy vs F1 Score')\n",
        "    axes[1,1].set_ylabel('Score')\n",
        "    axes[1,1].set_ylim(0, 1.1)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "print(\"‚úÖ F1 analysis and visualization functions ready\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rxl7qvJNLlyY",
        "outputId": "9176e782-9671-4669-c051-476bdea54526"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ F1 analysis and visualization functions ready\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def compare_predictions_with_labels_f1_speed_position(model, features_list, vehicle_details, label_encoder, max_sequence=60):\n",
        "    \"\"\"\n",
        "    Compare model predictions with actual vehicle labels using F1 score.\n",
        "    \"\"\"\n",
        "    predictions = []\n",
        "    actual_labels = []\n",
        "    comparisons = []\n",
        "\n",
        "    print(\"üîç Comparing predictions with actual labels (F1 Score Evaluation)...\")\n",
        "    print(\"-\" * 80)\n",
        "    print(f\"{'VehNr':<8} {'Actual Label':<15} {'Predicted':<15} {'Match':<8} {'Confidence':<12}\")\n",
        "    print(\"-\" * 80)\n",
        "\n",
        "    correct_predictions = 0\n",
        "\n",
        "    X_speed, X_pos = prepare_speed_position_data(features_list, max_sequence=max_sequence)\n",
        "\n",
        "    for i, vehicle_info in enumerate(vehicle_details):\n",
        "        # Predict\n",
        "        prediction_probs = model.predict([X_speed[i:i+1], X_pos[i:i+1]], verbose=0)\n",
        "        predicted_class_idx = np.argmax(prediction_probs)\n",
        "        predicted_label = label_encoder.inverse_transform([predicted_class_idx])[0]\n",
        "        confidence = np.max(prediction_probs)\n",
        "        actual_label = vehicle_info['actual_label']\n",
        "        is_correct = predicted_label == actual_label\n",
        "\n",
        "        if is_correct:\n",
        "            correct_predictions += 1\n",
        "\n",
        "        predictions.append(predicted_label)\n",
        "        actual_labels.append(actual_label)\n",
        "        comparison = {\n",
        "            'VehNr': vehicle_info['VehNr'],\n",
        "            'actual': actual_label,\n",
        "            'predicted': predicted_label,\n",
        "            'correct': is_correct,\n",
        "            'confidence': confidence,\n",
        "            'file': vehicle_info['file']\n",
        "        }\n",
        "        comparisons.append(comparison)\n",
        "\n",
        "        if i < 20:\n",
        "            match_symbol = \"‚úÖ\" if is_correct else \"‚ùå\"\n",
        "            print(f\"{vehicle_info['VehNr']:<8} {actual_label:<15} {predicted_label:<15} {match_symbol:<8} {confidence:.3f}\")\n",
        "\n",
        "    # Calculate F1 scores\n",
        "    try:\n",
        "        f1_macro = f1_score(actual_labels, predictions, average='macro')\n",
        "        f1_weighted = f1_score(actual_labels, predictions, average='weighted')\n",
        "        f1_per_class = f1_score(actual_labels, predictions, average=None, labels=label_encoder.classes_)\n",
        "        precision_macro = precision_score(actual_labels, predictions, average='macro')\n",
        "        recall_macro = recall_score(actual_labels, predictions, average='macro')\n",
        "        accuracy = correct_predictions / len(vehicle_details)\n",
        "\n",
        "        print(\"-\" * 80)\n",
        "        print(f\"üéØ Overall Accuracy: {accuracy:.3f} ({correct_predictions}/{len(vehicle_details)})\")\n",
        "        print(f\"üìä F1 Score (Macro): {f1_macro:.3f}\")\n",
        "        print(f\"üìä F1 Score (Weighted): {f1_weighted:.3f}\")\n",
        "        print(f\"üìä Precision (Macro): {precision_macro:.3f}\")\n",
        "        print(f\"üìä Recall (Macro): {recall_macro:.3f}\")\n",
        "\n",
        "        print(\"\\nüè∑Ô∏è F1 Score per Class:\")\n",
        "        for i, class_name in enumerate(label_encoder.classes_):\n",
        "            if i < len(f1_per_class):\n",
        "                print(f\"  {class_name:15s}: {f1_per_class[i]:.3f}\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è Could not calculate F1 scores: {e}\")\n",
        "        f1_macro = f1_weighted = 0.0\n",
        "\n",
        "    return comparisons, accuracy, f1_macro, f1_weighted\n"
      ],
      "metadata": {
        "id": "6kh-oGxmKpJ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import io\n",
        "import json\n",
        "import os\n",
        "import hashlib\n",
        "\n",
        "class ContinuousLearningSystem:\n",
        "\n",
        "    def __init__(self, model_path, scaler_path, le_path, data_path, processed_path, box_client=None, box_folder=None):\n",
        "        self.model_path = model_path\n",
        "        self.scaler_path = scaler_path\n",
        "        self.le_path = le_path\n",
        "        self.data_path = data_path\n",
        "        self.processed_path = processed_path\n",
        "        self.processed_hashes = self.load_processed_hashes()\n",
        "        self.box_client = box_client\n",
        "        self.box_folder = box_folder\n",
        "\n",
        "        # Load existing model artifacts if they exist\n",
        "        self.model = self.load_model()\n",
        "        self.scaler = self.load_scaler()\n",
        "        self.label_encoder = self.load_label_encoder()\n",
        "\n",
        "\n",
        "    def load_processed_hashes(self):\n",
        "        try:\n",
        "            if os.path.exists(self.processed_path):\n",
        "                with open(self.processed_path, 'r') as f:\n",
        "                    return set(json.load(f))\n",
        "            else:\n",
        "                return set()\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading processed hashes: {e}\")\n",
        "            return set()\n",
        "    def load_model(self):\n",
        "        try:\n",
        "            if os.path.exists(self.model_path):\n",
        "                print(f\"Loading existing model from {self.model_path}\")\n",
        "                return load_model(self.model_path)\n",
        "            else:\n",
        "                print(\"No existing model found. A new model will be built.\")\n",
        "                return None\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading model from {self.model_path}: {e}\")\n",
        "            return None\n",
        "\n",
        "    def load_scaler(self):\n",
        "        try:\n",
        "            if os.path.exists(self.scaler_path):\n",
        "                print(f\"Loading existing scaler from {self.scaler_path}\")\n",
        "                return joblib.load(self.scaler_path)\n",
        "            else:\n",
        "                print(\"No existing scaler found.\")\n",
        "                return None\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading scaler from {self.scaler_path}: {e}\")\n",
        "            return None\n",
        "\n",
        "    def load_label_encoder(self):\n",
        "        try:\n",
        "            if os.path.exists(self.le_path):\n",
        "                print(f\"Loading existing label encoder from {self.le_path}\")\n",
        "                return joblib.load(self.le_path)\n",
        "            else:\n",
        "                print(\"No existing label encoder found.\")\n",
        "                return None\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading label encoder from {self.le_path}: {e}\")\n",
        "            return None\n",
        "\n",
        "\n",
        "\n",
        "    def save_processed_hashes(self):\n",
        "        try:\n",
        "            with open(self.processed_path, 'w') as f:\n",
        "                json.dump(list(self.processed_hashes), f)\n",
        "        except Exception as e:\n",
        "            print(f\"Error saving processed hashes: {e}\")\n",
        "\n",
        "    def csv_stream_hash(self, csv_stream):\n",
        "        pos = csv_stream.tell()\n",
        "        csv_stream.seek(0)\n",
        "        content = csv_stream.read()\n",
        "        csv_stream.seek(pos)  # Restore position\n",
        "        return hashlib.sha256(content.encode('utf-8')).hexdigest()\n",
        "\n",
        "    def stream_all_csv_files_from_box(self, folder):\n",
        "        \"\"\"Yield (file_name, csv_stream) for every CSV file in Box folder and subfolders.\"\"\"\n",
        "        for item in folder.get_items(limit=2000):\n",
        "            if item.type == 'folder':\n",
        "                yield from self.stream_all_csv_files_from_box(self.box_client.folder(item.id))\n",
        "            elif item.type == 'file' and item.name.endswith('.csv'):\n",
        "                file_content = item.content()\n",
        "                csv_stream = io.StringIO(file_content.decode('utf-8'))\n",
        "                yield item.name, csv_stream\n",
        "\n",
        "    def get_new_box_files(self):\n",
        "        \"\"\"Get new CSV files from Box that have not been processed yet.\"\"\"\n",
        "        new_files = []\n",
        "        if not self.box_client or not self.box_folder:\n",
        "            print(\"Box client or folder not configured.\")\n",
        "            return new_files\n",
        "\n",
        "        for file_name, csv_stream in self.stream_all_csv_files_from_box(self.box_folder):\n",
        "            file_hash = self.csv_stream_hash(csv_stream)\n",
        "            if file_hash not in self.processed_hashes:\n",
        "                new_files.append((file_name, csv_stream, file_hash))\n",
        "        return new_files\n",
        "\n",
        "\n",
        "    def update_model(self):\n",
        "        model = self.model\n",
        "        scaler = self.scaler\n",
        "        label_encoder = self.label_encoder\n",
        "\n",
        "        new_files = self.get_new_box_files()\n",
        "\n",
        "        # Process training data\n",
        "        train_features, train_labels, _ = self.process_labeled_data_streams(train_files)\n",
        "        # Process testing data\n",
        "        test_features, test_labels, _ = self.process_labeled_data_streams(test_files)\n",
        "        if not new_files:\n",
        "            print(\"No new files to process from Box\")\n",
        "            return model, scaler, label_encoder\n",
        "\n",
        "        new_features, new_labels, _ = self.process_labeled_data_streams(new_files)\n",
        "\n",
        "        if not new_features:\n",
        "            print(\"No valid features extracted from new Box files\")\n",
        "            return model, scaler, label_encoder\n",
        "\n",
        "        if scaler is None:\n",
        "            print(\"Fitting new scaler on new data\")\n",
        "            static_features_list = [f['static'] for f in new_features]\n",
        "            scaler = StandardScaler()\n",
        "            scaler.fit(static_features_list)\n",
        "            self.scaler = scaler\n",
        "\n",
        "        if label_encoder is None:\n",
        "            print(\"Fitting new label encoder on new data\")\n",
        "            label_encoder = LabelEncoder()\n",
        "            label_encoder.fit(new_labels)\n",
        "            self.label_encoder = label_encoder\n",
        "\n",
        "        X_ts_new, X_static_new, y_new = self.prepare_new_data(new_features, new_labels, scaler, label_encoder)\n",
        "\n",
        "        if model is None:\n",
        "            print(\"Building a new model for initial training.\")\n",
        "            num_features = X_ts_new.shape[-1] if X_ts_new.shape[-1] > 0 else 3\n",
        "            static_feature_count = X_static_new.shape[-1] if X_static_new.shape[-1] > 0 else 13\n",
        "            num_classes = len(label_encoder.classes_) if label_encoder else 3\n",
        "            model = build_speed_position_model(sequence_length=X_ts_new.shape[1], num_features=num_features, static_feature_count=static_feature_count, num_classes=num_classes)\n",
        "            self.model = model\n",
        "\n",
        "        print(f\"Training model with {len(new_features)} new samples\")\n",
        "        self.model.fit([X_ts_new, X_static_new], y_new, epochs=3, batch_size=32, validation_split=0.1)\n",
        "\n",
        "        self.save_model()\n",
        "        self.save_scaler()\n",
        "        self.save_label_encoder()\n",
        "\n",
        "        for _, _, fhash in new_files:\n",
        "            self.processed_hashes.add(fhash)\n",
        "        self.save_processed_hashes()\n",
        "\n",
        "        print(f\"Updated model with {len(new_features)} new samples\")\n",
        "        return self.model, self.scaler, self.label_encoder\n",
        "\n",
        "    def process_labeled_data_streams(self, file_streams):\n",
        "        \"\"\"Process labeled data from a list of (file_name, csv_stream) tuples.\"\"\"\n",
        "        all_features = []\n",
        "        all_labels = []\n",
        "        vehicle_details = []\n",
        "        processed_count = 0\n",
        "\n",
        "        for file_name, csv_stream, _ in file_streams:\n",
        "            try:\n",
        "                print(f\"üìÑ Processing {file_name} from Box stream...\")\n",
        "                df = pd.read_csv(csv_stream)\n",
        "                df = extract_vehicle_labels(df)\n",
        "                print(f\"   Labels in {file_name}:\")\n",
        "                file_labels = df['behavior_label'].value_counts()\n",
        "                for label, count in file_labels.items():\n",
        "                    print(f\"     {label}: {count}\")\n",
        "                for idx, row in df.iterrows():\n",
        "                    if row['behavior_label'] == 'autonomous':\n",
        "                        continue\n",
        "                    features = extract_speed_position_features(row)\n",
        "                    if features is not None:\n",
        "                        all_features.append(features)\n",
        "                        all_labels.append(row['behavior_label'])\n",
        "                        vehicle_details.append({\n",
        "                            'VehNr': row['VehNr'],\n",
        "                            'VehTypeName': row['VehTypeName'],\n",
        "                            'actual_label': row['behavior_label'],\n",
        "                            'file': file_name\n",
        "                        })\n",
        "                        processed_count += 1\n",
        "                print(f\"   ‚úÖ Extracted {processed_count} valid vehicles so far\")\n",
        "            except Exception as e:\n",
        "                print(f\"   ‚ùå Error processing {file_name}: {e}\")\n",
        "\n",
        "        return all_features, all_labels, vehicle_details\n",
        "\n",
        "\n",
        "    def save_model(self):\n",
        "        try:\n",
        "            if self.model:\n",
        "                self.model.save(self.model_path)\n",
        "                print(f\"Model saved to {self.model_path}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error saving model to {self.model_path}: {e}\")\n",
        "\n",
        "    def save_scaler(self):\n",
        "        try:\n",
        "            if self.scaler:\n",
        "                joblib.dump(self.scaler, self.scaler_path)\n",
        "                print(f\"Scaler saved to {self.scaler_path}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error saving scaler to {self.scaler_path}: {e}\")\n",
        "\n",
        "    def save_label_encoder(self):\n",
        "        try:\n",
        "            if self.label_encoder:\n",
        "                joblib.dump(self.label_encoder, self.le_path)\n",
        "                print(f\"Label encoder saved to {self.le_path}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error saving label encoder to {self.le_path}: {e}\")\n",
        "\n",
        "    def load_all_data_from_box(self):\n",
        "        \"\"\"\n",
        "        Load all vehicle data from Box into a single list.\n",
        "        Each item: {'features': ..., 'label': ..., 'details': {...}}\n",
        "        \"\"\"\n",
        "        all_data = []\n",
        "        file_counter = 0\n",
        "\n",
        "        for file_name, csv_stream in self.stream_all_csv_files_from_box(self.box_folder):\n",
        "            fhash = self.csv_stream_hash(csv_stream)\n",
        "            if fhash in self.processed_hashes:\n",
        "                continue\n",
        "\n",
        "            try:\n",
        "                csv_stream.seek(0)\n",
        "                df = pd.read_csv(csv_stream)\n",
        "                df = extract_vehicle_labels(df)\n",
        "\n",
        "                for _, row in df.iterrows():\n",
        "                    if row['behavior_label'] == 'autonomous':\n",
        "                        continue\n",
        "                    features = extract_speed_position_features(row)\n",
        "                    if features is not None:\n",
        "                        record = {\n",
        "                            'features': features,\n",
        "                            'label': row['behavior_label'],\n",
        "                            'details': {\n",
        "                                'VehNr': row['VehNr'],\n",
        "                                'VehTypeName': row['VehTypeName'],\n",
        "                                'file': file_name\n",
        "                            }\n",
        "                        }\n",
        "                        all_data.append(record)\n",
        "                self.processed_hashes.add(fhash)\n",
        "                file_counter += 1\n",
        "                if file_counter == 50: return all_data\n",
        "                if file_counter % 10 == 0:\n",
        "                    print(f\"Files loaded: {file_counter}\")\n",
        "                    self.save_processed_hashes()\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing {file_name}: {e}\")\n",
        "\n",
        "        self.save_processed_hashes()\n",
        "        return all_data\n"
      ],
      "metadata": {
        "id": "9LhczUrMPr2g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Extract the data from the folder"
      ],
      "metadata": {
        "id": "nNTWaDAaJBFf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "DRIVE_PATH = '/content/drive/MyDrive/Test_Output'\n",
        "MODEL_PATH = f'{DRIVE_PATH}/traffic_model.keras'\n",
        "SCALER_PATH = f'{DRIVE_PATH}/scaler.joblib'\n",
        "LE_PATH = f'{DRIVE_PATH}/label_encoder.joblib'\n",
        "PROCESSED_PATH = f'{DRIVE_PATH}/processed.json'\n",
        "\n",
        "os.makedirs(DRIVE_PATH, exist_ok=True)\n",
        "\n",
        "print(\"‚úÖ All output and data will be stored in:\", DRIVE_PATH)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CEY68Ww-lOur",
        "outputId": "6ab36aba-adda-42a2-ca82-2c6e7f36ce1d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "‚úÖ All output and data will be stored in: /content/drive/MyDrive/Test_Output\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "main_folder_id = '326383492292'\n",
        "main_folder = client.folder(main_folder_id).get()\n",
        "\n",
        "# Find the \"Parsed Time Series Data\" subfolder\n",
        "parsed_folder = None\n",
        "for item in main_folder.get_items(limit=100):\n",
        "    if item.type == 'folder' and item.name == 'Parsed Time Series Data':\n",
        "        parsed_folder = client.folder(item.id)\n",
        "        break\n",
        "\n",
        "if not parsed_folder:\n",
        "    raise Exception(\"Parsed Time Series Data folder not found.\")"
      ],
      "metadata": {
        "id": "7iXMloUoJA1j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def stream_all_csv_files_from_box(folder, client):\n",
        "    \"\"\"\n",
        "    Recursively yield (file_name, csv_stream) for every CSV in all subfolders.\n",
        "    \"\"\"\n",
        "    for item in folder.get_items(limit = None):\n",
        "        if item.type == 'folder':\n",
        "            yield from stream_all_csv_files_from_box(client.folder(item.id), client)\n",
        "        elif item.type == 'file' and item.name.endswith('.csv'):\n",
        "            file_content = item.content()\n",
        "            csv_stream = io.StringIO(file_content.decode('utf-8'))\n",
        "            yield item.name, csv_stream\n"
      ],
      "metadata": {
        "id": "xjFZ1nKPJHrU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def split_train_test(all_data, train_ratio=0.8, seed=42):\n",
        "    \"\"\"\n",
        "    Split data into train and test sets by ratio.\n",
        "    \"\"\"\n",
        "    random.seed(seed)\n",
        "    random.shuffle(all_data)\n",
        "    split_idx = int(len(all_data) * train_ratio)\n",
        "    train_data = all_data[:split_idx]\n",
        "    test_data = all_data[split_idx:]\n",
        "    return train_data, test_data"
      ],
      "metadata": {
        "id": "K91AW5DxRJjH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def unpack_data(data_list):\n",
        "    features = [item['features'] for item in data_list]\n",
        "    labels = [item['label'] for item in data_list]\n",
        "    details = [item['details'] for item in data_list]\n",
        "    return features, labels, details\n"
      ],
      "metadata": {
        "id": "54jdBYCgRMRn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cl_system = ContinuousLearningSystem(MODEL_PATH, SCALER_PATH, LE_PATH, \"\", PROCESSED_PATH, client, parsed_folder)\n",
        "\n",
        "\n",
        "# 1. Load all data\n",
        "all_data = cl_system.load_all_data_from_box()\n",
        "# 2. Split into train and test sets\n",
        "train_data, test_data = split_train_test(all_data, train_ratio=0.8)\n",
        "\n",
        "# 3. Unpack features and labels\n",
        "train_features, train_labels, train_details = unpack_data(train_data)\n",
        "test_features, test_labels, test_details = unpack_data(test_data)\n",
        "\n",
        "\n",
        "max_sequence = 60\n",
        "\n",
        "# Extract raw sequences\n",
        "speed_sequences = [f['speeds'].squeeze() for f in train_features]\n",
        "position_sequences = [f['positions'] for f in train_features]\n",
        "\n",
        "# Pad speed sequences\n",
        "X_speed_train = pad_sequences(speed_sequences, maxlen=max_sequence, padding='post', dtype='float32')\n",
        "X_speed_train = np.expand_dims(X_speed_train, axis=-1)  # shape: (num_samples, max_sequence, 1)\n",
        "\n",
        "# Pad position sequences (pad each coordinate separately)\n",
        "X_pos_train_x = pad_sequences([p[:, 0] for p in position_sequences], maxlen=max_sequence, padding='post', dtype='float32')\n",
        "X_pos_train_y = pad_sequences([p[:, 1] for p in position_sequences], maxlen=max_sequence, padding='post', dtype='float32')\n",
        "X_pos_train = np.stack([X_pos_train_x, X_pos_train_y], axis=-1)  # shape: (num_samples, max_sequence, 2)\n",
        "\n",
        "\n",
        "if cl_system.label_encoder is None:\n",
        "    print(\"Fitting new label encoder on training data\")\n",
        "    cl_system.label_encoder = LabelEncoder().fit(train_labels)\n",
        "\n",
        "# 3. Prepare model input arrays\n",
        "#X_speed_train, X_pos_train = prepare_speed_position_data(train_features)\n",
        "y_train = cl_system.label_encoder.transform(train_labels)\n",
        "\n",
        "# 4. Build the model if needed\n",
        "if cl_system.model is None:\n",
        "    print(\"Building a new model for initial training.\")\n",
        "    sequence_length = X_speed_train.shape[1]\n",
        "    num_classes = len(cl_system.label_encoder.classes_)\n",
        "    cl_system.model = build_speed_position_model(\n",
        "        sequence_length=sequence_length,\n",
        "        num_classes=num_classes\n",
        "    )\n",
        "\n",
        "# 5. Train the model\n",
        "print(f\"Training model with {len(train_features)} samples\")\n",
        "cl_system.model.fit(\n",
        "    [X_speed_train, X_pos_train], y_train,\n",
        "    epochs=3, batch_size=32, validation_split=0.2\n",
        ")\n",
        "\n",
        "\n",
        "print(\"Saving model and artifacts to Google Drive...\")\n",
        "cl_system.save_model()\n",
        "cl_system.save_scaler()\n",
        "cl_system.save_label_encoder()\n",
        "print(\"‚úÖ Model, scaler, and label encoder updated and saved in:\", DRIVE_PATH)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "hfYft1cqRTA7",
        "outputId": "9abe963d-870e-4e1e-8c40-1075f911471f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No existing model found. A new model will be built.\n",
            "No existing scaler found.\n",
            "No existing label encoder found.\n",
            "Files loaded: 10\n",
            "Files loaded: 20\n",
            "Files loaded: 30\n",
            "Files loaded: 40\n",
            "Fitting new label encoder on training data\n",
            "Building a new model for initial training.\n",
            "üß† Model Architecture:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional_7\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_7\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "‚îè‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îì\n",
              "‚îÉ\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m‚îÉ\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m‚îÉ\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m‚îÉ\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m‚îÉ\n",
              "‚î°‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î©\n",
              "‚îÇ speed (\u001b[38;5;33mInputLayer\u001b[0m)  ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m60\u001b[0m, \u001b[38;5;34m1\u001b[0m)     ‚îÇ          \u001b[38;5;34m0\u001b[0m ‚îÇ -                 ‚îÇ\n",
              "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
              "‚îÇ position            ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m60\u001b[0m, \u001b[38;5;34m2\u001b[0m)     ‚îÇ          \u001b[38;5;34m0\u001b[0m ‚îÇ -                 ‚îÇ\n",
              "‚îÇ (\u001b[38;5;33mInputLayer\u001b[0m)        ‚îÇ                   ‚îÇ            ‚îÇ                   ‚îÇ\n",
              "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
              "‚îÇ acceleration        ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m60\u001b[0m, \u001b[38;5;34m1\u001b[0m)     ‚îÇ          \u001b[38;5;34m0\u001b[0m ‚îÇ speed[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       ‚îÇ\n",
              "‚îÇ (\u001b[38;5;33mLambda\u001b[0m)            ‚îÇ                   ‚îÇ            ‚îÇ                   ‚îÇ\n",
              "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
              "‚îÇ lateral (\u001b[38;5;33mLambda\u001b[0m)    ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m60\u001b[0m, \u001b[38;5;34m1\u001b[0m)     ‚îÇ          \u001b[38;5;34m0\u001b[0m ‚îÇ position[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    ‚îÇ\n",
              "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
              "‚îÇ concatenate_14      ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m60\u001b[0m, \u001b[38;5;34m3\u001b[0m)     ‚îÇ          \u001b[38;5;34m0\u001b[0m ‚îÇ speed[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],      ‚îÇ\n",
              "‚îÇ (\u001b[38;5;33mConcatenate\u001b[0m)       ‚îÇ                   ‚îÇ            ‚îÇ acceleration[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m‚Ä¶\u001b[0m ‚îÇ\n",
              "‚îÇ                     ‚îÇ                   ‚îÇ            ‚îÇ lateral[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     ‚îÇ\n",
              "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
              "‚îÇ masking_7 (\u001b[38;5;33mMasking\u001b[0m) ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m60\u001b[0m, \u001b[38;5;34m3\u001b[0m)     ‚îÇ          \u001b[38;5;34m0\u001b[0m ‚îÇ concatenate_14[\u001b[38;5;34m0\u001b[0m‚Ä¶ ‚îÇ\n",
              "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
              "‚îÇ conv1d_14 (\u001b[38;5;33mConv1D\u001b[0m)  ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m58\u001b[0m, \u001b[38;5;34m64\u001b[0m)    ‚îÇ        \u001b[38;5;34m640\u001b[0m ‚îÇ masking_7[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   ‚îÇ\n",
              "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
              "‚îÇ batch_normalizatio‚Ä¶ ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m58\u001b[0m, \u001b[38;5;34m64\u001b[0m)    ‚îÇ        \u001b[38;5;34m256\u001b[0m ‚îÇ conv1d_14[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   ‚îÇ\n",
              "‚îÇ (\u001b[38;5;33mBatchNormalizatio‚Ä¶\u001b[0m ‚îÇ                   ‚îÇ            ‚îÇ                   ‚îÇ\n",
              "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
              "‚îÇ not_equal_7         ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m60\u001b[0m, \u001b[38;5;34m3\u001b[0m)     ‚îÇ          \u001b[38;5;34m0\u001b[0m ‚îÇ concatenate_14[\u001b[38;5;34m0\u001b[0m‚Ä¶ ‚îÇ\n",
              "‚îÇ (\u001b[38;5;33mNotEqual\u001b[0m)          ‚îÇ                   ‚îÇ            ‚îÇ                   ‚îÇ\n",
              "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
              "‚îÇ dropout_14          ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m58\u001b[0m, \u001b[38;5;34m64\u001b[0m)    ‚îÇ          \u001b[38;5;34m0\u001b[0m ‚îÇ batch_normalizat‚Ä¶ ‚îÇ\n",
              "‚îÇ (\u001b[38;5;33mDropout\u001b[0m)           ‚îÇ                   ‚îÇ            ‚îÇ                   ‚îÇ\n",
              "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
              "‚îÇ any_7 (\u001b[38;5;33mAny\u001b[0m)         ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m60\u001b[0m)        ‚îÇ          \u001b[38;5;34m0\u001b[0m ‚îÇ not_equal_7[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] ‚îÇ\n",
              "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
              "‚îÇ conv1d_15 (\u001b[38;5;33mConv1D\u001b[0m)  ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m128\u001b[0m)   ‚îÇ     \u001b[38;5;34m24,704\u001b[0m ‚îÇ dropout_14[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  ‚îÇ\n",
              "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
              "‚îÇ lstm_14 (\u001b[38;5;33mLSTM\u001b[0m)      ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m60\u001b[0m, \u001b[38;5;34m128\u001b[0m)   ‚îÇ     \u001b[38;5;34m67,584\u001b[0m ‚îÇ masking_7[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],  ‚îÇ\n",
              "‚îÇ                     ‚îÇ                   ‚îÇ            ‚îÇ any_7[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       ‚îÇ\n",
              "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
              "‚îÇ batch_normalizatio‚Ä¶ ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m128\u001b[0m)   ‚îÇ        \u001b[38;5;34m512\u001b[0m ‚îÇ conv1d_15[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   ‚îÇ\n",
              "‚îÇ (\u001b[38;5;33mBatchNormalizatio‚Ä¶\u001b[0m ‚îÇ                   ‚îÇ            ‚îÇ                   ‚îÇ\n",
              "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
              "‚îÇ batch_normalizatio‚Ä¶ ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m60\u001b[0m, \u001b[38;5;34m128\u001b[0m)   ‚îÇ        \u001b[38;5;34m512\u001b[0m ‚îÇ lstm_14[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],    ‚îÇ\n",
              "‚îÇ (\u001b[38;5;33mBatchNormalizatio‚Ä¶\u001b[0m ‚îÇ                   ‚îÇ            ‚îÇ any_7[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       ‚îÇ\n",
              "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
              "‚îÇ global_max_pooling‚Ä¶ ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       ‚îÇ          \u001b[38;5;34m0\u001b[0m ‚îÇ batch_normalizat‚Ä¶ ‚îÇ\n",
              "‚îÇ (\u001b[38;5;33mGlobalMaxPooling1‚Ä¶\u001b[0m ‚îÇ                   ‚îÇ            ‚îÇ                   ‚îÇ\n",
              "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
              "‚îÇ lstm_15 (\u001b[38;5;33mLSTM\u001b[0m)      ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        ‚îÇ     \u001b[38;5;34m49,408\u001b[0m ‚îÇ batch_normalizat‚Ä¶ ‚îÇ\n",
              "‚îÇ                     ‚îÇ                   ‚îÇ            ‚îÇ any_7[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       ‚îÇ\n",
              "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
              "‚îÇ concatenate_15      ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m192\u001b[0m)       ‚îÇ          \u001b[38;5;34m0\u001b[0m ‚îÇ global_max_pooli‚Ä¶ ‚îÇ\n",
              "‚îÇ (\u001b[38;5;33mConcatenate\u001b[0m)       ‚îÇ                   ‚îÇ            ‚îÇ lstm_15[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     ‚îÇ\n",
              "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
              "‚îÇ dense_21 (\u001b[38;5;33mDense\u001b[0m)    ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       ‚îÇ     \u001b[38;5;34m49,408\u001b[0m ‚îÇ concatenate_15[\u001b[38;5;34m0\u001b[0m‚Ä¶ ‚îÇ\n",
              "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
              "‚îÇ batch_normalizatio‚Ä¶ ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       ‚îÇ      \u001b[38;5;34m1,024\u001b[0m ‚îÇ dense_21[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    ‚îÇ\n",
              "‚îÇ (\u001b[38;5;33mBatchNormalizatio‚Ä¶\u001b[0m ‚îÇ                   ‚îÇ            ‚îÇ                   ‚îÇ\n",
              "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
              "‚îÇ dropout_15          ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       ‚îÇ          \u001b[38;5;34m0\u001b[0m ‚îÇ batch_normalizat‚Ä¶ ‚îÇ\n",
              "‚îÇ (\u001b[38;5;33mDropout\u001b[0m)           ‚îÇ                   ‚îÇ            ‚îÇ                   ‚îÇ\n",
              "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
              "‚îÇ dense_22 (\u001b[38;5;33mDense\u001b[0m)    ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       ‚îÇ     \u001b[38;5;34m32,896\u001b[0m ‚îÇ dropout_15[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  ‚îÇ\n",
              "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
              "‚îÇ dense_23 (\u001b[38;5;33mDense\u001b[0m)    ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         ‚îÇ        \u001b[38;5;34m129\u001b[0m ‚îÇ dense_22[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    ‚îÇ\n",
              "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">‚îè‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îì\n",
              "‚îÉ<span style=\"font-weight: bold\"> Layer (type)        </span>‚îÉ<span style=\"font-weight: bold\"> Output Shape      </span>‚îÉ<span style=\"font-weight: bold\">    Param # </span>‚îÉ<span style=\"font-weight: bold\"> Connected to      </span>‚îÉ\n",
              "‚î°‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î©\n",
              "‚îÇ speed (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)  ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">60</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)     ‚îÇ          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> ‚îÇ -                 ‚îÇ\n",
              "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
              "‚îÇ position            ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">60</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)     ‚îÇ          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> ‚îÇ -                 ‚îÇ\n",
              "‚îÇ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        ‚îÇ                   ‚îÇ            ‚îÇ                   ‚îÇ\n",
              "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
              "‚îÇ acceleration        ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">60</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)     ‚îÇ          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> ‚îÇ speed[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       ‚îÇ\n",
              "‚îÇ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)            ‚îÇ                   ‚îÇ            ‚îÇ                   ‚îÇ\n",
              "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
              "‚îÇ lateral (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)    ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">60</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)     ‚îÇ          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> ‚îÇ position[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    ‚îÇ\n",
              "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
              "‚îÇ concatenate_14      ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">60</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)     ‚îÇ          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> ‚îÇ speed[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],      ‚îÇ\n",
              "‚îÇ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       ‚îÇ                   ‚îÇ            ‚îÇ acceleration[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">‚Ä¶</span> ‚îÇ\n",
              "‚îÇ                     ‚îÇ                   ‚îÇ            ‚îÇ lateral[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     ‚îÇ\n",
              "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
              "‚îÇ masking_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Masking</span>) ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">60</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)     ‚îÇ          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> ‚îÇ concatenate_14[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>‚Ä¶ ‚îÇ\n",
              "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
              "‚îÇ conv1d_14 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)  ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">58</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)    ‚îÇ        <span style=\"color: #00af00; text-decoration-color: #00af00\">640</span> ‚îÇ masking_7[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   ‚îÇ\n",
              "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
              "‚îÇ batch_normalizatio‚Ä¶ ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">58</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)    ‚îÇ        <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> ‚îÇ conv1d_14[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   ‚îÇ\n",
              "‚îÇ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio‚Ä¶</span> ‚îÇ                   ‚îÇ            ‚îÇ                   ‚îÇ\n",
              "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
              "‚îÇ not_equal_7         ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">60</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)     ‚îÇ          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> ‚îÇ concatenate_14[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>‚Ä¶ ‚îÇ\n",
              "‚îÇ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">NotEqual</span>)          ‚îÇ                   ‚îÇ            ‚îÇ                   ‚îÇ\n",
              "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
              "‚îÇ dropout_14          ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">58</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)    ‚îÇ          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> ‚îÇ batch_normalizat‚Ä¶ ‚îÇ\n",
              "‚îÇ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           ‚îÇ                   ‚îÇ            ‚îÇ                   ‚îÇ\n",
              "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
              "‚îÇ any_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Any</span>)         ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">60</span>)        ‚îÇ          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> ‚îÇ not_equal_7[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] ‚îÇ\n",
              "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
              "‚îÇ conv1d_15 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)  ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)   ‚îÇ     <span style=\"color: #00af00; text-decoration-color: #00af00\">24,704</span> ‚îÇ dropout_14[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  ‚îÇ\n",
              "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
              "‚îÇ lstm_14 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)      ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">60</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)   ‚îÇ     <span style=\"color: #00af00; text-decoration-color: #00af00\">67,584</span> ‚îÇ masking_7[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],  ‚îÇ\n",
              "‚îÇ                     ‚îÇ                   ‚îÇ            ‚îÇ any_7[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       ‚îÇ\n",
              "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
              "‚îÇ batch_normalizatio‚Ä¶ ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)   ‚îÇ        <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> ‚îÇ conv1d_15[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   ‚îÇ\n",
              "‚îÇ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio‚Ä¶</span> ‚îÇ                   ‚îÇ            ‚îÇ                   ‚îÇ\n",
              "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
              "‚îÇ batch_normalizatio‚Ä¶ ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">60</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)   ‚îÇ        <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> ‚îÇ lstm_14[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],    ‚îÇ\n",
              "‚îÇ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio‚Ä¶</span> ‚îÇ                   ‚îÇ            ‚îÇ any_7[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       ‚îÇ\n",
              "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
              "‚îÇ global_max_pooling‚Ä¶ ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       ‚îÇ          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> ‚îÇ batch_normalizat‚Ä¶ ‚îÇ\n",
              "‚îÇ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalMaxPooling1‚Ä¶</span> ‚îÇ                   ‚îÇ            ‚îÇ                   ‚îÇ\n",
              "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
              "‚îÇ lstm_15 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)      ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        ‚îÇ     <span style=\"color: #00af00; text-decoration-color: #00af00\">49,408</span> ‚îÇ batch_normalizat‚Ä¶ ‚îÇ\n",
              "‚îÇ                     ‚îÇ                   ‚îÇ            ‚îÇ any_7[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       ‚îÇ\n",
              "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
              "‚îÇ concatenate_15      ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>)       ‚îÇ          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> ‚îÇ global_max_pooli‚Ä¶ ‚îÇ\n",
              "‚îÇ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       ‚îÇ                   ‚îÇ            ‚îÇ lstm_15[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     ‚îÇ\n",
              "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
              "‚îÇ dense_21 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)    ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       ‚îÇ     <span style=\"color: #00af00; text-decoration-color: #00af00\">49,408</span> ‚îÇ concatenate_15[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>‚Ä¶ ‚îÇ\n",
              "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
              "‚îÇ batch_normalizatio‚Ä¶ ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       ‚îÇ      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> ‚îÇ dense_21[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    ‚îÇ\n",
              "‚îÇ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio‚Ä¶</span> ‚îÇ                   ‚îÇ            ‚îÇ                   ‚îÇ\n",
              "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
              "‚îÇ dropout_15          ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       ‚îÇ          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> ‚îÇ batch_normalizat‚Ä¶ ‚îÇ\n",
              "‚îÇ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           ‚îÇ                   ‚îÇ            ‚îÇ                   ‚îÇ\n",
              "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
              "‚îÇ dense_22 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)    ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       ‚îÇ     <span style=\"color: #00af00; text-decoration-color: #00af00\">32,896</span> ‚îÇ dropout_15[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  ‚îÇ\n",
              "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
              "‚îÇ dense_23 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)    ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         ‚îÇ        <span style=\"color: #00af00; text-decoration-color: #00af00\">129</span> ‚îÇ dense_22[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    ‚îÇ\n",
              "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m227,073\u001b[0m (887.00 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">227,073</span> (887.00 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m225,921\u001b[0m (882.50 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">225,921</span> (882.50 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m1,152\u001b[0m (4.50 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,152</span> (4.50 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training model with 13172 samples\n",
            "Epoch 1/3\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "InvalidArgumentError",
          "evalue": "Graph execution error:\n\nDetected at node functional_7_1/lstm_15_1/Assert/Assert defined at (most recent call last):\n  File \"<frozen runpy>\", line 198, in _run_module_as_main\n\n  File \"<frozen runpy>\", line 88, in _run_code\n\n  File \"/usr/local/lib/python3.11/dist-packages/colab_kernel_launcher.py\", line 37, in <module>\n\n  File \"/usr/local/lib/python3.11/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelapp.py\", line 712, in start\n\n  File \"/usr/local/lib/python3.11/dist-packages/tornado/platform/asyncio.py\", line 205, in start\n\n  File \"/usr/lib/python3.11/asyncio/base_events.py\", line 608, in run_forever\n\n  File \"/usr/lib/python3.11/asyncio/base_events.py\", line 1936, in _run_once\n\n  File \"/usr/lib/python3.11/asyncio/events.py\", line 84, in _run\n\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 510, in dispatch_queue\n\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 499, in process_one\n\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 406, in dispatch_shell\n\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 730, in execute_request\n\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/ipkernel.py\", line 383, in do_execute\n\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/zmqshell.py\", line 528, in run_cell\n\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n\n  File \"/tmp/ipython-input-96-2323280157.py\", line 50, in <cell line: 0>\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\", line 371, in fit\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\", line 219, in function\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\", line 132, in multi_step_on_iterator\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\", line 113, in one_step_on_data\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\", line 57, in train_step\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/layers/layer.py\", line 908, in __call__\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/ops/operation.py\", line 46, in __call__\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\", line 156, in error_handler\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/models/functional.py\", line 182, in call\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/ops/function.py\", line 171, in _run_through_graph\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/models/functional.py\", line 637, in call\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/layers/layer.py\", line 908, in __call__\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/ops/operation.py\", line 46, in __call__\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\", line 156, in error_handler\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/layers/rnn/lstm.py\", line 584, in call\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/layers/rnn/rnn.py\", line 402, in call\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/layers/rnn/lstm.py\", line 551, in inner_loop\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/rnn.py\", line 841, in lstm\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/rnn.py\", line 874, in _cudnn_lstm\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/rnn.py\", line 557, in _assert_valid_mask\n\nassertion failed: [You are passing a RNN mask that does not correspond to right-padded sequences, while using cuDNN, which is not supported. With cuDNN, RNN masks can only be used for right-padding, e.g. `[[True, True, False, False]]` would be a valid mask, but any mask that isn\\'t just contiguous `True`\\'s on the left and contiguous `False`\\'s on the right would be invalid. You can pass `use_cudnn=False` to your RNN layer to stop using cuDNN (this may be slower).]\n\t [[{{node functional_7_1/lstm_15_1/Assert/Assert}}]] [Op:__inference_multi_step_on_iterator_182392]",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-96-2323280157.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;31m# 5. Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Training model with {len(train_features)} samples\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m cl_system.model.fit(\n\u001b[0m\u001b[1;32m     51\u001b[0m     \u001b[0;34m[\u001b[0m\u001b[0mX_speed_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_pos_train\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;31m# `keras.config.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m       \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\" name: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m   \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m     \u001b[0mkeras_symbolic_tensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m_is_keras_symbolic_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mkeras_symbolic_tensors\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m: Graph execution error:\n\nDetected at node functional_7_1/lstm_15_1/Assert/Assert defined at (most recent call last):\n  File \"<frozen runpy>\", line 198, in _run_module_as_main\n\n  File \"<frozen runpy>\", line 88, in _run_code\n\n  File \"/usr/local/lib/python3.11/dist-packages/colab_kernel_launcher.py\", line 37, in <module>\n\n  File \"/usr/local/lib/python3.11/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelapp.py\", line 712, in start\n\n  File \"/usr/local/lib/python3.11/dist-packages/tornado/platform/asyncio.py\", line 205, in start\n\n  File \"/usr/lib/python3.11/asyncio/base_events.py\", line 608, in run_forever\n\n  File \"/usr/lib/python3.11/asyncio/base_events.py\", line 1936, in _run_once\n\n  File \"/usr/lib/python3.11/asyncio/events.py\", line 84, in _run\n\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 510, in dispatch_queue\n\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 499, in process_one\n\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 406, in dispatch_shell\n\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 730, in execute_request\n\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/ipkernel.py\", line 383, in do_execute\n\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/zmqshell.py\", line 528, in run_cell\n\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n\n  File \"/tmp/ipython-input-96-2323280157.py\", line 50, in <cell line: 0>\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\", line 371, in fit\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\", line 219, in function\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\", line 132, in multi_step_on_iterator\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\", line 113, in one_step_on_data\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\", line 57, in train_step\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/layers/layer.py\", line 908, in __call__\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/ops/operation.py\", line 46, in __call__\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\", line 156, in error_handler\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/models/functional.py\", line 182, in call\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/ops/function.py\", line 171, in _run_through_graph\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/models/functional.py\", line 637, in call\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/layers/layer.py\", line 908, in __call__\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/ops/operation.py\", line 46, in __call__\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\", line 156, in error_handler\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/layers/rnn/lstm.py\", line 584, in call\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/layers/rnn/rnn.py\", line 402, in call\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/layers/rnn/lstm.py\", line 551, in inner_loop\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/rnn.py\", line 841, in lstm\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/rnn.py\", line 874, in _cudnn_lstm\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/rnn.py\", line 557, in _assert_valid_mask\n\nassertion failed: [You are passing a RNN mask that does not correspond to right-padded sequences, while using cuDNN, which is not supported. With cuDNN, RNN masks can only be used for right-padding, e.g. `[[True, True, False, False]]` would be a valid mask, but any mask that isn\\'t just contiguous `True`\\'s on the left and contiguous `False`\\'s on the right would be invalid. You can pass `use_cudnn=False` to your RNN layer to stop using cuDNN (this may be slower).]\n\t [[{{node functional_7_1/lstm_15_1/Assert/Assert}}]] [Op:__inference_multi_step_on_iterator_182392]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_speed_test, X_pos_test = prepare_speed_position_data(test_features)\n",
        "y_test = cl_system.label_encoder.transform(test_labels)\n"
      ],
      "metadata": {
        "id": "-rKsaQDsoTsr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(cl_system.model.input_shape)\n"
      ],
      "metadata": {
        "id": "4ZRkgjbNrVcb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_probs = cl_system.model.predict([X_speed_test, X_pos_test])\n",
        "y_pred = np.argmax(y_pred_probs, axis=1)\n",
        "\n",
        "from sklearn.metrics import f1_score, classification_report\n",
        "\n",
        "f1_macro = f1_score(y_test, y_pred, average='macro')\n",
        "f1_weighted = f1_score(y_test, y_pred, average='weighted')\n",
        "print(\"F1 Score (macro):\", f1_macro)\n",
        "print(\"F1 Score (weighted):\", f1_weighted)\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, y_pred, target_names=cl_system.label_encoder.classes_))\n"
      ],
      "metadata": {
        "id": "zIVTAsdooVHT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}