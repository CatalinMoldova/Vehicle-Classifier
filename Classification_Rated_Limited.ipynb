{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        " \"\"\"\n",
        "# Abu Dhabi Traffic Flow Vehicle Behavior Classification Project\n",
        "\n",
        "## Research Objective:\n",
        "Train a CNN-LSTM hybrid model using actual vehicle labels from Aimsun data:\n",
        "- HDV Aggressive → Aggressive behavior\n",
        "- HDV Conventional Gipps Model → Normal behavior\n",
        "- HDV Cooperative → Cooperative behavior\n",
        "- CAV → Autonomous vehicle (excluded from training)\n",
        "\n",
        "## Model Validation:\n",
        "Compare CNN-LSTM predictions with actual vehicle labels to evaluate accuracy and F1 scores. Split the data into 80% training and 20% testing. Get the data from BOX using BOX API\n",
        "\"\"\"\n",
        "print(\"🚗 Abu Dhabi Traffic Flow Vehicle Behavior Classification System\")\n",
        "print(\"📊 Training with Actual Vehicle Labels from Data Files\")\n"
      ],
      "metadata": {
        "id": "19aUYFyUEL3B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "CHeck the GPU"
      ],
      "metadata": {
        "id": "ayA6YEbYEfTA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HXIpGdvyEE_T",
        "outputId": "b04a8ce4-1e1c-478d-f975-143888427f99"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sat Jul 26 17:07:17 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   67C    P0             34W /   70W |    7562MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from psutil import virtual_memory\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "if ram_gb < 20:\n",
        "  print('Not using a high-RAM runtime')\n",
        "else:\n",
        "  print('You are using a high-RAM runtime!')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jaOFC8HaEJpm",
        "outputId": "3ca9f5e9-7a3b-4ab3-81b4-a9bd5ba32517"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your runtime has 54.8 gigabytes of available RAM\n",
            "\n",
            "You are using a high-RAM runtime!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "flowchart TD\n",
        "\n",
        "    A[Raw CSV Data] --> B[Data Parsing & Cleaning]\n",
        "    B --> C[Feature Extraction]\n",
        "    C --> D[Data Preparation]\n",
        "    D --> E[Model Training (CNN-LSTM)]\n",
        "    E --> F[Prediction & Continuous Learning]\n",
        "    F --> G[Behavior Output: Aggressive/Cooperative/Normal]\n"
      ],
      "metadata": {
        "id": "iGLsituzEhKB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade tensorflow\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jI05iFqnYAti",
        "outputId": "8f599109-26a4-4e91-a78a-970232752711"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.19.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (5.29.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.14.1)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.73.1)\n",
            "Requirement already satisfied: tensorboard~=2.19.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.19.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.8.0)\n",
            "Requirement already satisfied: numpy<2.2.0,>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.0.2)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.14.0)\n",
            "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.5.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.1.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.7.14)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.8.2)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard~=2.19.0->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install boxsdk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lDM7amQRbYUK",
        "outputId": "789d4f72-0b44-41c7-8acf-ed8c49fdc52a"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: boxsdk in /usr/local/lib/python3.11/dist-packages (3.14.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from boxsdk) (25.3.0)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.11/dist-packages (from boxsdk) (2.5.0)\n",
            "Requirement already satisfied: requests<3,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from boxsdk) (2.32.3)\n",
            "Requirement already satisfied: requests-toolbelt>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from boxsdk) (1.0.0)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.11/dist-packages (from boxsdk) (2.9.0.post0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.4.3->boxsdk) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.4.3->boxsdk) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.4.3->boxsdk) (2025.7.14)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil->boxsdk) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install \"boxsdk[jwt]\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nob-Ws-EbYxC",
        "outputId": "bfd924a1-b907-4f67-d917-2cb36d88c461"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: boxsdk[jwt] in /usr/local/lib/python3.11/dist-packages (3.14.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from boxsdk[jwt]) (25.3.0)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.11/dist-packages (from boxsdk[jwt]) (2.5.0)\n",
            "Requirement already satisfied: requests<3,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from boxsdk[jwt]) (2.32.3)\n",
            "Requirement already satisfied: requests-toolbelt>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from boxsdk[jwt]) (1.0.0)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.11/dist-packages (from boxsdk[jwt]) (2.9.0.post0)\n",
            "Requirement already satisfied: pyjwt>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from boxsdk[jwt]) (2.10.1)\n",
            "Requirement already satisfied: cryptography>=3 in /usr/local/lib/python3.11/dist-packages (from boxsdk[jwt]) (43.0.3)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography>=3->boxsdk[jwt]) (1.17.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.4.3->boxsdk[jwt]) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.4.3->boxsdk[jwt]) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.4.3->boxsdk[jwt]) (2025.7.14)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil->boxsdk[jwt]) (1.17.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography>=3->boxsdk[jwt]) (2.22)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, json, numpy as np, pandas as pd, warnings\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model, load_model\n",
        "from tensorflow.keras.layers import (Input, LSTM, Conv1D, Dense, Dropout, BatchNormalization, Concatenate, GlobalMaxPooling1D, Masking)\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "import joblib\n",
        "import matplotlib.pyplot as plt\n",
        "import io\n",
        "import json\n",
        "import hashlib\n",
        "from tensorflow.keras.layers import Lambda\n",
        "from sklearn.metrics import f1_score, precision_score, recall_score, classification_report\n",
        "from boxsdk import Client, JWTAuth\n",
        "import pandas as pd\n",
        "import random\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.saving import register_keras_serializable\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "auth = JWTAuth.from_settings_file('key.json')\n",
        "client = Client(auth)\n",
        "\n",
        "\n",
        "@register_keras_serializable(package=\"custom_layers\")\n",
        "def compute_accel(speed_tensor):\n",
        "    # … your numpy/tf logic …\n",
        "    return acceleration_tensor\n",
        "\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "print(\"✅ All libraries imported successfully\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z4oDAKdSElhK",
        "outputId": "21266cfc-3c3e-42a7-d661-ee3b94fb283d"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ All libraries imported successfully\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Processing functions"
      ],
      "metadata": {
        "id": "Cg4LXHlyF9tG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_array_string(array_str):\n",
        "    \"\"\"Parse array strings with error handling\"\"\"\n",
        "    try:\n",
        "        if pd.isna(array_str): return []\n",
        "        cleaned = str(array_str).replace('inf', '0').replace('-inf', '0').replace('nan', 'None')\n",
        "        return eval(cleaned)\n",
        "    except:\n",
        "        return []\n",
        "\n",
        "def interpolate_missing_values(sequence):\n",
        "    \"\"\"Handle missing values using linear interpolation\"\"\"\n",
        "    \"\"\"\n",
        "    Interpolate only None or np.nan values in the sequence.\n",
        "    Leave -inf and +inf unchanged.\n",
        "    \"\"\"\n",
        "    if not sequence:\n",
        "        return []\n",
        "    arr = np.array(sequence, dtype=float)\n",
        "    # Identify missing values (None or np.nan)\n",
        "    missing_mask = np.isnan(arr)\n",
        "    # Valid values are finite and not missing\n",
        "    valid_mask = ~missing_mask & np.isfinite(arr)\n",
        "    valid_indices = np.where(valid_mask)[0]\n",
        "\n",
        "    # If all are missing, return zeros except for -inf/+inf\n",
        "    if np.all(missing_mask | ~np.isfinite(arr)):\n",
        "        return [v if np.isinf(v) else 0.0 for v in arr]\n",
        "\n",
        "    # If only one valid value, fill missing with that value (but keep infs)\n",
        "    if len(valid_indices) == 1:\n",
        "        fill_value = arr[valid_indices[0]]\n",
        "        return [\n",
        "            v if not np.isnan(v) else fill_value\n",
        "            for v in arr\n",
        "        ]\n",
        "\n",
        "    # Standard case: interpolate only missing values, leave infs untouched\n",
        "    interp_values = np.copy(arr)\n",
        "    # Indices to interpolate: missing and not inf\n",
        "    interp_indices = np.where(missing_mask & ~np.isinf(arr))[0]\n",
        "    if len(valid_indices) >= 2 and len(interp_indices) > 0:\n",
        "        # Interpolate only where needed\n",
        "        interp_result = np.interp(\n",
        "            interp_indices, valid_indices, arr[valid_mask]\n",
        "        )\n",
        "        interp_values[interp_indices] = interp_result\n",
        "\n",
        "    # Convert to list and return\n",
        "    return interp_values.tolist()\n",
        "\n",
        "def parse_coordinate_string(coord_str):\n",
        "    \"\"\"Parse coordinate data with interpolation\"\"\"\n",
        "    try:\n",
        "        if pd.isna(coord_str): return []\n",
        "        cleaned_str = str(coord_str).replace('inf', '0').replace('-inf', '0').replace('nan', 'None')\n",
        "        coords = eval(cleaned_str)\n",
        "        x_coords = [c[0] for c in coords]\n",
        "        y_coords = [c[1] for c in coords]\n",
        "        return list(zip(\n",
        "            interpolate_missing_values(x_coords),\n",
        "            interpolate_missing_values(y_coords)\n",
        "        ))\n",
        "    except:\n",
        "        return []\n",
        "\n",
        "def calculate_lateral_speeds(front_coords, rear_coords):\n",
        "    \"\"\"Calculate lateral movement speeds\"\"\"\n",
        "    if len(front_coords) < 2: return []\n",
        "    lateral_speeds = []\n",
        "    for i in range(1, len(front_coords)):\n",
        "        try:\n",
        "            dx = front_coords[i][0] - front_coords[i-1][0]\n",
        "            dy = front_coords[i][1] - front_coords[i-1][1]\n",
        "            lateral_speeds.append(np.sqrt(dx**2 + dy**2))\n",
        "        except:\n",
        "            lateral_speeds.append(0)\n",
        "    return interpolate_missing_values(lateral_speeds)\n"
      ],
      "metadata": {
        "id": "BjE3mvMzFv6C"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_vehicle_labels(df):\n",
        "    \"\"\"Extract labels directly from VehTypeName column\"\"\"\n",
        "    def map_vehicle_type(veh_type_name):\n",
        "        veh_type = str(veh_type_name).strip()\n",
        "        if 'CAV' in veh_type:\n",
        "            return 'autonomous'  # Will be excluded from training\n",
        "        elif 'Aggressive' in veh_type:\n",
        "            return 'aggressive'\n",
        "        elif 'Cooperative' in veh_type:\n",
        "            return 'cooperative'\n",
        "        elif 'Conventional' in veh_type or 'Gipps' in veh_type:\n",
        "            return 'normal'\n",
        "        else:\n",
        "            return 'normal'  # Default classification\n",
        "\n",
        "    df['behavior_label'] = df['VehTypeName'].apply(map_vehicle_type)\n",
        "    return df"
      ],
      "metadata": {
        "id": "U_mUvROnQ6K7"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Feature extraction and if any record is more than half empty then it is discarded"
      ],
      "metadata": {
        "id": "f0_t3CzdF_9n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_speed_position_features(vehicle_row, min_sequence_length=5):\n",
        "    \"\"\"\n",
        "    Extract only speed and position sequences from vehicle data row.\n",
        "    Returns None if data is insufficient.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Parse speed sequence\n",
        "        speeds = parse_array_string(vehicle_row['Speeds'])\n",
        "        # Parse front coordinates (positions)\n",
        "        front_coords = parse_coordinate_string(vehicle_row['VehFrontCoords'])\n",
        "\n",
        "        # Validation: Check for at least 30 invalid values in any sequence\n",
        "        def count_invalid(seq):\n",
        "            return sum(\n",
        "                (v is None) or\n",
        "                (isinstance(v, float) and (np.isnan(v) or np.isinf(v)))\n",
        "                for v in seq\n",
        "            )\n",
        "\n",
        "        if count_invalid(speeds) >= 30 or count_invalid(front_coords) >= 30:\n",
        "            return None\n",
        "\n",
        "        # Clean sequences\n",
        "        speeds_clean = interpolate_missing_values(speeds)\n",
        "        positions_clean = front_coords  # Already interpolated by parse_coordinate_string\n",
        "\n",
        "        # Ensure consistent length\n",
        "        min_length = min(len(speeds_clean), len(positions_clean))\n",
        "        if min_length < min_sequence_length:\n",
        "            return None\n",
        "\n",
        "        # Truncate to same length\n",
        "        speeds_clean = speeds_clean[:min_length]\n",
        "        positions_clean = positions_clean[:min_length]\n",
        "\n",
        "        # Convert to arrays\n",
        "        speeds_arr = np.array(speeds_clean).reshape(-1, 1)\n",
        "        positions_arr = np.array(positions_clean).reshape(-1, 2)\n",
        "\n",
        "        return {\n",
        "            'speeds': speeds_arr,          # shape (N, 1)\n",
        "            'positions': positions_arr,    # shape (N, 2)\n",
        "            'sequence_length': min_length\n",
        "        }\n",
        "    except Exception as e:\n",
        "        print(f\"Feature extraction error: {e}\")\n",
        "        return None\n"
      ],
      "metadata": {
        "id": "ngSBzjcLF8rM"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data reading function"
      ],
      "metadata": {
        "id": "47gNKBafEzBN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def read_aimsun_data(file_path, column_map=None):\n",
        "    \"\"\"Read CSV with flexible column mapping\"\"\"\n",
        "    DEFAULT_MAP = {\n",
        "        'vehicle_id': 'VehNr', 'timestep': 'Timestep', 'speed': 'Speeds',\n",
        "        'acceleration': 'Accelerations', 'front_coords': 'VehFrontCoords',\n",
        "        'rear_coords': 'VehRearCoords', 'vehicle_type': 'VehTypeName', 'length': 'Length'\n",
        "    }\n",
        "    col_map = column_map or DEFAULT_MAP\n",
        "    try:\n",
        "        df = pd.read_csv(file_path)\n",
        "        return df.rename(columns={v: k for k, v in col_map.items()})\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading {file_path}: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "def extract_vehicle_table(df, vehicle_id):\n",
        "    \"\"\"Extract time-series table for specific vehicle\"\"\"\n",
        "    vehicle_data = df[df['vehicle_id'] == vehicle_id].copy()\n",
        "    vehicle_data['front_coords'] = vehicle_data['front_coords'].apply(parse_coordinate_string)\n",
        "    vehicle_data['rear_coords'] = vehicle_data['rear_coords'].apply(parse_coordinate_string)\n",
        "    vehicle_data['position'] = vehicle_data.apply(\n",
        "        lambda x: np.mean([x['front_coords'], x['rear_coords']], axis=0), axis=1)\n",
        "    return vehicle_data[['timestep', 'speed', 'acceleration', 'position']]\n",
        "\n",
        "print(\"✅ Data reading functions ready\")\n"
      ],
      "metadata": {
        "id": "jTs8hiznEo0o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a024fb28-44bd-4a02-91ae-e592f65f5bf3"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Data reading functions ready\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def process_labeled_data_speed_position(csv_files, folder_path, max_files=None):\n",
        "    \"\"\"\n",
        "    Process data files using only speed and position features.\n",
        "    \"\"\"\n",
        "    all_features = []\n",
        "    all_labels = []\n",
        "    #vehicle_details = []\n",
        "    processed_count = 0\n",
        "\n",
        "    files_to_process = csv_files[:max_files] if max_files else csv_files\n",
        "\n",
        "    for filename in files_to_process:\n",
        "        try:\n",
        "            print(f\"📄 Processing {filename}...\")\n",
        "            file_path = os.path.join(folder_path, filename)\n",
        "            df = pd.read_csv(file_path)\n",
        "\n",
        "            # Extract labels from VehTypeName\n",
        "            df = extract_vehicle_labels(df)\n",
        "\n",
        "            print(f\"   Labels in {filename}:\")\n",
        "            file_labels = df['behavior_label'].value_counts()\n",
        "            for label, count in file_labels.items():\n",
        "                print(f\"     {label}: {count}\")\n",
        "\n",
        "            for idx, row in df.iterrows():\n",
        "                if row['behavior_label'] == 'autonomous':\n",
        "                    continue\n",
        "\n",
        "                features = extract_speed_position_features(row)\n",
        "                if features is not None:\n",
        "                    all_features.append(features)\n",
        "                    all_labels.append(row['behavior_label'])\n",
        "                    processed_count += 1\n",
        "\n",
        "            print(f\"   ✅ Extracted {processed_count} valid vehicles so far\")\n",
        "        except Exception as e:\n",
        "            print(f\"   ❌ Error: {e}\")\n",
        "\n",
        "    return all_features, all_labels\n"
      ],
      "metadata": {
        "id": "ms4Ms3QEKjEa"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_speed_position_data(features_list, max_sequence=60):\n",
        "    \"\"\"\n",
        "    Prepare padded speed and position arrays for model input.\n",
        "    Returns:\n",
        "        X_speed: (num_samples, max_sequence, 1)\n",
        "        X_pos: (num_samples, max_sequence, 2)\n",
        "    \"\"\"\n",
        "    X_speed = []\n",
        "    X_pos = []\n",
        "    for features in features_list:\n",
        "        speeds = features['speeds'][:max_sequence]\n",
        "        positions = features['positions'][:max_sequence]\n",
        "        seq_len = min(len(speeds), max_sequence)\n",
        "\n",
        "        # Pad if needed\n",
        "        speed_padded = np.zeros((max_sequence, 1))\n",
        "        pos_padded = np.zeros((max_sequence, 2))\n",
        "        speed_padded[:seq_len] = speeds[:seq_len]\n",
        "        pos_padded[:seq_len] = positions[:seq_len]\n",
        "\n",
        "        X_speed.append(speed_padded)\n",
        "        X_pos.append(pos_padded)\n",
        "    return np.array(X_speed), np.array(X_pos)\n"
      ],
      "metadata": {
        "id": "NjHLMrcCWEjs"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Replace your build_speed_position_model_fixed function with this version:\n",
        "\n",
        "def build_speed_position_model_fixed(sequence_length=60, num_classes=3):\n",
        "    \"\"\"\n",
        "    Fixed model that properly handles multiple classes and computes\n",
        "    acceleration/lateral movement correctly. Fixed cuDNN compatibility.\n",
        "    \"\"\"\n",
        "    # Inputs\n",
        "    speed_input = Input(shape=(sequence_length, 1), name='speed')\n",
        "    position_input = Input(shape=(sequence_length, 2), name='position')\n",
        "\n",
        "    # Compute acceleration (difference between consecutive speeds)\n",
        "    def compute_acceleration(speed_tensor):\n",
        "        # Pad with zeros at the beginning for the first timestep\n",
        "        zeros = tf.zeros_like(speed_tensor[:, :1, :])\n",
        "        accel = speed_tensor[:, 1:, :] - speed_tensor[:, :-1, :]\n",
        "        return tf.concat([zeros, accel], axis=1)\n",
        "\n",
        "    accel = Lambda(compute_acceleration, name='acceleration')(speed_input)\n",
        "\n",
        "    # Compute lateral movement (Euclidean distance between consecutive positions)\n",
        "    def compute_lateral_movement(pos_tensor):\n",
        "        # Calculate displacement vectors\n",
        "        displacement = pos_tensor[:, 1:, :] - pos_tensor[:, :-1, :]\n",
        "        # Calculate magnitudes (lateral speed)\n",
        "        lateral_speed = tf.norm(displacement, axis=-1, keepdims=True)\n",
        "        # Pad with zeros at the beginning\n",
        "        zeros = tf.zeros_like(lateral_speed[:, :1, :])\n",
        "        return tf.concat([zeros, lateral_speed], axis=1)\n",
        "\n",
        "    lateral = Lambda(compute_lateral_movement, name='lateral')(position_input)\n",
        "\n",
        "    # Concatenate all features\n",
        "    features = Concatenate(axis=-1)([speed_input, accel, lateral])  # shape: (batch, seq, 3)\n",
        "\n",
        "    # REMOVE MASKING - this causes cuDNN issues\n",
        "    # masked = Masking(mask_value=0.0)(features)\n",
        "\n",
        "    # CNN branch - work directly on features\n",
        "    cnn = Conv1D(64, 3, activation='relu', padding='same')(features)\n",
        "    cnn = BatchNormalization()(cnn)\n",
        "    cnn = Dropout(0.3)(cnn)\n",
        "    cnn = Conv1D(128, 3, activation='relu', padding='same')(cnn)\n",
        "    cnn = BatchNormalization()(cnn)\n",
        "    cnn_out = GlobalMaxPooling1D()(cnn)\n",
        "\n",
        "    # LSTM branch - disable cuDNN to avoid masking issues\n",
        "    lstm = LSTM(128, return_sequences=True, use_cudnn=False)(features)  # FIX: Disable cuDNN\n",
        "    lstm = BatchNormalization()(lstm)\n",
        "    lstm_out = LSTM(64, use_cudnn=False)(lstm)  # FIX: Disable cuDNN\n",
        "\n",
        "    # Feature fusion\n",
        "    combined = Concatenate()([cnn_out, lstm_out])\n",
        "    x = Dense(256, activation='relu')(combined)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Dropout(0.5)(x)\n",
        "    x = Dense(128, activation='relu')(x)\n",
        "\n",
        "    # FIXED: Output layer should have num_classes neurons, not 1\n",
        "    output = Dense(num_classes, activation='softmax')(x)\n",
        "\n",
        "    model = Model(inputs=[speed_input, position_input], outputs=output)\n",
        "\n",
        "    # Use appropriate optimizer and learning rate\n",
        "    model.compile(\n",
        "        optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
        "        loss='sparse_categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "\n",
        "    print(\"🧠 Fixed Model Architecture:\")\n",
        "    model.summary()\n",
        "    return model\n",
        "\n",
        "# Also fix your prepare_speed_position_data function:\n",
        "\n",
        "def prepare_speed_position_data(features_list, max_sequence=60):\n",
        "    \"\"\"\n",
        "    Prepare padded speed and position arrays for model input.\n",
        "    Fixed for cuDNN compatibility.\n",
        "    Returns:\n",
        "        X_speed: (num_samples, max_sequence, 1)\n",
        "        X_pos: (num_samples, max_sequence, 2)\n",
        "    \"\"\"\n",
        "    X_speed = []\n",
        "    X_pos = []\n",
        "    for features in features_list:\n",
        "        speeds = features['speeds'][:max_sequence]\n",
        "        positions = features['positions'][:max_sequence]\n",
        "        seq_len = min(len(speeds), max_sequence)\n",
        "\n",
        "        # Pad if needed - ensure proper dtype\n",
        "        speed_padded = np.zeros((max_sequence, 1), dtype=np.float32)  # FIX: Add dtype\n",
        "        pos_padded = np.zeros((max_sequence, 2), dtype=np.float32)    # FIX: Add dtype\n",
        "        speed_padded[:seq_len] = speeds[:seq_len]\n",
        "        pos_padded[:seq_len] = positions[:seq_len]\n",
        "\n",
        "        X_speed.append(speed_padded)\n",
        "        X_pos.append(pos_padded)\n",
        "    return np.array(X_speed, dtype=np.float32), np.array(X_pos, dtype=np.float32)  # FIX: Ensure dtype"
      ],
      "metadata": {
        "id": "1m9Vc8MLLr4w"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "graph TD\n",
        "\n",
        "    TS[Time Series Input<br/>60 x 3] --> Mask[Masking Layer]\n",
        "    Mask --> CNN1[Conv1D 64<br/>Kernel=3]\n",
        "    CNN1 --> BN1[BatchNorm]\n",
        "    BN1 --> Drop1[Dropout 0.3]\n",
        "    Drop1 --> CNN2[Conv1D 128<br/>Kernel=3]\n",
        "    CNN2 --> BN2[BatchNorm]\n",
        "    BN2 --> GMP[GlobalMaxPooling1D]\n",
        "    \n",
        "    Mask --> LSTM1[LSTM 128<br/>return_sequences=True]\n",
        "    LSTM1 --> BN3[BatchNorm]\n",
        "    BN3 --> LSTM2[LSTM 64]\n",
        "    \n",
        "    S[Static Features<br/>13 dims] --> Dense1[Dense 64]\n",
        "    Dense1 --> BN4[BatchNorm]\n",
        "    \n",
        "    GMP --> Concat[Concatenate]\n",
        "    LSTM2 --> Concat\n",
        "    BN4 --> Concat\n",
        "    \n",
        "    Concat --> Dense2[Dense 256]\n",
        "    Dense2 --> BN5[BatchNorm]\n",
        "    BN5 --> Drop2[Dropout 0.5]\n",
        "    Drop2 --> Dense3[Dense 128]\n",
        "    Dense3 --> Output[Softmax Output<br/>3 Classes]\n"
      ],
      "metadata": {
        "id": "AELTfAu-LuCN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calculate the F1 score"
      ],
      "metadata": {
        "id": "_KFMVLbeKpxk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def classify_vehicle_behavior_speed_position(vehicle_data, model, max_sequence=60):\n",
        "    \"\"\"\n",
        "    Classify vehicle behavior using only speed and position sequences.\n",
        "    \"\"\"\n",
        "    features = extract_speed_position_features(vehicle_data)\n",
        "    if not features:\n",
        "        return \"unknown\"\n",
        "\n",
        "    speeds = features['speeds'][:max_sequence]\n",
        "    positions = features['positions'][:max_sequence]\n",
        "    seq_len = min(len(speeds), max_sequence)\n",
        "\n",
        "    # Pad\n",
        "    speed_padded = np.zeros((1, max_sequence, 1))\n",
        "    pos_padded = np.zeros((1, max_sequence, 2))\n",
        "    speed_padded[0, :seq_len] = speeds\n",
        "    pos_padded[0, :seq_len] = positions\n",
        "\n",
        "    prediction = model.predict([speed_padded, pos_padded])\n",
        "    class_id = np.argmax(prediction)\n",
        "    return {0: 'aggressive', 1: 'cooperative', 2: 'normal'}.get(class_id, 'unknown')\n"
      ],
      "metadata": {
        "id": "Xv_LU62xLhDy"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def analyze_f1_performance(comparisons, actual_labels, predictions):\n",
        "    \"\"\"Detailed F1 score analysis with visualizations\"\"\"\n",
        "\n",
        "    # Classification report\n",
        "    print(\"\\n📋 Detailed Classification Report:\")\n",
        "    print(\"=\" * 60)\n",
        "    print(classification_report(actual_labels, predictions))\n",
        "\n",
        "    # F1 scores by confidence ranges\n",
        "    df_comp = pd.DataFrame(comparisons)\n",
        "    confidence_ranges = [(0.0, 0.5), (0.5, 0.7), (0.7, 0.9), (0.9, 1.0)]\n",
        "\n",
        "    print(\"\\n📊 F1 Score by Confidence Ranges:\")\n",
        "    print(\"-\" * 50)\n",
        "    for low, high in confidence_ranges:\n",
        "        mask = (df_comp['confidence'] >= low) & (df_comp['confidence'] < high)\n",
        "        subset = df_comp[mask]\n",
        "        if len(subset) > 0:\n",
        "            subset_f1 = f1_score(subset['actual'], subset['predicted'], average='macro')\n",
        "            print(f\"Confidence {low:.1f}-{high:.1f}: F1={subset_f1:.3f} (n={len(subset)})\")\n",
        "\n",
        "    # Misclassified vehicles analysis\n",
        "    print(f\"\\n❌ Misclassified Vehicles Analysis:\")\n",
        "    misclassified = df_comp[df_comp['correct'] == False]\n",
        "    if not misclassified.empty:\n",
        "        print(f\"Total misclassified: {len(misclassified)}\")\n",
        "        for behavior in ['aggressive', 'cooperative', 'normal']:\n",
        "            behavior_miss = misclassified[misclassified['actual'] == behavior]\n",
        "            if len(behavior_miss) > 0:\n",
        "                print(f\"  {behavior}: {len(behavior_miss)} vehicles\")\n",
        "\n",
        "def plot_f1_results(comparisons, actual_labels, predictions):\n",
        "    \"\"\"Visualize F1 score results\"\"\"\n",
        "\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
        "\n",
        "    # 1. F1 scores by class\n",
        "    classes = sorted(set(actual_labels + predictions))\n",
        "    if len(classes) > 1:\n",
        "        f1_per_class = f1_score(actual_labels, predictions, average=None, labels=classes)\n",
        "        axes[0,0].bar(classes, f1_per_class, color=['#ff9999','#66b3ff','#99ff99'])\n",
        "        axes[0,0].set_title('F1 Score by Vehicle Behavior Class')\n",
        "        axes[0,0].set_ylabel('F1 Score')\n",
        "        axes[0,0].set_ylim(0, 1.1)\n",
        "\n",
        "    # 2. Prediction confidence distribution\n",
        "    df_comp = pd.DataFrame(comparisons)\n",
        "    axes[0,1].hist([df_comp[df_comp['correct']]['confidence'],\n",
        "                   df_comp[~df_comp['correct']]['confidence']],\n",
        "                  bins=20, alpha=0.7, label=['Correct', 'Incorrect'])\n",
        "    axes[0,1].set_title('Prediction Confidence Distribution')\n",
        "    axes[0,1].set_xlabel('Confidence')\n",
        "    axes[0,1].legend()\n",
        "\n",
        "    # 3. Confusion matrix\n",
        "    if len(classes) > 1:\n",
        "        from sklearn.metrics import confusion_matrix\n",
        "        import seaborn as sns\n",
        "        cm = confusion_matrix(actual_labels, predictions, labels=classes)\n",
        "        sns.heatmap(cm, annot=True, fmt='d', xticklabels=classes, yticklabels=classes, ax=axes[1,0])\n",
        "        axes[1,0].set_title('Confusion Matrix')\n",
        "        axes[1,0].set_xlabel('Predicted')\n",
        "        axes[1,0].set_ylabel('Actual')\n",
        "\n",
        "    # 4. F1 vs Accuracy comparison\n",
        "    accuracy = df_comp['correct'].mean()\n",
        "    f1_macro = f1_score(actual_labels, predictions, average='macro') if len(classes) > 1 else accuracy\n",
        "\n",
        "    metrics = ['Accuracy', 'F1-Macro']\n",
        "    values = [accuracy, f1_macro]\n",
        "    axes[1,1].bar(metrics, values, color=['lightblue', 'lightcoral'])\n",
        "    axes[1,1].set_title('Accuracy vs F1 Score')\n",
        "    axes[1,1].set_ylabel('Score')\n",
        "    axes[1,1].set_ylim(0, 1.1)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "print(\"✅ F1 analysis and visualization functions ready\")\n"
      ],
      "metadata": {
        "id": "rxl7qvJNLlyY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7629acde-8a79-43d0-a15a-83594b1a2e4b"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ F1 analysis and visualization functions ready\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def compare_predictions_with_labels_f1_speed_position(model, features_list, vehicle_details, label_encoder, max_sequence=60):\n",
        "    \"\"\"\n",
        "    Compare model predictions with actual vehicle labels using F1 score.\n",
        "    \"\"\"\n",
        "    predictions = []\n",
        "    actual_labels = []\n",
        "    comparisons = []\n",
        "\n",
        "    print(\"🔍 Comparing predictions with actual labels (F1 Score Evaluation)...\")\n",
        "    print(\"-\" * 80)\n",
        "    print(f\"{'VehNr':<8} {'Actual Label':<15} {'Predicted':<15} {'Match':<8} {'Confidence':<12}\")\n",
        "    print(\"-\" * 80)\n",
        "\n",
        "    correct_predictions = 0\n",
        "\n",
        "    X_speed, X_pos = prepare_speed_position_data(features_list, max_sequence=max_sequence)\n",
        "\n",
        "    for i, vehicle_info in enumerate(vehicle_details):\n",
        "        # Predict\n",
        "        prediction_probs = model.predict([X_speed[i:i+1], X_pos[i:i+1]], verbose=0)\n",
        "        predicted_class_idx = np.argmax(prediction_probs)\n",
        "        predicted_label = label_encoder.inverse_transform([predicted_class_idx])[0]\n",
        "        confidence = np.max(prediction_probs)\n",
        "        actual_label = vehicle_info['actual_label']\n",
        "        is_correct = predicted_label == actual_label\n",
        "\n",
        "        if is_correct:\n",
        "            correct_predictions += 1\n",
        "\n",
        "        predictions.append(predicted_label)\n",
        "        actual_labels.append(actual_label)\n",
        "        comparison = {\n",
        "            'VehNr': vehicle_info['VehNr'],\n",
        "            'actual': actual_label,\n",
        "            'predicted': predicted_label,\n",
        "            'correct': is_correct,\n",
        "            'confidence': confidence,\n",
        "            'file': vehicle_info['file']\n",
        "        }\n",
        "        comparisons.append(comparison)\n",
        "\n",
        "        if i < 20:\n",
        "            match_symbol = \"✅\" if is_correct else \"❌\"\n",
        "            print(f\"{vehicle_info['VehNr']:<8} {actual_label:<15} {predicted_label:<15} {match_symbol:<8} {confidence:.3f}\")\n",
        "\n",
        "    # Calculate F1 scores\n",
        "    try:\n",
        "        f1_macro = f1_score(actual_labels, predictions, average='macro')\n",
        "        f1_weighted = f1_score(actual_labels, predictions, average='weighted')\n",
        "        f1_per_class = f1_score(actual_labels, predictions, average=None, labels=label_encoder.classes_)\n",
        "        precision_macro = precision_score(actual_labels, predictions, average='macro')\n",
        "        recall_macro = recall_score(actual_labels, predictions, average='macro')\n",
        "        accuracy = correct_predictions / len(vehicle_details)\n",
        "\n",
        "        print(\"-\" * 80)\n",
        "        print(f\"🎯 Overall Accuracy: {accuracy:.3f} ({correct_predictions}/{len(vehicle_details)})\")\n",
        "        print(f\"📊 F1 Score (Macro): {f1_macro:.3f}\")\n",
        "        print(f\"📊 F1 Score (Weighted): {f1_weighted:.3f}\")\n",
        "        print(f\"📊 Precision (Macro): {precision_macro:.3f}\")\n",
        "        print(f\"📊 Recall (Macro): {recall_macro:.3f}\")\n",
        "\n",
        "        print(\"\\n🏷️ F1 Score per Class:\")\n",
        "        for i, class_name in enumerate(label_encoder.classes_):\n",
        "            if i < len(f1_per_class):\n",
        "                print(f\"  {class_name:15s}: {f1_per_class[i]:.3f}\")\n",
        "    except Exception as e:\n",
        "        print(f\"⚠️ Could not calculate F1 scores: {e}\")\n",
        "        f1_macro = f1_weighted = 0.0\n",
        "\n",
        "    return comparisons, accuracy, f1_macro, f1_weighted\n"
      ],
      "metadata": {
        "id": "6kh-oGxmKpJ_"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import io\n",
        "import json\n",
        "import os\n",
        "import hashlib\n",
        "\n",
        "class ContinuousLearningSystem:\n",
        "\n",
        "    def __init__(self, model_path, scaler_path, le_path, data_path, processed_path, box_client=None, box_folder=None):\n",
        "        self.model_path = model_path\n",
        "        self.scaler_path = scaler_path\n",
        "        self.le_path = le_path\n",
        "        self.data_path = data_path\n",
        "        self.processed_path = processed_path\n",
        "        self.processed_hashes = self.load_processed_hashes()\n",
        "        self.box_client = box_client\n",
        "        self.box_folder = box_folder\n",
        "\n",
        "        # Load existing model artifacts if they exist\n",
        "        self.model = self.load_model()\n",
        "        self.scaler = self.load_scaler()\n",
        "        self.label_encoder = self.load_label_encoder()\n",
        "\n",
        "\n",
        "    def load_processed_hashes(self):\n",
        "        try:\n",
        "            if os.path.exists(self.processed_path):\n",
        "                with open(self.processed_path, 'r') as f:\n",
        "                    return set(json.load(f))\n",
        "            else:\n",
        "                return set()\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading processed hashes: {e}\")\n",
        "            return set()\n",
        "    def load_model(self):\n",
        "        try:\n",
        "            if os.path.exists(self.model_path):\n",
        "                print(f\"Loading existing model from {self.model_path}\")\n",
        "                return load_model(self.model_path)\n",
        "            else:\n",
        "                print(\"No existing model found. A new model will be built.\")\n",
        "                return None\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading model from {self.model_path}: {e}\")\n",
        "            return None\n",
        "\n",
        "    def load_scaler(self):\n",
        "        try:\n",
        "            if os.path.exists(self.scaler_path):\n",
        "                print(f\"Loading existing scaler from {self.scaler_path}\")\n",
        "                return joblib.load(self.scaler_path)\n",
        "            else:\n",
        "                print(\"No existing scaler found.\")\n",
        "                return None\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading scaler from {self.scaler_path}: {e}\")\n",
        "            return None\n",
        "\n",
        "    def load_label_encoder(self):\n",
        "        try:\n",
        "            if os.path.exists(self.le_path):\n",
        "                print(f\"Loading existing label encoder from {self.le_path}\")\n",
        "                return joblib.load(self.le_path)\n",
        "            else:\n",
        "                print(\"No existing label encoder found.\")\n",
        "                return None\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading label encoder from {self.le_path}: {e}\")\n",
        "            return None\n",
        "\n",
        "\n",
        "\n",
        "    def save_processed_hashes(self):\n",
        "        try:\n",
        "            with open(self.processed_path, 'w') as f:\n",
        "                json.dump(list(self.processed_hashes), f)\n",
        "        except Exception as e:\n",
        "            print(f\"Error saving processed hashes: {e}\")\n",
        "\n",
        "    def csv_stream_hash(self, csv_stream):\n",
        "        pos = csv_stream.tell()\n",
        "        csv_stream.seek(0)\n",
        "        content = csv_stream.read()\n",
        "        csv_stream.seek(pos)  # Restore position\n",
        "        return hashlib.sha256(content.encode('utf-8')).hexdigest()\n",
        "\n",
        "    def stream_all_csv_files_from_box(self, folder):\n",
        "        \"\"\"Yield (file_name, csv_stream) for every CSV file in Box folder and subfolders.\"\"\"\n",
        "        for item in folder.get_items(limit=2000):\n",
        "            if item.type == 'folder':\n",
        "                yield from self.stream_all_csv_files_from_box(self.box_client.folder(item.id))\n",
        "            elif item.type == 'file' and item.name.endswith('.csv'):\n",
        "                file_content = item.content()\n",
        "                csv_stream = io.StringIO(file_content.decode('utf-8'))\n",
        "                yield item.name, csv_stream\n",
        "\n",
        "    def get_new_box_files(self):\n",
        "        \"\"\"Get new CSV files from Box that have not been processed yet.\"\"\"\n",
        "        new_files = []\n",
        "        if not self.box_client or not self.box_folder:\n",
        "            print(\"Box client or folder not configured.\")\n",
        "            return new_files\n",
        "\n",
        "        for file_name, csv_stream in self.stream_all_csv_files_from_box(self.box_folder):\n",
        "            file_hash = self.csv_stream_hash(csv_stream)\n",
        "            if file_hash not in self.processed_hashes:\n",
        "                new_files.append((file_name, csv_stream, file_hash))\n",
        "        return new_files\n",
        "\n",
        "\n",
        "    def update_model(self):\n",
        "        model = self.model\n",
        "        scaler = self.scaler\n",
        "        label_encoder = self.label_encoder\n",
        "\n",
        "        # fetch all file lists\n",
        "        train_files = [...]    # wherever you get these\n",
        "        test_files  = [...]\n",
        "\n",
        "        # Process existing training & testing data\n",
        "        train_features, train_labels, _ = self.process_labeled_data_streams(train_files)\n",
        "        test_features,  test_labels,  _ = self.process_labeled_data_streams(test_files)\n",
        "\n",
        "        # Check for newly arrived data\n",
        "        new_files = self.get_new_box_files()\n",
        "        if not new_files:\n",
        "            print(\"No new files to process from Box\")\n",
        "            return model, scaler, label_encoder\n",
        "\n",
        "        new_features, new_labels, _ = self.process_labeled_data_streams(new_files)\n",
        "        if not new_features:\n",
        "            print(\"No valid features extracted from new Box files\")\n",
        "            return model, scaler, label_encoder\n",
        "\n",
        "        # (re)fit scaler & encoder if needed\n",
        "        if scaler is None:\n",
        "            print(\"Fitting new scaler on new data\")\n",
        "            static_feats = [f['static'] for f in new_features]\n",
        "            scaler = StandardScaler().fit(static_feats)\n",
        "            self.scaler = scaler\n",
        "\n",
        "        if label_encoder is None:\n",
        "            print(\"Fitting new label encoder on new data\")\n",
        "            label_encoder = LabelEncoder().fit(new_labels)\n",
        "            self.label_encoder = label_encoder\n",
        "\n",
        "        # —— NEW: split incoming new_features/new_labels 80/20 —— #\n",
        "        new_train_feats, new_test_feats, new_train_lbls, new_test_lbls = train_test_split(\n",
        "            new_features,\n",
        "            new_labels,\n",
        "            test_size=0.2,\n",
        "            random_state=42,\n",
        "            stratify=new_labels  # if you want to preserve class balance\n",
        "        )\n",
        "\n",
        "        # Merge splits into your main train/test sets\n",
        "        train_features.extend(new_train_feats)\n",
        "        train_labels.extend(new_train_lbls)\n",
        "        test_features.extend(new_test_feats)\n",
        "        test_labels.extend(new_test_lbls)\n",
        "\n",
        "        # Prepare model inputs\n",
        "        X_ts_train, X_static_train, y_train = self.prepare_new_data(\n",
        "            train_features, train_labels, scaler, label_encoder\n",
        "        )\n",
        "        X_ts_test,  X_static_test,  y_test  = self.prepare_new_data(\n",
        "            test_features, test_labels, scaler, label_encoder\n",
        "        )\n",
        "\n",
        "        # Build model if first time\n",
        "        if model is None:\n",
        "            print(\"Building a new model for initial training.\")\n",
        "            num_features         = X_ts_train.shape[-1] if X_ts_train.size else 3\n",
        "            static_feature_count = X_static_train.shape[-1] if X_static_train.size else 13\n",
        "            num_classes          = len(label_encoder.classes_) if label_encoder else 3\n",
        "\n",
        "            model = build_speed_position_model(\n",
        "                sequence_length=X_ts_train.shape[1],\n",
        "                num_features=num_features,\n",
        "                static_feature_count=static_feature_count,\n",
        "                num_classes=num_classes\n",
        "            )\n",
        "            self.model = model\n",
        "\n",
        "        # Train on the enlarged training set\n",
        "        print(f\"Training model on {len(train_features)} total training samples\")\n",
        "        model.fit(\n",
        "            [X_ts_train, X_static_train],\n",
        "            y_train,\n",
        "            epochs=3,\n",
        "            batch_size=32,\n",
        "            validation_split=0.2\n",
        "        )\n",
        "\n",
        "        # (Optional) Evaluate on the enlarged test set\n",
        "        loss, acc = model.evaluate([X_ts_test, X_static_test], y_test, verbose=0)\n",
        "        print(f\"Test loss: {loss:.4f}, Test accuracy: {acc:.4f}\")\n",
        "\n",
        "        # Save everything\n",
        "        self.save_model()\n",
        "        self.save_scaler()\n",
        "        self.save_label_encoder()\n",
        "\n",
        "        # Mark new files as processed\n",
        "        for _, _, fhash in new_files:\n",
        "            self.processed_hashes.add(fhash)\n",
        "        self.save_processed_hashes()\n",
        "\n",
        "        print(f\"Updated model with {len(new_features)} new samples \"\n",
        "              f\"({len(new_train_feats)}→train, {len(new_test_feats)}→test)\")\n",
        "        return self.model, self.scaler, self.label_encoder\n",
        "\n",
        "    def process_labeled_data_streams(self, file_streams):\n",
        "        \"\"\"Process labeled data from a list of (file_name, csv_stream) tuples.\"\"\"\n",
        "        all_features = []\n",
        "        all_labels = []\n",
        "        vehicle_details = []\n",
        "        processed_count = 0\n",
        "\n",
        "        for file_name, csv_stream, _ in file_streams:\n",
        "            try:\n",
        "                print(f\"📄 Processing {file_name} from Box stream...\")\n",
        "                df = pd.read_csv(csv_stream)\n",
        "                df = extract_vehicle_labels(df)\n",
        "                print(f\"   Labels in {file_name}:\")\n",
        "                file_labels = df['behavior_label'].value_counts()\n",
        "                for label, count in file_labels.items():\n",
        "                    print(f\"     {label}: {count}\")\n",
        "                for idx, row in df.iterrows():\n",
        "                    if row['behavior_label'] == 'autonomous':\n",
        "                        continue\n",
        "                    features = extract_speed_position_features(row)\n",
        "                    if features is not None:\n",
        "                        all_features.append(features)\n",
        "                        all_labels.append(row['behavior_label'])\n",
        "                        vehicle_details.append({\n",
        "                            'VehNr': row['VehNr'],\n",
        "                            'VehTypeName': row['VehTypeName'],\n",
        "                            'actual_label': row['behavior_label'],\n",
        "                            'file': file_name\n",
        "                        })\n",
        "                        processed_count += 1\n",
        "                print(f\"   ✅ Extracted {processed_count} valid vehicles so far\")\n",
        "            except Exception as e:\n",
        "                print(f\"   ❌ Error processing {file_name}: {e}\")\n",
        "\n",
        "        return all_features, all_labels, vehicle_details\n",
        "\n",
        "\n",
        "    def save_model(self):\n",
        "        try:\n",
        "            if self.model:\n",
        "                self.model.save(self.model_path)\n",
        "                print(f\"Model saved to {self.model_path}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error saving model to {self.model_path}: {e}\")\n",
        "\n",
        "    def save_scaler(self):\n",
        "        try:\n",
        "            if self.scaler:\n",
        "                joblib.dump(self.scaler, self.scaler_path)\n",
        "                print(f\"Scaler saved to {self.scaler_path}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error saving scaler to {self.scaler_path}: {e}\")\n",
        "\n",
        "    def save_label_encoder(self):\n",
        "        try:\n",
        "            if self.label_encoder:\n",
        "                joblib.dump(self.label_encoder, self.le_path)\n",
        "                print(f\"Label encoder saved to {self.le_path}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error saving label encoder to {self.le_path}: {e}\")\n",
        "\n",
        "    def load_all_data_from_box(self):\n",
        "        \"\"\"\n",
        "        Load all vehicle data from Box into a single list.\n",
        "        Each item: {'features': ..., 'label': ..., 'details': {...}}\n",
        "        \"\"\"\n",
        "        all_data = []\n",
        "        file_counter = 0\n",
        "\n",
        "        for file_name, csv_stream in self.stream_all_csv_files_from_box(self.box_folder):\n",
        "            fhash = self.csv_stream_hash(csv_stream)\n",
        "            if fhash in self.processed_hashes:\n",
        "                continue\n",
        "\n",
        "            try:\n",
        "                csv_stream.seek(0)\n",
        "                df = pd.read_csv(csv_stream)\n",
        "                df = extract_vehicle_labels(df)\n",
        "\n",
        "                for _, row in df.iterrows():\n",
        "                    if row['behavior_label'] == 'autonomous':\n",
        "                        continue\n",
        "                    features = extract_speed_position_features(row)\n",
        "                    if features is not None:\n",
        "                        record = {\n",
        "                            'features': features,\n",
        "                            'label': row['behavior_label'],\n",
        "                            'details': {\n",
        "                                'VehNr': row['VehNr'],\n",
        "                                'VehTypeName': row['VehTypeName'],\n",
        "                                'file': file_name\n",
        "                            }\n",
        "                        }\n",
        "                        all_data.append(record)\n",
        "                self.processed_hashes.add(fhash)\n",
        "                file_counter += 1\n",
        "                if file_counter ==500: return all_data\n",
        "                if file_counter % 10 == 0:\n",
        "                    print(f\"Files loaded: {file_counter}\")\n",
        "                    self.save_processed_hashes()\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing {file_name}: {e}\")\n",
        "\n",
        "        self.save_processed_hashes()\n",
        "        return all_data\n"
      ],
      "metadata": {
        "id": "9LhczUrMPr2g"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Extract the data from the folder"
      ],
      "metadata": {
        "id": "nNTWaDAaJBFf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "DRIVE_PATH = '/content/drive/MyDrive/Test_Output'\n",
        "MODEL_PATH = f'{DRIVE_PATH}/traffic_model.keras'\n",
        "SCALER_PATH = f'{DRIVE_PATH}/scaler.joblib'\n",
        "LE_PATH = f'{DRIVE_PATH}/label_encoder.joblib'\n",
        "PROCESSED_PATH = f'{DRIVE_PATH}/processed.json'\n",
        "\n",
        "os.makedirs(DRIVE_PATH, exist_ok=True)\n",
        "\n",
        "print(\"✅ All output and data will be stored in:\", DRIVE_PATH)\n"
      ],
      "metadata": {
        "id": "CEY68Ww-lOur",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b118a883-289c-412f-e988-c83f12e51fe3"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "✅ All output and data will be stored in: /content/drive/MyDrive/Test_Output\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "main_folder_id = '326383492292'\n",
        "main_folder = client.folder(main_folder_id).get()\n",
        "\n",
        "# Find the \"Parsed Time Series Data\" subfolder\n",
        "parsed_folder = None\n",
        "for item in main_folder.get_items(limit=100):\n",
        "    if item.type == 'folder' and item.name == 'Parsed Time Series Data':\n",
        "        parsed_folder = client.folder(item.id)\n",
        "        break\n",
        "\n",
        "if not parsed_folder:\n",
        "    raise Exception(\"Parsed Time Series Data folder not found.\")"
      ],
      "metadata": {
        "id": "7iXMloUoJA1j"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def debug_scenario_composition(scenario_name='scen3_1rep3', check_all_files=True):\n",
        "    \"\"\"\n",
        "    Debug the vehicle composition across all files in a scenario\n",
        "    \"\"\"\n",
        "    print(f\"🔍 COMPREHENSIVE SCENARIO ANALYSIS: {scenario_name}\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    # Navigate to the specific folder\n",
        "    main_folder = client.folder(main_folder_id)\n",
        "    parsed_folder = None\n",
        "\n",
        "    for item in main_folder.get_items():\n",
        "        if item.name == 'Parsed Time Series Data':\n",
        "            parsed_folder = client.folder(item.id)\n",
        "            break\n",
        "\n",
        "    target_folder = None\n",
        "    for item in parsed_folder.get_items():\n",
        "        if item.name == scenario_name:\n",
        "            target_folder = client.folder(item.id)\n",
        "            break\n",
        "\n",
        "    if not target_folder:\n",
        "        print(f\"❌ Folder {scenario_name} not found\")\n",
        "        return\n",
        "\n",
        "    # Aggregate statistics across all files\n",
        "    total_vehicles = {}\n",
        "    all_vehicle_types = set()\n",
        "    file_count = 0\n",
        "\n",
        "    # Check all CSV files in the folder\n",
        "    for file_item in target_folder.get_items(limit=1000):\n",
        "        if file_item.type == 'file' and file_item.name.endswith('.csv'):\n",
        "            file_count += 1\n",
        "\n",
        "            try:\n",
        "                content = file_item.content()\n",
        "                df = pd.read_csv(io.StringIO(content.decode('utf-8')))\n",
        "\n",
        "                if 'VehTypeName' in df.columns:\n",
        "                    # Count vehicle types in this file\n",
        "                    veh_counts = df['VehTypeName'].value_counts()\n",
        "\n",
        "                    for vtype, count in veh_counts.items():\n",
        "                        vtype_clean = str(vtype).strip()\n",
        "                        all_vehicle_types.add(vtype_clean)\n",
        "\n",
        "                        if vtype_clean not in total_vehicles:\n",
        "                            total_vehicles[vtype_clean] = 0\n",
        "                        total_vehicles[vtype_clean] += count\n",
        "\n",
        "                if not check_all_files and file_count >= 5:  # Sample first 5 files if not checking all\n",
        "                    break\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"❌ Error reading {file_item.name}: {e}\")\n",
        "\n",
        "    print(f\"\\n📊 Analyzed {file_count} files\")\n",
        "    print(f\"\\n🚗 All Vehicle Types Found:\")\n",
        "    for vtype in sorted(all_vehicle_types):\n",
        "        print(f\"   '{vtype}'\")\n",
        "\n",
        "    # Calculate percentages\n",
        "    grand_total = sum(total_vehicles.values())\n",
        "    print(f\"\\n📈 Vehicle Distribution (Total: {grand_total} vehicles):\")\n",
        "    for vtype, count in sorted(total_vehicles.items(), key=lambda x: x[1], reverse=True):\n",
        "        percentage = (count / grand_total) * 100\n",
        "        print(f\"   {vtype}: {count} ({percentage:.1f}%)\")\n",
        "\n",
        "    # Map to behavior categories\n",
        "    behavior_counts = {\n",
        "        'autonomous': 0,\n",
        "        'aggressive': 0,\n",
        "        'cooperative': 0,\n",
        "        'normal': 0\n",
        "    }\n",
        "\n",
        "    for vtype, count in total_vehicles.items():\n",
        "        if 'CAV' in vtype:\n",
        "            behavior_counts['autonomous'] += count\n",
        "        elif 'Aggressive' in vtype:\n",
        "            behavior_counts['aggressive'] += count\n",
        "        elif 'Cooperative' in vtype:\n",
        "            behavior_counts['cooperative'] += count\n",
        "        elif 'Conventional' in vtype:\n",
        "            behavior_counts['normal'] += count\n",
        "\n",
        "\n",
        "    print(f\"\\n🏷️ Behavior Category Distribution:\")\n",
        "    for behavior, count in behavior_counts.items():\n",
        "        percentage = (count / grand_total) * 100 if grand_total > 0 else 0\n",
        "        print(f\"   {behavior}: {count} ({percentage:.1f}%)\")\n",
        "\n",
        "\n",
        "    return all_vehicle_types, total_vehicles\n",
        "\n",
        "# Run the analysis\n",
        "all_types, counts = debug_scenario_composition('scen3_5rep3', check_all_files=True)"
      ],
      "metadata": {
        "id": "qLjkJ6D8_6iY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b404ec4b-8b1d-44ca-ba3e-614960f5f352"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔍 COMPREHENSIVE SCENARIO ANALYSIS: scen3_5rep3\n",
            "================================================================================\n",
            "\n",
            "📊 Analyzed 61 files\n",
            "\n",
            "🚗 All Vehicle Types Found:\n",
            "   'CAV'\n",
            "   'HDV Aggressive'\n",
            "   'HDV Conventional Gipps Model'\n",
            "   'HDV Cooperative'\n",
            "\n",
            "📈 Vehicle Distribution (Total: 35133 vehicles):\n",
            "   HDV Conventional Gipps Model: 14028 (39.9%)\n",
            "   HDV Cooperative: 8891 (25.3%)\n",
            "   HDV Aggressive: 8671 (24.7%)\n",
            "   CAV: 3543 (10.1%)\n",
            "\n",
            "🏷️ Behavior Category Distribution:\n",
            "   autonomous: 3543 (10.1%)\n",
            "   aggressive: 8671 (24.7%)\n",
            "   cooperative: 8891 (25.3%)\n",
            "   normal: 14028 (39.9%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# RATE-LIMITED ULTRA TRAINER - RESPECTS BOX API LIMITS\n",
        "# Fixed version that won't hit rate limits\n",
        "\n",
        "import os\n",
        "import time\n",
        "import gc\n",
        "import pickle\n",
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import io\n",
        "import threading\n",
        "import random\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "from typing import List, Tuple, Optional\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# ====== GPU MEMORY MANAGEMENT ======\n",
        "print(\"🔧 Setting up GPU memory management...\")\n",
        "import tensorflow as tf\n",
        "try:\n",
        "    gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "    if gpus:\n",
        "        tf.config.experimental.set_memory_growth(gpus[0], True)\n",
        "        print(f\"✅ GPU memory growth enabled: {gpus[0]}\")\n",
        "    else:\n",
        "        print(\"⚠️ No GPU detected - using CPU\")\n",
        "except Exception as e:\n",
        "    print(f\"⚠️ GPU setup warning: {e}\")\n",
        "\n",
        "# ====== RATE LIMITER CLASS ======\n",
        "class BoxAPIRateLimiter:\n",
        "    \"\"\"\n",
        "    Handles Box API rate limiting and retry logic\n",
        "    \"\"\"\n",
        "    def __init__(self, max_requests_per_second=8, max_retries=5):\n",
        "        self.max_requests_per_second = max_requests_per_second\n",
        "        self.max_retries = max_retries\n",
        "        self.last_request_time = 0\n",
        "        self.request_count = 0\n",
        "        self.window_start = time.time()\n",
        "        self.lock = threading.Lock()\n",
        "\n",
        "    def wait_if_needed(self):\n",
        "        \"\"\"Wait if we're approaching rate limits\"\"\"\n",
        "        with self.lock:\n",
        "            current_time = time.time()\n",
        "\n",
        "            # Reset counter every second\n",
        "            if current_time - self.window_start >= 1.0:\n",
        "                self.request_count = 0\n",
        "                self.window_start = current_time\n",
        "\n",
        "            # If we're at the limit, wait\n",
        "            if self.request_count >= self.max_requests_per_second:\n",
        "                sleep_time = 1.0 - (current_time - self.window_start)\n",
        "                if sleep_time > 0:\n",
        "                    time.sleep(sleep_time)\n",
        "                self.request_count = 0\n",
        "                self.window_start = time.time()\n",
        "\n",
        "            self.request_count += 1\n",
        "\n",
        "    def execute_with_retry(self, func, *args, **kwargs):\n",
        "        \"\"\"Execute function with automatic retry on rate limit\"\"\"\n",
        "        for attempt in range(self.max_retries):\n",
        "            try:\n",
        "                self.wait_if_needed()\n",
        "                return func(*args, **kwargs)\n",
        "\n",
        "            except Exception as e:\n",
        "                error_str = str(e).lower()\n",
        "\n",
        "                # Check if it's a rate limit error\n",
        "                if '429' in error_str or 'rate limit' in error_str or 'too many requests' in error_str:\n",
        "                    wait_time = 2 ** attempt + random.uniform(0, 1)  # Exponential backoff\n",
        "                    print(f\"⚠️ Rate limited (attempt {attempt + 1}), waiting {wait_time:.1f}s...\")\n",
        "                    time.sleep(wait_time)\n",
        "                    continue\n",
        "                else:\n",
        "                    # Not a rate limit error, re-raise\n",
        "                    raise e\n",
        "\n",
        "        # All retries exhausted\n",
        "        raise Exception(f\"Max retries ({self.max_retries}) exceeded for Box API call\")\n",
        "\n",
        "# Global rate limiter\n",
        "rate_limiter = BoxAPIRateLimiter(max_requests_per_second=6)  # Conservative limit\n",
        "\n",
        "# ====== RUNTIME KEEP-ALIVE SYSTEM ======\n",
        "class RuntimeKeepAlive:\n",
        "    def __init__(self):\n",
        "        self.running = False\n",
        "        self.thread = None\n",
        "\n",
        "    def start(self):\n",
        "        if not self.running:\n",
        "            self.running = True\n",
        "            self.thread = threading.Thread(target=self._keep_alive, daemon=True)\n",
        "            self.thread.start()\n",
        "            print(\"🔄 Runtime keep-alive started\")\n",
        "\n",
        "    def stop(self):\n",
        "        self.running = False\n",
        "        print(\"🛑 Runtime keep-alive stopped\")\n",
        "\n",
        "    def _keep_alive(self):\n",
        "        while self.running:\n",
        "            time.sleep(1800)  # 30 minutes\n",
        "            if self.running:\n",
        "                print(f\"💓 Runtime alive at {time.strftime('%H:%M:%S')}\")\n",
        "\n",
        "# Start keep-alive\n",
        "keep_alive = RuntimeKeepAlive()\n",
        "keep_alive.start()\n",
        "\n",
        "# ====== PROGRESS MONITOR ======\n",
        "class ProgressMonitor:\n",
        "    def __init__(self, checkpoint_dir):\n",
        "        self.checkpoint_dir = checkpoint_dir\n",
        "        self.progress_file = f\"{checkpoint_dir}/progress.json\"\n",
        "\n",
        "    def update(self, **kwargs):\n",
        "        try:\n",
        "            progress = {\n",
        "                'timestamp': time.time(),\n",
        "                'time_str': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
        "                **kwargs\n",
        "            }\n",
        "\n",
        "            with open(self.progress_file, 'w') as f:\n",
        "                json.dump(progress, f, indent=2)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"⚠️ Progress update failed: {e}\")\n",
        "\n",
        "    def show(self):\n",
        "        try:\n",
        "            if os.path.exists(self.progress_file):\n",
        "                with open(self.progress_file, 'r') as f:\n",
        "                    progress = json.load(f)\n",
        "\n",
        "                print(f\"📊 CURRENT PROGRESS:\")\n",
        "                print(f\"   Time: {progress.get('time_str', 'Unknown')}\")\n",
        "                print(f\"   Files processed: {progress.get('files_processed', 0):,}\")\n",
        "                print(f\"   Samples collected: {progress.get('samples_collected', 0):,}\")\n",
        "                print(f\"   Current batch: {progress.get('batch_num', 0)}\")\n",
        "                print(f\"   Phase: {progress.get('phase', 'Unknown')}\")\n",
        "                return progress\n",
        "            else:\n",
        "                print(\"📊 No progress data found\")\n",
        "                return {}\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Error reading progress: {e}\")\n",
        "            return {}\n",
        "\n",
        "# ====== RATE-LIMITED TRAINER ======\n",
        "class RateLimitedTrainer:\n",
        "    \"\"\"\n",
        "    Trainer that respects Box API rate limits\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, client, main_folder_id, checkpoint_dir='/content/drive/MyDrive/Training_Checkpoints'):\n",
        "        self.client = client\n",
        "        self.main_folder_id = main_folder_id\n",
        "        self.checkpoint_dir = checkpoint_dir\n",
        "\n",
        "        # RATE-LIMITED SETTINGS (Respects API limits)\n",
        "        self.batch_size = 100           # Smaller batches to reduce API calls\n",
        "        self.max_workers = 4            # Fewer workers to avoid overwhelming API\n",
        "        self.checkpoint_interval = 500   # Checkpoint more frequently\n",
        "        self.memory_cleanup_interval = 300\n",
        "        self.max_rows_per_file = 500    # Balance between speed and data quality\n",
        "        self.max_sequence_length = 50   # Good balance\n",
        "\n",
        "        # Rate limiting\n",
        "        self.rate_limiter = rate_limiter\n",
        "\n",
        "        # Create directories\n",
        "        os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "\n",
        "        # File paths\n",
        "        self.processed_files_log = f\"{checkpoint_dir}/processed_files.json\"\n",
        "        self.features_checkpoint = f\"{checkpoint_dir}/features_checkpoint.pkl\"\n",
        "        self.progress_monitor = ProgressMonitor(checkpoint_dir)\n",
        "\n",
        "        print(f\"🚀 Rate-Limited Trainer initialized\")\n",
        "        print(f\"   Batch size: {self.batch_size} (API-friendly)\")\n",
        "        print(f\"   Workers: {self.max_workers} (rate-limited)\")\n",
        "        print(f\"   API rate limit: 6 requests/second\")\n",
        "        print(f\"   Retry logic: Enabled\")\n",
        "\n",
        "    def save_checkpoint(self, processed_files, all_features, all_labels, batch_num):\n",
        "        \"\"\"Save training progress\"\"\"\n",
        "        try:\n",
        "            print(f\"💾 Saving checkpoint at batch {batch_num}...\")\n",
        "\n",
        "            with open(self.processed_files_log, 'w') as f:\n",
        "                json.dump(processed_files, f)\n",
        "\n",
        "            checkpoint_data = {\n",
        "                'features': all_features,\n",
        "                'labels': all_labels,\n",
        "                'batch_num': batch_num,\n",
        "                'timestamp': time.time()\n",
        "            }\n",
        "\n",
        "            with open(self.features_checkpoint, 'wb') as f:\n",
        "                pickle.dump(checkpoint_data, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "            self.progress_monitor.update(\n",
        "                batch_num=batch_num,\n",
        "                files_processed=len(processed_files),\n",
        "                samples_collected=len(all_features),\n",
        "                phase='data_processing'\n",
        "            )\n",
        "\n",
        "            print(f\"✅ Checkpoint saved: {len(processed_files):,} files, {len(all_features):,} samples\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Checkpoint save failed: {e}\")\n",
        "\n",
        "    def load_checkpoint(self):\n",
        "        \"\"\"Load previous training progress\"\"\"\n",
        "        print(\"🔄 Checking for previous checkpoints...\")\n",
        "\n",
        "        if not os.path.exists(self.processed_files_log):\n",
        "            print(\"No previous checkpoint found. Starting fresh.\")\n",
        "            return [], [], [], 0\n",
        "\n",
        "        try:\n",
        "            with open(self.processed_files_log, 'r') as f:\n",
        "                processed_files = json.load(f)\n",
        "\n",
        "            with open(self.features_checkpoint, 'rb') as f:\n",
        "                checkpoint_data = pickle.load(f)\n",
        "\n",
        "            print(f\"📚 Checkpoint loaded!\")\n",
        "            print(f\"   Files processed: {len(processed_files):,}\")\n",
        "            print(f\"   Samples: {len(checkpoint_data['features']):,}\")\n",
        "            print(f\"   Last batch: {checkpoint_data['batch_num']}\")\n",
        "\n",
        "            return (processed_files, checkpoint_data['features'],\n",
        "                   checkpoint_data['labels'], checkpoint_data['batch_num'])\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Error loading checkpoint: {e}\")\n",
        "            return [], [], [], 0\n",
        "\n",
        "    def get_csv_files_rate_limited(self, max_files=80000):\n",
        "        \"\"\"Rate-limited CSV discovery with rep1/rep2/rep3 filtering\"\"\"\n",
        "        print(f\"📡 RATE-LIMITED CSV DISCOVERY (max {max_files:,} files)\")\n",
        "        print(\"🎯 FILTERING: Only rep1, rep2, rep3 scenarios\")\n",
        "        print(\"=\" * 60)\n",
        "\n",
        "        self.progress_monitor.update(phase='file_discovery', files_target=max_files)\n",
        "\n",
        "        # Check cache\n",
        "        \"\"\"\n",
        "        cache_file = f\"{self.checkpoint_dir}/csv_files_cache_filtered.json\"\n",
        "\n",
        "        if os.path.exists(cache_file):\n",
        "            print(\"📁 Loading cached filtered file list...\")\n",
        "            try:\n",
        "                with open(cache_file, 'r') as f:\n",
        "                    cached_files = json.load(f)\n",
        "\n",
        "                if len(cached_files) >= max_files:\n",
        "                    print(f\"✅ Using {len(cached_files[:max_files]):,} files from cache (rep1/rep2/rep3 only)\")\n",
        "                    return cached_files[:max_files]\n",
        "            except:\n",
        "                print(\"Cache corrupted, rescanning...\")\n",
        "\n",
        "\n",
        "        \"\"\"\n",
        "        start_time = time.time()\n",
        "        # Navigate with rate limiting\n",
        "        def get_folder_items_safe(folder, limit=1000):\n",
        "            \"\"\"Get folder items with rate limiting\"\"\"\n",
        "            return self.rate_limiter.execute_with_retry(\n",
        "                lambda: list(folder.get_items(limit=limit))\n",
        "            )\n",
        "\n",
        "        # Get main folder\n",
        "        main_folder = self.client.folder(self.main_folder_id)\n",
        "\n",
        "        # Find Parsed Time Series Data\n",
        "        print(\"🔍 Finding Parsed Time Series Data folder...\")\n",
        "        parsed_folder = None\n",
        "\n",
        "        main_items = get_folder_items_safe(main_folder, limit=100)\n",
        "        for item in main_items:\n",
        "            if item.name == 'Parsed Time Series Data':\n",
        "                parsed_folder = self.client.folder(item.id)\n",
        "                break\n",
        "\n",
        "        if not parsed_folder:\n",
        "            print(\"❌ Parsed Time Series Data folder not found\")\n",
        "            return []\n",
        "\n",
        "        # Get scenario folders with rate limiting and filtering\n",
        "        print(\"📁 Getting scenario folders (filtering for rep1/rep2/rep3)...\")\n",
        "        scenario_folders = []\n",
        "        filtered_out_folders = []\n",
        "\n",
        "        parsed_items = get_folder_items_safe(parsed_folder, limit=500)\n",
        "        for item in parsed_items:\n",
        "            if item.type == 'folder':\n",
        "                folder_name = item.name\n",
        "\n",
        "                # FILTER: Only include rep1, rep2, rep3\n",
        "                if (folder_name.endswith('rep1') or\n",
        "                    folder_name.endswith('rep2') or\n",
        "                    folder_name.endswith('rep3')):\n",
        "                    scenario_folders.append((folder_name, item.id))\n",
        "                    print(f\"   ✅ Including: {folder_name}\")\n",
        "                else:\n",
        "                    filtered_out_folders.append(folder_name)\n",
        "                    print(f\"   ❌ Filtering out: {folder_name}\")\n",
        "\n",
        "        print(f\"\\n📊 Filtering results:\")\n",
        "        print(f\"   ✅ Included scenarios: {len(scenario_folders)}\")\n",
        "        print(f\"   ❌ Filtered out scenarios: {len(filtered_out_folders)}\")\n",
        "\n",
        "        if filtered_out_folders:\n",
        "            print(f\"   📋 Filtered out scenarios:\")\n",
        "            for folder in filtered_out_folders[:10]:  # Show first 10\n",
        "                print(f\"      - {folder}\")\n",
        "            if len(filtered_out_folders) > 10:\n",
        "                print(f\"      ... and {len(filtered_out_folders) - 10} more\")\n",
        "\n",
        "        if not scenario_folders:\n",
        "            print(\"❌ No rep1/rep2/rep3 scenarios found!\")\n",
        "            return []\n",
        "\n",
        "        # Rate-limited folder scanning\n",
        "        all_csv_files = []\n",
        "\n",
        "        def scan_folder_safe(folder_info):\n",
        "            \"\"\"Scan folder with rate limiting\"\"\"\n",
        "            folder_name, folder_id = folder_info\n",
        "            csv_files = []\n",
        "\n",
        "            try:\n",
        "                folder = self.client.folder(folder_id)\n",
        "\n",
        "                # Rate-limited file listing\n",
        "                items = self.rate_limiter.execute_with_retry(\n",
        "                    lambda: list(folder.get_items(limit=2000))\n",
        "                )\n",
        "\n",
        "                for item in items:\n",
        "                    if item.type == 'file' and item.name.endswith('.csv'):\n",
        "                        csv_files.append({\n",
        "                            'name': item.name,\n",
        "                            'id': item.id,\n",
        "                            'scenario': folder_name\n",
        "                        })\n",
        "\n",
        "                print(f\"   📊 {folder_name}: {len(csv_files)} CSV files\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"⚠️ Folder {folder_name} failed: {e}\")\n",
        "\n",
        "            return csv_files\n",
        "\n",
        "        # Sequential processing to respect rate limits\n",
        "        print(f\"\\n📄 Scanning {len(scenario_folders)} filtered folders for CSV files...\")\n",
        "        for folder_info in scenario_folders:\n",
        "            csv_files = scan_folder_safe(folder_info)\n",
        "            all_csv_files.extend(csv_files)\n",
        "\n",
        "            if len(all_csv_files) % 1000 == 0:\n",
        "                elapsed = time.time() - start_time\n",
        "                print(f\"   Found {len(all_csv_files):,} CSV files ({elapsed:.1f}s)\")\n",
        "\n",
        "            if len(all_csv_files) >= max_files:\n",
        "                break\n",
        "\n",
        "            # Small delay between folders to be extra safe\n",
        "            time.sleep(0.1)\n",
        "\n",
        "        # Analyze scenario distribution\n",
        "        scenario_counts = {}\n",
        "        for file_info in all_csv_files:\n",
        "            scenario = file_info['scenario']\n",
        "            scenario_counts[scenario] = scenario_counts.get(scenario, 0) + 1\n",
        "\n",
        "        print(f\"\\n📊 CSV files by scenario (rep1/rep2/rep3 only):\")\n",
        "        for scenario, count in sorted(scenario_counts.items()):\n",
        "            print(f\"   {scenario}: {count:,} files\")\n",
        "\n",
        "        # Cache results\n",
        "        try:\n",
        "            with open(cache_file, 'w') as f:\n",
        "                json.dump(all_csv_files[:max_files], f)\n",
        "            print(f\"💾 Cached {len(all_csv_files):,} filtered files\")\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "        elapsed = time.time() - start_time\n",
        "        print(f\"\\n✅ Filtered discovery complete:\")\n",
        "        print(f\"   Total CSV files: {len(all_csv_files):,}\")\n",
        "        print(f\"   Only rep1/rep2/rep3: ✅\")\n",
        "        print(f\"   Discovery time: {elapsed:.1f}s\")\n",
        "\n",
        "        return all_csv_files[:max_files]\n",
        "\n",
        "    def process_single_file_safe(self, file_info):\n",
        "        \"\"\"Process single file with rate limiting\"\"\"\n",
        "        try:\n",
        "            file_id = file_info['id']\n",
        "            filename = file_info['name']\n",
        "\n",
        "            # Rate-limited file download\n",
        "            def download_file():\n",
        "                file_obj = self.client.file(file_id)\n",
        "                return file_obj.content()\n",
        "\n",
        "            content = self.rate_limiter.execute_with_retry(download_file)\n",
        "\n",
        "            # Fast CSV parsing\n",
        "            df = pd.read_csv(\n",
        "                io.StringIO(content.decode('utf-8')),\n",
        "                usecols=[ 'VehTypeName', 'Speeds', 'VehFrontCoords'],\n",
        "                nrows=self.max_rows_per_file,\n",
        "                dtype={'VehTypeName': 'str'},\n",
        "                low_memory=False,\n",
        "                engine='c'\n",
        "            )\n",
        "\n",
        "\n",
        "            # DEBUG: Check what vehicle types we have\n",
        "            if self.debug_first_file and filename == self.debug_first_file:\n",
        "                print(f\"\\n🔍 DEBUG: Analyzing {filename}\")\n",
        "                print(f\"Total rows in file: {len(df)}\")\n",
        "                print(f\"Unique VehTypeName values:\")\n",
        "                for vtype in df['VehTypeName'].unique():\n",
        "                    print(f\"  - '{vtype}'\")\n",
        "                self.debug_first_file = None  # Only debug once\n",
        "\n",
        "            if len(df) == 0:\n",
        "                return [], [], filename\n",
        "\n",
        "            # Fast label mapping\n",
        "            def fast_map(veh_type):\n",
        "                s = str(veh_type)\n",
        "                if 'CAV' in s: return 'autonomous'\n",
        "                elif 'Aggressive' in s: return 'aggressive'\n",
        "                elif 'Cooperative' in s: return 'cooperative'\n",
        "                elif 'Conventional' in s: return 'normal'\n",
        "\n",
        "            df['behavior_label'] = df['VehTypeName'].apply(fast_map)\n",
        "\n",
        "\n",
        "            # DEBUG: Check label distribution after mapping\n",
        "            if not hasattr(self, 'file_count'):\n",
        "                self.file_count = 0\n",
        "            self.file_count += 1\n",
        "\n",
        "            if self.file_count <= 5:  # Debug first 5 files\n",
        "                print(f\"\\n📊 File {self.file_count}: {filename}\")\n",
        "                label_counts = df['behavior_label'].value_counts()\n",
        "                print(f\"Labels after mapping:\")\n",
        "                for label, count in label_counts.items():\n",
        "                    print(f\"  {label}: {count}\")\n",
        "\n",
        "            # Filter and process\n",
        "            valid_df = df[\n",
        "                (df['behavior_label'] != 'autonomous') &\n",
        "                (df['Speeds'].notna()) &\n",
        "                (df['VehFrontCoords'].notna())\n",
        "            ]\n",
        "\n",
        "\n",
        "            # DEBUG: Check what happened after filtering\n",
        "            if self.file_count <= 5:\n",
        "                if len(valid_df) > 0:\n",
        "                    valid_label_counts = valid_df['behavior_label'].value_counts()\n",
        "                    print(f\"Labels after filtering:\")\n",
        "                    for label, count in valid_label_counts.items():\n",
        "                        print(f\"  {label}: {count}\")\n",
        "                else:\n",
        "                    print(f\"  No valid rows after filtering!\")\n",
        "\n",
        "            # Check if we lost any normal vehicles\n",
        "            if 'normal' in label_counts and ('normal' not in valid_label_counts or valid_label_counts.get('normal', 0) == 0):\n",
        "                print(f\"  ⚠️ WARNING: Lost all 'normal' vehicles during filtering!\")\n",
        "\n",
        "            if len(valid_df) == 0:\n",
        "                return [], [], filename\n",
        "\n",
        "            # Feature extraction\n",
        "            features_list = []\n",
        "            labels_list = []\n",
        "\n",
        "            for _, row in valid_df.iterrows():\n",
        "                try:\n",
        "                    speeds_str = str(row['Speeds']).replace('inf', '0').replace('nan', '0').replace('-inf', '0')\n",
        "                    coords_str = str(row['VehFrontCoords']).replace('inf', '0').replace('nan', '0').replace('-inf', '0')\n",
        "\n",
        "                    speeds = eval(speeds_str)\n",
        "                    coords = eval(coords_str)\n",
        "\n",
        "                    if len(speeds) >= 5 and len(coords) >= 5:\n",
        "                        max_len = self.max_sequence_length\n",
        "                        speeds = speeds[:max_len]\n",
        "                        coords = coords[:max_len]\n",
        "\n",
        "                        min_len = min(len(speeds), len(coords))\n",
        "\n",
        "                        features = {\n",
        "                            'speeds': np.array(speeds[:min_len], dtype=np.float32).reshape(-1, 1),\n",
        "                            'positions': np.array(coords[:min_len], dtype=np.float32),\n",
        "                            'sequence_length': min_len\n",
        "                        }\n",
        "\n",
        "                        features_list.append(features)\n",
        "                        labels_list.append(row['behavior_label'])\n",
        "\n",
        "                except:\n",
        "                    continue\n",
        "\n",
        "            return features_list, labels_list, filename\n",
        "\n",
        "        except Exception as e:\n",
        "            return [], [], file_info.get('name', 'unknown')\n",
        "\n",
        "    def process_files_rate_limited(self, csv_files):\n",
        "        \"\"\"Process files with proper rate limiting\"\"\"\n",
        "        print(f\"📡 RATE-LIMITED PROCESSING {len(csv_files):,} files\")\n",
        "        print(f\"Workers: {self.max_workers} (API-safe)\")\n",
        "        print(\"=\" * 60)\n",
        "\n",
        "        # Load checkpoint\n",
        "        processed_files, all_features, all_labels, start_batch = self.load_checkpoint()\n",
        "        processed_files_set = set(processed_files)\n",
        "\n",
        "        # Filter remaining files\n",
        "        remaining_files = [f for f in csv_files if f['name'] not in processed_files_set]\n",
        "\n",
        "        if not remaining_files:\n",
        "            print(\"✅ All files already processed!\")\n",
        "            return all_features, all_labels\n",
        "\n",
        "        print(f\"📊 Processing {len(remaining_files):,} remaining files\")\n",
        "\n",
        "        batch_size = self.batch_size\n",
        "        total_batches = (len(remaining_files) + batch_size - 1) // batch_size\n",
        "\n",
        "        start_time = time.time()\n",
        "        files_processed = len(processed_files)\n",
        "\n",
        "        for batch_idx in range(start_batch, total_batches):\n",
        "            batch_start = batch_idx * batch_size\n",
        "            batch_end = min(batch_start + batch_size, len(remaining_files))\n",
        "            batch_files = remaining_files[batch_start:batch_end]\n",
        "\n",
        "            print(f\"\\n📦 RATE-LIMITED BATCH {batch_idx + 1}/{total_batches}\")\n",
        "            print(f\"   Files {batch_start + 1:,}-{batch_end:,}\")\n",
        "\n",
        "            batch_start_time = time.time()\n",
        "\n",
        "            # Rate-limited parallel processing\n",
        "            with ThreadPoolExecutor(max_workers=self.max_workers) as executor:\n",
        "                future_to_file = {\n",
        "                    executor.submit(self.process_single_file_safe, file_info): file_info\n",
        "                    for file_info in batch_files\n",
        "                }\n",
        "\n",
        "                batch_features = []\n",
        "                batch_labels = []\n",
        "                batch_processed = []\n",
        "\n",
        "                for future in as_completed(future_to_file):\n",
        "                    try:\n",
        "                        features_list, labels_list, filename = future.result(timeout=60)\n",
        "\n",
        "                        if features_list:\n",
        "                            batch_features.extend(features_list)\n",
        "                            batch_labels.extend(labels_list)\n",
        "\n",
        "                        batch_processed.append(filename)\n",
        "                        files_processed += 1\n",
        "\n",
        "                    except Exception as e:\n",
        "                        print(f\"⚠️ File processing failed: {e}\")\n",
        "                        continue\n",
        "\n",
        "            # Add batch results\n",
        "            all_features.extend(batch_features)\n",
        "            all_labels.extend(batch_labels)\n",
        "            processed_files.extend(batch_processed)\n",
        "\n",
        "            batch_time = time.time() - batch_start_time\n",
        "            total_time = time.time() - start_time\n",
        "\n",
        "            # Progress stats\n",
        "            rate = files_processed / total_time if total_time > 0 else 0\n",
        "            eta = (len(remaining_files) - files_processed) / rate if rate > 0 else 0\n",
        "\n",
        "            print(f\"   ✅ Batch complete: {len(batch_features):,} samples ({batch_time:.1f}s)\")\n",
        "            print(f\"   📊 Progress: {files_processed:,}/{len(remaining_files):,} files\")\n",
        "            print(f\"   📡 Rate: {rate:.1f} files/sec (API-safe)\")\n",
        "            print(f\"   ⏱️ ETA: {eta/60:.1f} min\")\n",
        "            print(f\"   💾 Total samples: {len(all_features):,}\")\n",
        "\n",
        "            # Update progress\n",
        "            self.progress_monitor.update(\n",
        "                batch_num=batch_idx + 1,\n",
        "                files_processed=files_processed,\n",
        "                samples_collected=len(all_features),\n",
        "                processing_rate=rate,\n",
        "                eta_minutes=eta/60\n",
        "            )\n",
        "\n",
        "            # Checkpoint more frequently\n",
        "            if (batch_idx + 1) % (self.checkpoint_interval // batch_size) == 0:\n",
        "                self.save_checkpoint(processed_files, all_features, all_labels, batch_idx + 1)\n",
        "\n",
        "            # Memory cleanup\n",
        "            if (batch_idx + 1) % (self.memory_cleanup_interval // batch_size) == 0:\n",
        "                print(\"🧹 Memory cleanup...\")\n",
        "                del batch_features, batch_labels\n",
        "                gc.collect()\n",
        "\n",
        "        # Final checkpoint\n",
        "        self.save_checkpoint(processed_files, all_features, all_labels, total_batches)\n",
        "\n",
        "        total_time = time.time() - start_time\n",
        "        print(f\"\\n✅ RATE-LIMITED PROCESSING COMPLETE\")\n",
        "        print(f\"   Files processed: {files_processed:,}\")\n",
        "        print(f\"   Total samples: {len(all_features):,}\")\n",
        "        print(f\"   Total time: {total_time:.1f}s ({total_time/60:.1f} min)\")\n",
        "        print(f\"   Safe rate: {files_processed/total_time:.1f} files/sec\")\n",
        "\n",
        "        return all_features, all_labels\n",
        "\n",
        "    def train_with_rate_limits(self, max_files=80000):\n",
        "        \"\"\"Train with proper rate limiting\"\"\"\n",
        "        print(\"📡 RATE-LIMITED 80K FILE TRAINING\")\n",
        "        print(\"=\" * 70)\n",
        "\n",
        "        # Initialize debug flags\n",
        "        self.debug_first_file = True  # Will be set to first filename\n",
        "        self.file_count = 0\n",
        "        self.unexpected_types = set()\n",
        "\n",
        "\n",
        "\n",
        "        training_start = time.time()\n",
        "\n",
        "        self.progress_monitor.update(phase='starting', max_files=max_files)\n",
        "\n",
        "        # Step 1: Rate-limited file discovery\n",
        "        print(\"🔍 Step 1: Rate-limited file discovery...\")\n",
        "        csv_files = self.get_csv_files_rate_limited(max_files=max_files)\n",
        "\n",
        "        if not csv_files:\n",
        "            print(\"❌ No CSV files found\")\n",
        "            return None\n",
        "\n",
        "        print(f\"🎯 Will process {len(csv_files):,} files with rate limiting\")\n",
        "\n",
        "        # Step 2: Rate-limited processing\n",
        "        print(\"\\n📡 Step 2: Rate-limited processing...\")\n",
        "        all_features, all_labels = self.process_files_rate_limited(csv_files)\n",
        "\n",
        "        if not all_features:\n",
        "            print(\"❌ No features extracted\")\n",
        "            return None\n",
        "\n",
        "\n",
        "        # Step 3: Data analysis\n",
        "        print(f\"\\n📊 Step 3: Analyzing dataset...\")\n",
        "        from collections import Counter\n",
        "        label_distribution = Counter(all_labels)\n",
        "\n",
        "        print(f\"RATE-LIMITED DATASET SUMMARY:\")\n",
        "        print(f\"   Files processed: {len(csv_files):,}\")\n",
        "        print(f\"   Total vehicles: {len(all_features):,}\")\n",
        "        print(f\"   Label distribution:\")\n",
        "        for label, count in label_distribution.items():\n",
        "            percentage = (count / len(all_labels)) * 100\n",
        "            print(f\"     {label}: {count:,} ({percentage:.1f}%)\")\n",
        "\n",
        "        self.progress_monitor.update(\n",
        "            phase='data_preparation',\n",
        "            total_samples=len(all_features),\n",
        "            label_distribution=dict(label_distribution)\n",
        "        )\n",
        "\n",
        "        # Step 4: Data preparation\n",
        "        print(f\"\\n🔧 Step 4: Data preparation...\")\n",
        "\n",
        "        def prepare_data_safe(features_list, max_sequence=50):\n",
        "            total_samples = len(features_list)\n",
        "\n",
        "            X_speed = np.zeros((total_samples, max_sequence, 1), dtype=np.float32)\n",
        "            X_pos = np.zeros((total_samples, max_sequence, 2), dtype=np.float32)\n",
        "\n",
        "            for i, features in enumerate(features_list):\n",
        "                try:\n",
        "                    speeds = features['speeds'][:max_sequence]\n",
        "                    positions = features['positions'][:max_sequence]\n",
        "                    seq_len = min(len(speeds), len(positions), max_sequence)\n",
        "\n",
        "                    X_speed[i, :seq_len, 0] = speeds[:seq_len, 0]\n",
        "                    X_pos[i, :seq_len, :] = positions[:seq_len, :]\n",
        "\n",
        "                except:\n",
        "                    continue\n",
        "\n",
        "                if i % 10000 == 0:\n",
        "                    print(f\"   Processed {i:,}/{total_samples:,} samples\")\n",
        "\n",
        "            return X_speed, X_pos\n",
        "\n",
        "        X_speed, X_pos = prepare_data_safe(all_features, max_sequence=self.max_sequence_length)\n",
        "\n",
        "        # Clean up\n",
        "        del all_features\n",
        "        gc.collect()\n",
        "\n",
        "        print(f\"✅ Data prepared: {X_speed.shape[0]:,} samples\")\n",
        "\n",
        "        # Continue with training (same as before)\n",
        "        split_idx = int(0.8 * len(all_labels))\n",
        "\n",
        "        X_speed_train = X_speed[:split_idx]\n",
        "        X_speed_test = X_speed[split_idx:]\n",
        "        X_pos_train = X_pos[:split_idx]\n",
        "        X_pos_test = X_pos[split_idx:]\n",
        "\n",
        "        train_labels = all_labels[:split_idx]\n",
        "        test_labels = all_labels[split_idx:]\n",
        "\n",
        "        print(f\"Train samples: {len(train_labels):,}\")\n",
        "        print(f\"Test samples: {len(test_labels):,}\")\n",
        "\n",
        "        # Label encoding\n",
        "        from sklearn.preprocessing import LabelEncoder\n",
        "        label_encoder = LabelEncoder()\n",
        "        label_encoder.fit(all_labels)\n",
        "\n",
        "        y_train = label_encoder.transform(train_labels)\n",
        "        y_test = label_encoder.transform(test_labels)\n",
        "\n",
        "\n",
        "        from collections import Counter\n",
        "        print(f\"Label distribution in all_labels: {Counter(all_labels)}\")\n",
        "        print(f\"Unique labels: {set(all_labels)}\")\n",
        "        print(f\"Total samples: {len(all_labels)}\")\n",
        "\n",
        "        print(f\"Classes: {label_encoder.classes_}\")\n",
        "\n",
        "        self.progress_monitor.update(\n",
        "            phase='model_training',\n",
        "            train_samples=len(y_train),\n",
        "            test_samples=len(y_test),\n",
        "            classes=list(label_encoder.classes_)\n",
        "        )\n",
        "\n",
        "        # Model training\n",
        "        print(f\"\\n🚀 Step 5: Model training...\")\n",
        "\n",
        "        num_classes = len(label_encoder.classes_)\n",
        "        model = build_speed_position_model_fixed(\n",
        "            sequence_length=self.max_sequence_length,\n",
        "            num_classes=num_classes\n",
        "        )\n",
        "\n",
        "        print(f\"Training with rate-limited dataset...\")\n",
        "        print(f\"Training samples: {len(y_train):,}\")\n",
        "\n",
        "        try:\n",
        "            callbacks = [\n",
        "                tf.keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True),\n",
        "                tf.keras.callbacks.ReduceLROnPlateau(factor=0.5, patience=3),\n",
        "            ]\n",
        "\n",
        "            history = model.fit(\n",
        "                [X_speed_train, X_pos_train], y_train,\n",
        "                epochs=20,\n",
        "                batch_size=256,\n",
        "                validation_split=0.15,\n",
        "                callbacks=callbacks,\n",
        "                verbose=1\n",
        "            )\n",
        "\n",
        "            print(\"✅ Training completed!\")\n",
        "\n",
        "            # Evaluation\n",
        "            print(f\"\\n📊 Evaluation...\")\n",
        "\n",
        "            y_pred_probs = model.predict([X_speed_test, X_pos_test], batch_size=512, verbose=0)\n",
        "            y_pred = np.argmax(y_pred_probs, axis=1)\n",
        "\n",
        "            accuracy = np.mean(y_test == y_pred)\n",
        "\n",
        "            from sklearn.metrics import classification_report, f1_score\n",
        "            f1_macro = f1_score(y_test, y_pred, average='macro')\n",
        "\n",
        "            print(f\"RATE-LIMITED RESULTS:\")\n",
        "            print(f\"   Test Accuracy: {accuracy:.3f}\")\n",
        "            print(f\"   F1 Score (macro): {f1_macro:.3f}\")\n",
        "\n",
        "            print(f\"\\nClassification Report:\")\n",
        "            print(classification_report(y_test, y_pred, target_names=label_encoder.classes_))\n",
        "\n",
        "            # Save model\n",
        "            print(f\"\\n💾 Saving model...\")\n",
        "\n",
        "            model.save(f\"{self.checkpoint_dir}/rate_limited_80k_model.keras\")\n",
        "\n",
        "            cl_system.model = model\n",
        "            cl_system.label_encoder = label_encoder\n",
        "            cl_system.save_model()\n",
        "            cl_system.save_label_encoder()\n",
        "\n",
        "            total_time = time.time() - training_start\n",
        "\n",
        "            self.progress_monitor.update(\n",
        "                phase='completed',\n",
        "                total_time=total_time,\n",
        "                final_accuracy=accuracy,\n",
        "                f1_macro=f1_macro\n",
        "            )\n",
        "\n",
        "            print(f\"\\n🎉 RATE-LIMITED TRAINING COMPLETE!\")\n",
        "            print(f\"=\" * 60)\n",
        "            print(f\"Files processed: {len(csv_files):,}\")\n",
        "            print(f\"Total vehicles: {len(all_labels):,}\")\n",
        "            print(f\"Training time: {total_time:.1f}s ({total_time/3600:.1f} hours)\")\n",
        "            print(f\"Final accuracy: {accuracy:.3f}\")\n",
        "            print(f\"API-safe: No rate limit errors\")\n",
        "\n",
        "            return {\n",
        "                'model': model,\n",
        "                'label_encoder': label_encoder,\n",
        "                'history': history,\n",
        "                'accuracy': accuracy,\n",
        "                'f1_macro': f1_macro,\n",
        "                'files_processed': len(csv_files),\n",
        "                'total_vehicles': len(all_labels),\n",
        "                'training_time': total_time,\n",
        "                'label_distribution': label_distribution\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Training failed: {e}\")\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "            return None\n",
        "\n",
        "        # At the end, check how many normal vehicles made it through\n",
        "        normal_extracted = sum(1 for label in labels_list if label == 'normal')\n",
        "        print(f\"   Normal vehicles: {normal_in_valid} in valid_df -> {normal_extracted} extracted\")\n",
        "\n",
        "\n",
        "        # DEBUG: Print summary of what we found\n",
        "        print(f\"\\n🔍 DEBUG SUMMARY:\")\n",
        "        print(f\"Total files processed: {files_processed}\")\n",
        "        print(f\"Total samples collected: {len(all_features)}\")\n",
        "\n",
        "        # Check label distribution\n",
        "        from collections import Counter\n",
        "        label_counts = Counter(all_labels)\n",
        "        print(f\"Final label distribution:\")\n",
        "        for label, count in label_counts.items():\n",
        "            print(f\"  {label}: {count}\")\n",
        "\n",
        "        if hasattr(self, 'unexpected_types') and self.unexpected_types:\n",
        "            print(f\"\\nUnexpected vehicle types found:\")\n",
        "            for vtype in sorted(self.unexpected_types):\n",
        "                print(f\"  - '{vtype}'\")\n",
        "\n",
        "    def debug_process_single_file_ultra_fast(self, file_info):\n",
        "      \"\"\"Debug version with label tracking\"\"\"\n",
        "      try:\n",
        "          file_id = file_info['id']\n",
        "          filename = file_info['name']\n",
        "\n",
        "          # Get file and download\n",
        "          file_obj = self.client.file(file_id)\n",
        "          content = file_obj.content()\n",
        "\n",
        "          # Ultra-fast CSV parsing\n",
        "          df = pd.read_csv(\n",
        "              io.StringIO(content.decode('utf-8')),\n",
        "              usecols=['VehNr', 'VehTypeName', 'Speeds', 'VehFrontCoords'],\n",
        "              nrows=self.max_rows_per_file,\n",
        "              dtype={'VehNr': 'str', 'VehTypeName': 'str'},\n",
        "              low_memory=False,\n",
        "              engine='c'\n",
        "          )\n",
        "\n",
        "          if len(df) == 0:\n",
        "              return [], [], filename, {}\n",
        "\n",
        "          # Debug: Check unique VehTypeName values\n",
        "          unique_types = df['VehTypeName'].unique()\n",
        "\n",
        "          # Ultra-fast label mapping with debug info\n",
        "          def ultra_fast_map(veh_type):\n",
        "              s = str(veh_type)\n",
        "              if 'CAV' in s: return 'autonomous'\n",
        "              elif 'Aggressive' in s: return 'aggressive'\n",
        "              elif 'Cooperative' in s: return 'cooperative'\n",
        "              elif 'Conventional' in s or 'Gipps' in s or 'Normal' in s: return 'normal'\n",
        "              else: return 'normal'\n",
        "\n",
        "          df['behavior_label'] = df['VehTypeName'].apply(ultra_fast_map)\n",
        "\n",
        "          # Debug: Count labels before filtering\n",
        "          label_counts_before = df['behavior_label'].value_counts().to_dict()\n",
        "\n",
        "          # Filter and process\n",
        "          valid_df = df[\n",
        "              (df['behavior_label'] != 'autonomous') &\n",
        "              (df['Speeds'].notna()) &\n",
        "              (df['VehFrontCoords'].notna())\n",
        "          ]\n",
        "\n",
        "          # Debug: Count labels after filtering\n",
        "          label_counts_after = valid_df['behavior_label'].value_counts().to_dict()\n",
        "\n",
        "          debug_info = {\n",
        "              'unique_types': list(unique_types),\n",
        "              'labels_before_filter': label_counts_before,\n",
        "              'labels_after_filter': label_counts_after\n",
        "          }\n",
        "\n",
        "          if len(valid_df) == 0:\n",
        "              return [], [], filename, debug_info\n",
        "\n",
        "          # Rest of processing...\n",
        "          features_list = []\n",
        "          labels_list = []\n",
        "\n",
        "          for _, row in valid_df.iterrows():\n",
        "              try:\n",
        "                  speeds_str = str(row['Speeds']).replace('inf', '0').replace('nan', '0').replace('-inf', '0')\n",
        "                  coords_str = str(row['VehFrontCoords']).replace('inf', '0').replace('nan', '0').replace('-inf', '0')\n",
        "\n",
        "                  speeds = eval(speeds_str)\n",
        "                  coords = eval(coords_str)\n",
        "\n",
        "                  if len(speeds) >= 3 and len(coords) >= 3:\n",
        "                      max_len = self.max_sequence_length\n",
        "                      speeds = speeds[:max_len]\n",
        "                      coords = coords[:max_len]\n",
        "\n",
        "                      min_len = min(len(speeds), len(coords))\n",
        "\n",
        "                      features = {\n",
        "                          'speeds': np.array(speeds[:min_len], dtype=np.float32).reshape(-1, 1),\n",
        "                          'positions': np.array(coords[:min_len], dtype=np.float32),\n",
        "                          'sequence_length': min_len\n",
        "                      }\n",
        "\n",
        "                      features_list.append(features)\n",
        "                      labels_list.append(row['behavior_label'])\n",
        "\n",
        "              except:\n",
        "                  continue\n",
        "\n",
        "          return features_list, labels_list, filename, debug_info\n",
        "\n",
        "      except Exception as e:\n",
        "          print(f\"Error processing file: {e}\")\n",
        "          return [], [], file_info.get('name', 'unknown'), {}\n",
        "# ====== MONITORING FUNCTIONS ======\n",
        "def show_progress():\n",
        "    \"\"\"Show current training progress\"\"\"\n",
        "    try:\n",
        "        monitor = ProgressMonitor('/content/drive/MyDrive/Training_Checkpoints')\n",
        "        return monitor.show()\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error showing progress: {e}\")\n",
        "\n",
        "def resume_training():\n",
        "    \"\"\"Resume training from checkpoint\"\"\"\n",
        "    print(\"🔄 RESUMING RATE-LIMITED TRAINING FROM CHECKPOINT\")\n",
        "    trainer = RateLimitedTrainer(client, main_folder_id)\n",
        "    return trainer.train_with_rate_limits(max_files=80000)\n",
        "\n",
        "# ====== MAIN EXECUTION ======\n",
        "print(\"📡 RATE-LIMITED 80K TRAINER READY\")\n",
        "print(\"=\" * 70)\n",
        "print(\"🔧 Rate limiting features:\")\n",
        "print(\"   ✅ 6 requests/second limit\")\n",
        "print(\"   ✅ Exponential backoff retry\")\n",
        "print(\"   ✅ 4 workers (API-safe)\")\n",
        "print(\"   ✅ 100 files per batch\")\n",
        "print(\"   ✅ Connection pool management\")\n",
        "print(\"   ✅ Auto-retry on 429 errors\")\n",
        "print()\n",
        "print(\"📋 Available commands:\")\n",
        "print(\"   show_progress() - Check current progress\")\n",
        "print(\"   resume_training() - Resume from checkpoint\")\n",
        "\n",
        "# Create and start rate-limited trainer\n",
        "print(\"\\n📡 STARTING RATE-LIMITED 80K FILE TRAINING\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "trainer = RateLimitedTrainer(client, main_folder_id)\n",
        "\n",
        "print(f\"Rate-Limited Configuration:\")\n",
        "print(f\"   Max files: 80,000\")\n",
        "print(f\"   Workers: 4 (API-safe)\")\n",
        "print(f\"   Rate limit: 6 requests/second\")\n",
        "print(f\"   Batch size: 100 files\")\n",
        "print(f\"   Auto-retry: Enabled\")\n",
        "print(f\"   Checkpoints: Every 500 files\")\n",
        "\n",
        "# Start rate-limited training\n",
        "result = trainer.train_with_rate_limits(max_files=80000)\n",
        "\n",
        "if result:\n",
        "    print(f\"\\n🎉 RATE-LIMITED SUCCESS!\")\n",
        "    print(f\"   Files processed: {result['files_processed']:,}\")\n",
        "    print(f\"   Total vehicles: {result['total_vehicles']:,}\")\n",
        "    print(f\"   Training time: {result['training_time']/3600:.1f} hours\")\n",
        "    print(f\"   Final accuracy: {result['accuracy']:.3f}\")\n",
        "    print(f\"   No API errors: ✅\")\n",
        "\n",
        "    # Stop keep-alive\n",
        "    keep_alive.stop()\n",
        "else:\n",
        "    print(f\"\\n❌ Training failed - run resume_training() to continue\")\n",
        "\n",
        "print(f\"\\n💾 All checkpoints saved to: /content/drive/MyDrive/Training_Checkpoints\")\n",
        "print(f\"📡 API-safe trainer - no more 429 errors!\")"
      ],
      "metadata": {
        "id": "jIVGbR0q6eCa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "dde0c3ec-075a-4846-9a18-47e07c51d0bb"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔧 Setting up GPU memory management...\n",
            "✅ GPU memory growth enabled: PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')\n",
            "🔄 Runtime keep-alive started\n",
            "📡 RATE-LIMITED 80K TRAINER READY\n",
            "======================================================================\n",
            "🔧 Rate limiting features:\n",
            "   ✅ 6 requests/second limit\n",
            "   ✅ Exponential backoff retry\n",
            "   ✅ 4 workers (API-safe)\n",
            "   ✅ 100 files per batch\n",
            "   ✅ Connection pool management\n",
            "   ✅ Auto-retry on 429 errors\n",
            "\n",
            "📋 Available commands:\n",
            "   show_progress() - Check current progress\n",
            "   resume_training() - Resume from checkpoint\n",
            "\n",
            "📡 STARTING RATE-LIMITED 80K FILE TRAINING\n",
            "======================================================================\n",
            "🚀 Rate-Limited Trainer initialized\n",
            "   Batch size: 100 (API-friendly)\n",
            "   Workers: 4 (rate-limited)\n",
            "   API rate limit: 6 requests/second\n",
            "   Retry logic: Enabled\n",
            "Rate-Limited Configuration:\n",
            "   Max files: 80,000\n",
            "   Workers: 4 (API-safe)\n",
            "   Rate limit: 6 requests/second\n",
            "   Batch size: 100 files\n",
            "   Auto-retry: Enabled\n",
            "   Checkpoints: Every 500 files\n",
            "📡 RATE-LIMITED 80K FILE TRAINING\n",
            "======================================================================\n",
            "🔍 Step 1: Rate-limited file discovery...\n",
            "📡 RATE-LIMITED CSV DISCOVERY (max 80,000 files)\n",
            "🎯 FILTERING: Only rep1, rep2, rep3 scenarios\n",
            "============================================================\n",
            "🔍 Finding Parsed Time Series Data folder...\n",
            "📁 Getting scenario folders (filtering for rep1/rep2/rep3)...\n",
            "   ✅ Including: scen10_1rep1\n",
            "   ❌ Filtering out: scen10_1rep10\n",
            "   ✅ Including: scen10_1rep2\n",
            "   ✅ Including: scen10_1rep3\n",
            "   ❌ Filtering out: scen10_1rep4\n",
            "   ❌ Filtering out: scen10_1rep5\n",
            "   ❌ Filtering out: scen10_1rep6\n",
            "   ❌ Filtering out: scen10_1rep7\n",
            "   ❌ Filtering out: scen10_1rep8\n",
            "   ❌ Filtering out: scen10_1rep9\n",
            "   ✅ Including: scen10_2rep1\n",
            "   ❌ Filtering out: scen10_2rep10\n",
            "   ✅ Including: scen10_2rep2\n",
            "   ✅ Including: scen10_2rep3\n",
            "   ❌ Filtering out: scen10_2rep4\n",
            "   ❌ Filtering out: scen10_2rep5\n",
            "   ❌ Filtering out: scen10_2rep6\n",
            "   ❌ Filtering out: scen10_2rep7\n",
            "   ❌ Filtering out: scen10_2rep8\n",
            "   ❌ Filtering out: scen10_2rep9\n",
            "   ✅ Including: scen10_3rep1\n",
            "   ❌ Filtering out: scen10_3rep10\n",
            "   ✅ Including: scen10_3rep2\n",
            "   ✅ Including: scen10_3rep3\n",
            "   ❌ Filtering out: scen10_3rep4\n",
            "   ❌ Filtering out: scen10_3rep5\n",
            "   ❌ Filtering out: scen10_3rep6\n",
            "   ❌ Filtering out: scen10_3rep7\n",
            "   ❌ Filtering out: scen10_3rep8\n",
            "   ❌ Filtering out: scen10_3rep9\n",
            "   ✅ Including: scen10_4rep1\n",
            "   ❌ Filtering out: scen10_4rep10\n",
            "   ✅ Including: scen10_4rep2\n",
            "   ✅ Including: scen10_4rep3\n",
            "   ❌ Filtering out: scen10_4rep4\n",
            "   ❌ Filtering out: scen10_4rep5\n",
            "   ❌ Filtering out: scen10_4rep6\n",
            "   ❌ Filtering out: scen10_4rep7\n",
            "   ❌ Filtering out: scen10_4rep8\n",
            "   ❌ Filtering out: scen10_4rep9\n",
            "   ✅ Including: scen10_5rep1\n",
            "   ❌ Filtering out: scen10_5rep10\n",
            "   ✅ Including: scen10_5rep2\n",
            "   ✅ Including: scen10_5rep3\n",
            "   ❌ Filtering out: scen10_5rep4\n",
            "   ❌ Filtering out: scen10_5rep5\n",
            "   ❌ Filtering out: scen10_5rep6\n",
            "   ❌ Filtering out: scen10_5rep7\n",
            "   ❌ Filtering out: scen10_5rep8\n",
            "   ❌ Filtering out: scen10_5rep9\n",
            "   ✅ Including: scen10_6rep1\n",
            "   ❌ Filtering out: scen10_6rep10\n",
            "   ✅ Including: scen10_6rep2\n",
            "   ✅ Including: scen10_6rep3\n",
            "   ❌ Filtering out: scen10_6rep4\n",
            "   ❌ Filtering out: scen10_6rep5\n",
            "   ❌ Filtering out: scen10_6rep6\n",
            "   ❌ Filtering out: scen10_6rep7\n",
            "   ❌ Filtering out: scen10_6rep8\n",
            "   ❌ Filtering out: scen10_6rep9\n",
            "   ✅ Including: scen10_7rep1\n",
            "   ❌ Filtering out: scen10_7rep10\n",
            "   ✅ Including: scen10_7rep2\n",
            "   ✅ Including: scen10_7rep3\n",
            "   ❌ Filtering out: scen10_7rep4\n",
            "   ❌ Filtering out: scen10_7rep5\n",
            "   ❌ Filtering out: scen10_7rep6\n",
            "   ❌ Filtering out: scen10_7rep7\n",
            "   ❌ Filtering out: scen10_7rep8\n",
            "   ❌ Filtering out: scen10_7rep9\n",
            "   ✅ Including: scen11_2rep1\n",
            "   ❌ Filtering out: scen11_2rep10\n",
            "   ✅ Including: scen11_2rep2\n",
            "   ✅ Including: scen11_2rep3\n",
            "   ❌ Filtering out: scen11_2rep4\n",
            "   ❌ Filtering out: scen11_2rep5\n",
            "   ❌ Filtering out: scen11_2rep6\n",
            "   ❌ Filtering out: scen11_2rep7\n",
            "   ❌ Filtering out: scen11_2rep8\n",
            "   ❌ Filtering out: scen11_2rep9\n",
            "   ✅ Including: scen11_3rep1\n",
            "   ❌ Filtering out: scen11_3rep10\n",
            "   ✅ Including: scen11_3rep2\n",
            "   ✅ Including: scen11_3rep3\n",
            "   ❌ Filtering out: scen11_3rep4\n",
            "   ❌ Filtering out: scen11_3rep5\n",
            "   ❌ Filtering out: scen11_3rep6\n",
            "   ❌ Filtering out: scen11_3rep7\n",
            "   ❌ Filtering out: scen11_3rep8\n",
            "   ❌ Filtering out: scen11_3rep9\n",
            "   ✅ Including: scen11_4rep1\n",
            "   ❌ Filtering out: scen11_4rep10\n",
            "   ✅ Including: scen11_4rep2\n",
            "   ✅ Including: scen11_4rep3\n",
            "   ❌ Filtering out: scen11_4rep4\n",
            "   ❌ Filtering out: scen11_4rep5\n",
            "   ❌ Filtering out: scen11_4rep6\n",
            "   ❌ Filtering out: scen11_4rep7\n",
            "   ❌ Filtering out: scen11_4rep8\n",
            "   ❌ Filtering out: scen11_4rep9\n",
            "   ✅ Including: scen11_5rep1\n",
            "   ❌ Filtering out: scen11_5rep10\n",
            "   ✅ Including: scen11_5rep2\n",
            "   ✅ Including: scen11_5rep3\n",
            "   ❌ Filtering out: scen11_5rep4\n",
            "   ❌ Filtering out: scen11_5rep5\n",
            "   ❌ Filtering out: scen11_5rep6\n",
            "   ❌ Filtering out: scen11_5rep7\n",
            "   ❌ Filtering out: scen11_5rep8\n",
            "   ❌ Filtering out: scen11_5rep9\n",
            "   ✅ Including: scen11_6rep1\n",
            "   ❌ Filtering out: scen11_6rep10\n",
            "   ✅ Including: scen11_6rep2\n",
            "   ✅ Including: scen11_6rep3\n",
            "   ❌ Filtering out: scen11_6rep4\n",
            "   ❌ Filtering out: scen11_6rep5\n",
            "   ❌ Filtering out: scen11_6rep6\n",
            "   ❌ Filtering out: scen11_6rep7\n",
            "   ❌ Filtering out: scen11_6rep8\n",
            "   ❌ Filtering out: scen11_6rep9\n",
            "   ✅ Including: scen11_7rep1\n",
            "   ❌ Filtering out: scen11_7rep10\n",
            "   ✅ Including: scen11_7rep2\n",
            "   ✅ Including: scen11_7rep3\n",
            "   ❌ Filtering out: scen11_7rep4\n",
            "   ❌ Filtering out: scen11_7rep5\n",
            "   ❌ Filtering out: scen11_7rep6\n",
            "   ❌ Filtering out: scen11_7rep7\n",
            "   ❌ Filtering out: scen11_7rep8\n",
            "   ❌ Filtering out: scen11_7rep9\n",
            "   ✅ Including: scen12_1rep1\n",
            "   ❌ Filtering out: scen12_1rep10\n",
            "   ✅ Including: scen12_1rep2\n",
            "   ✅ Including: scen12_1rep3\n",
            "   ❌ Filtering out: scen12_1rep4\n",
            "   ❌ Filtering out: scen12_1rep5\n",
            "   ❌ Filtering out: scen12_1rep6\n",
            "   ❌ Filtering out: scen12_1rep7\n",
            "   ❌ Filtering out: scen12_1rep8\n",
            "   ❌ Filtering out: scen12_1rep9\n",
            "   ✅ Including: scen12_2rep1\n",
            "   ❌ Filtering out: scen12_2rep10\n",
            "   ✅ Including: scen12_2rep2\n",
            "   ✅ Including: scen12_2rep3\n",
            "   ❌ Filtering out: scen12_2rep4\n",
            "   ❌ Filtering out: scen12_2rep5\n",
            "   ❌ Filtering out: scen12_2rep6\n",
            "   ❌ Filtering out: scen12_2rep7\n",
            "   ❌ Filtering out: scen12_2rep8\n",
            "   ❌ Filtering out: scen12_2rep9\n",
            "   ✅ Including: scen12_3rep1\n",
            "   ❌ Filtering out: scen12_3rep10\n",
            "   ✅ Including: scen12_3rep2\n",
            "   ✅ Including: scen12_3rep3\n",
            "   ❌ Filtering out: scen12_3rep4\n",
            "   ❌ Filtering out: scen12_3rep5\n",
            "   ❌ Filtering out: scen12_3rep6\n",
            "   ❌ Filtering out: scen12_3rep7\n",
            "   ❌ Filtering out: scen12_3rep8\n",
            "   ❌ Filtering out: scen12_3rep9\n",
            "   ✅ Including: scen12_4rep1\n",
            "   ❌ Filtering out: scen12_4rep10\n",
            "   ✅ Including: scen12_4rep2\n",
            "   ✅ Including: scen12_4rep3\n",
            "   ❌ Filtering out: scen12_4rep4\n",
            "   ❌ Filtering out: scen12_4rep5\n",
            "   ❌ Filtering out: scen12_4rep6\n",
            "   ❌ Filtering out: scen12_4rep7\n",
            "   ❌ Filtering out: scen12_4rep8\n",
            "   ❌ Filtering out: scen12_4rep9\n",
            "   ✅ Including: scen12_5rep1\n",
            "   ❌ Filtering out: scen12_5rep10\n",
            "   ✅ Including: scen12_5rep2\n",
            "   ✅ Including: scen12_5rep3\n",
            "   ❌ Filtering out: scen12_5rep4\n",
            "   ❌ Filtering out: scen12_5rep5\n",
            "   ❌ Filtering out: scen12_5rep6\n",
            "   ❌ Filtering out: scen12_5rep7\n",
            "   ❌ Filtering out: scen12_5rep8\n",
            "   ❌ Filtering out: scen12_5rep9\n",
            "   ✅ Including: scen12_6rep1\n",
            "   ❌ Filtering out: scen12_6rep10\n",
            "   ✅ Including: scen12_6rep2\n",
            "   ✅ Including: scen12_6rep3\n",
            "   ❌ Filtering out: scen12_6rep4\n",
            "   ❌ Filtering out: scen12_6rep5\n",
            "   ❌ Filtering out: scen12_6rep6\n",
            "   ❌ Filtering out: scen12_6rep7\n",
            "   ❌ Filtering out: scen12_6rep8\n",
            "   ❌ Filtering out: scen12_6rep9\n",
            "   ✅ Including: scen13_1rep1\n",
            "   ❌ Filtering out: scen13_1rep10\n",
            "   ✅ Including: scen13_1rep2\n",
            "   ✅ Including: scen13_1rep3\n",
            "   ❌ Filtering out: scen13_1rep4\n",
            "   ❌ Filtering out: scen13_1rep5\n",
            "   ❌ Filtering out: scen13_1rep6\n",
            "   ❌ Filtering out: scen13_1rep7\n",
            "   ❌ Filtering out: scen13_1rep8\n",
            "   ❌ Filtering out: scen13_1rep9\n",
            "   ✅ Including: scen13_2rep1\n",
            "   ❌ Filtering out: scen13_2rep10\n",
            "   ✅ Including: scen13_2rep2\n",
            "   ✅ Including: scen13_2rep3\n",
            "   ❌ Filtering out: scen13_2rep4\n",
            "   ❌ Filtering out: scen13_2rep5\n",
            "   ❌ Filtering out: scen13_2rep6\n",
            "   ❌ Filtering out: scen13_2rep7\n",
            "   ❌ Filtering out: scen13_2rep8\n",
            "   ❌ Filtering out: scen13_2rep9\n",
            "   ✅ Including: scen13_3rep1\n",
            "   ❌ Filtering out: scen13_3rep10\n",
            "   ✅ Including: scen13_3rep2\n",
            "   ✅ Including: scen13_3rep3\n",
            "   ❌ Filtering out: scen13_3rep4\n",
            "   ❌ Filtering out: scen13_3rep5\n",
            "   ❌ Filtering out: scen13_3rep6\n",
            "   ❌ Filtering out: scen13_3rep7\n",
            "   ❌ Filtering out: scen13_3rep8\n",
            "   ❌ Filtering out: scen13_3rep9\n",
            "   ✅ Including: scen13_4rep1\n",
            "   ❌ Filtering out: scen13_4rep10\n",
            "   ✅ Including: scen13_4rep2\n",
            "   ✅ Including: scen13_4rep3\n",
            "   ❌ Filtering out: scen13_4rep4\n",
            "   ❌ Filtering out: scen13_4rep5\n",
            "   ❌ Filtering out: scen13_4rep6\n",
            "   ❌ Filtering out: scen13_4rep7\n",
            "   ❌ Filtering out: scen13_4rep8\n",
            "   ❌ Filtering out: scen13_4rep9\n",
            "   ✅ Including: scen13_5rep1\n",
            "   ❌ Filtering out: scen13_5rep10\n",
            "   ✅ Including: scen13_5rep2\n",
            "   ✅ Including: scen13_5rep3\n",
            "   ❌ Filtering out: scen13_5rep4\n",
            "   ❌ Filtering out: scen13_5rep5\n",
            "   ❌ Filtering out: scen13_5rep6\n",
            "   ❌ Filtering out: scen13_5rep7\n",
            "   ❌ Filtering out: scen13_5rep8\n",
            "   ❌ Filtering out: scen13_5rep9\n",
            "   ✅ Including: scen13_6rep1\n",
            "   ❌ Filtering out: scen13_6rep10\n",
            "   ✅ Including: scen13_6rep2\n",
            "   ✅ Including: scen13_6rep3\n",
            "   ❌ Filtering out: scen13_6rep4\n",
            "   ❌ Filtering out: scen13_6rep5\n",
            "   ❌ Filtering out: scen13_6rep6\n",
            "   ❌ Filtering out: scen13_6rep7\n",
            "   ❌ Filtering out: scen13_6rep8\n",
            "   ❌ Filtering out: scen13_6rep9\n",
            "   ✅ Including: scen14_2rep1\n",
            "   ❌ Filtering out: scen14_2rep10\n",
            "   ✅ Including: scen14_2rep2\n",
            "   ✅ Including: scen14_2rep3\n",
            "   ❌ Filtering out: scen14_2rep4\n",
            "   ❌ Filtering out: scen14_2rep5\n",
            "   ❌ Filtering out: scen14_2rep6\n",
            "   ❌ Filtering out: scen14_2rep7\n",
            "   ❌ Filtering out: scen14_2rep8\n",
            "   ❌ Filtering out: scen14_2rep9\n",
            "   ✅ Including: scen14_3rep1\n",
            "   ❌ Filtering out: scen14_3rep10\n",
            "   ✅ Including: scen14_3rep2\n",
            "   ✅ Including: scen14_3rep3\n",
            "   ❌ Filtering out: scen14_3rep4\n",
            "   ❌ Filtering out: scen14_3rep5\n",
            "   ❌ Filtering out: scen14_3rep6\n",
            "   ❌ Filtering out: scen14_3rep7\n",
            "   ❌ Filtering out: scen14_3rep8\n",
            "   ❌ Filtering out: scen14_3rep9\n",
            "   ✅ Including: scen14_4rep1\n",
            "   ❌ Filtering out: scen14_4rep10\n",
            "   ✅ Including: scen14_4rep2\n",
            "   ✅ Including: scen14_4rep3\n",
            "   ❌ Filtering out: scen14_4rep4\n",
            "   ❌ Filtering out: scen14_4rep5\n",
            "   ❌ Filtering out: scen14_4rep6\n",
            "   ❌ Filtering out: scen14_4rep7\n",
            "   ❌ Filtering out: scen14_4rep8\n",
            "   ❌ Filtering out: scen14_4rep9\n",
            "   ✅ Including: scen14_5rep1\n",
            "   ❌ Filtering out: scen14_5rep10\n",
            "   ✅ Including: scen14_5rep2\n",
            "   ✅ Including: scen14_5rep3\n",
            "   ❌ Filtering out: scen14_5rep4\n",
            "   ❌ Filtering out: scen14_5rep5\n",
            "   ❌ Filtering out: scen14_5rep6\n",
            "   ❌ Filtering out: scen14_5rep7\n",
            "   ❌ Filtering out: scen14_5rep8\n",
            "   ❌ Filtering out: scen14_5rep9\n",
            "   ✅ Including: scen14_6rep1\n",
            "   ❌ Filtering out: scen14_6rep10\n",
            "   ✅ Including: scen14_6rep2\n",
            "   ✅ Including: scen14_6rep3\n",
            "   ❌ Filtering out: scen14_6rep4\n",
            "   ❌ Filtering out: scen14_6rep5\n",
            "   ❌ Filtering out: scen14_6rep6\n",
            "   ❌ Filtering out: scen14_6rep7\n",
            "   ❌ Filtering out: scen14_6rep8\n",
            "   ❌ Filtering out: scen14_6rep9\n",
            "   ✅ Including: scen15_1rep1\n",
            "   ❌ Filtering out: scen15_1rep10\n",
            "   ✅ Including: scen15_1rep2\n",
            "   ✅ Including: scen15_1rep3\n",
            "   ❌ Filtering out: scen15_1rep4\n",
            "   ❌ Filtering out: scen15_1rep5\n",
            "   ❌ Filtering out: scen15_1rep6\n",
            "   ❌ Filtering out: scen15_1rep7\n",
            "   ❌ Filtering out: scen15_1rep8\n",
            "   ❌ Filtering out: scen15_1rep9\n",
            "   ✅ Including: scen15_2rep1\n",
            "   ❌ Filtering out: scen15_2rep10\n",
            "   ✅ Including: scen15_2rep2\n",
            "   ✅ Including: scen15_2rep3\n",
            "   ❌ Filtering out: scen15_2rep4\n",
            "   ❌ Filtering out: scen15_2rep5\n",
            "   ❌ Filtering out: scen15_2rep6\n",
            "   ❌ Filtering out: scen15_2rep7\n",
            "   ❌ Filtering out: scen15_2rep8\n",
            "   ❌ Filtering out: scen15_2rep9\n",
            "   ✅ Including: scen15_3rep1\n",
            "   ❌ Filtering out: scen15_3rep10\n",
            "   ✅ Including: scen15_3rep2\n",
            "   ✅ Including: scen15_3rep3\n",
            "   ❌ Filtering out: scen15_3rep4\n",
            "   ❌ Filtering out: scen15_3rep5\n",
            "   ❌ Filtering out: scen15_3rep6\n",
            "   ❌ Filtering out: scen15_3rep7\n",
            "   ❌ Filtering out: scen15_3rep8\n",
            "   ❌ Filtering out: scen15_3rep9\n",
            "   ✅ Including: scen15_4rep1\n",
            "   ❌ Filtering out: scen15_4rep10\n",
            "   ✅ Including: scen15_4rep2\n",
            "   ✅ Including: scen15_4rep3\n",
            "   ❌ Filtering out: scen15_4rep4\n",
            "   ❌ Filtering out: scen15_4rep5\n",
            "   ❌ Filtering out: scen15_4rep6\n",
            "   ❌ Filtering out: scen15_4rep7\n",
            "   ❌ Filtering out: scen15_4rep8\n",
            "   ❌ Filtering out: scen15_4rep9\n",
            "   ✅ Including: scen15_5rep1\n",
            "   ❌ Filtering out: scen15_5rep10\n",
            "   ✅ Including: scen15_5rep2\n",
            "   ✅ Including: scen15_5rep3\n",
            "   ❌ Filtering out: scen15_5rep4\n",
            "   ❌ Filtering out: scen15_5rep5\n",
            "   ❌ Filtering out: scen15_5rep6\n",
            "   ❌ Filtering out: scen15_5rep7\n",
            "   ❌ Filtering out: scen15_5rep8\n",
            "   ❌ Filtering out: scen15_5rep9\n",
            "   ✅ Including: scen16_1rep1\n",
            "   ❌ Filtering out: scen16_1rep10\n",
            "   ✅ Including: scen16_1rep2\n",
            "   ✅ Including: scen16_1rep3\n",
            "   ❌ Filtering out: scen16_1rep4\n",
            "   ❌ Filtering out: scen16_1rep5\n",
            "   ❌ Filtering out: scen16_1rep6\n",
            "   ❌ Filtering out: scen16_1rep7\n",
            "   ❌ Filtering out: scen16_1rep8\n",
            "   ❌ Filtering out: scen16_1rep9\n",
            "   ✅ Including: scen16_2rep1\n",
            "   ❌ Filtering out: scen16_2rep10\n",
            "   ✅ Including: scen16_2rep2\n",
            "   ✅ Including: scen16_2rep3\n",
            "   ❌ Filtering out: scen16_2rep4\n",
            "   ❌ Filtering out: scen16_2rep5\n",
            "   ❌ Filtering out: scen16_2rep6\n",
            "   ❌ Filtering out: scen16_2rep7\n",
            "   ❌ Filtering out: scen16_2rep8\n",
            "   ❌ Filtering out: scen16_2rep9\n",
            "   ✅ Including: scen16_3rep1\n",
            "   ❌ Filtering out: scen16_3rep10\n",
            "   ✅ Including: scen16_3rep2\n",
            "   ✅ Including: scen16_3rep3\n",
            "   ❌ Filtering out: scen16_3rep4\n",
            "   ❌ Filtering out: scen16_3rep5\n",
            "   ❌ Filtering out: scen16_3rep6\n",
            "   ❌ Filtering out: scen16_3rep7\n",
            "   ❌ Filtering out: scen16_3rep8\n",
            "   ❌ Filtering out: scen16_3rep9\n",
            "   ✅ Including: scen16_4rep1\n",
            "   ❌ Filtering out: scen16_4rep10\n",
            "   ✅ Including: scen16_4rep2\n",
            "   ✅ Including: scen16_4rep3\n",
            "   ❌ Filtering out: scen16_4rep4\n",
            "   ❌ Filtering out: scen16_4rep5\n",
            "   ❌ Filtering out: scen16_4rep6\n",
            "   ❌ Filtering out: scen16_4rep7\n",
            "   ❌ Filtering out: scen16_4rep8\n",
            "   ❌ Filtering out: scen16_4rep9\n",
            "   ✅ Including: scen16_5rep1\n",
            "   ❌ Filtering out: scen16_5rep10\n",
            "   ✅ Including: scen16_5rep2\n",
            "   ✅ Including: scen16_5rep3\n",
            "   ❌ Filtering out: scen16_5rep4\n",
            "   ❌ Filtering out: scen16_5rep5\n",
            "   ❌ Filtering out: scen16_5rep6\n",
            "   ❌ Filtering out: scen16_5rep7\n",
            "   ❌ Filtering out: scen16_5rep8\n",
            "   ❌ Filtering out: scen16_5rep9\n",
            "   ✅ Including: scen17_2rep1\n",
            "   ❌ Filtering out: scen17_2rep10\n",
            "   ✅ Including: scen17_2rep2\n",
            "   ✅ Including: scen17_2rep3\n",
            "   ❌ Filtering out: scen17_2rep4\n",
            "   ❌ Filtering out: scen17_2rep5\n",
            "   ❌ Filtering out: scen17_2rep6\n",
            "   ❌ Filtering out: scen17_2rep7\n",
            "   ❌ Filtering out: scen17_2rep8\n",
            "   ❌ Filtering out: scen17_2rep9\n",
            "   ✅ Including: scen17_3rep1\n",
            "   ❌ Filtering out: scen17_3rep10\n",
            "   ✅ Including: scen17_3rep2\n",
            "   ✅ Including: scen17_3rep3\n",
            "   ❌ Filtering out: scen17_3rep4\n",
            "   ❌ Filtering out: scen17_3rep5\n",
            "   ❌ Filtering out: scen17_3rep6\n",
            "   ❌ Filtering out: scen17_3rep7\n",
            "   ❌ Filtering out: scen17_3rep8\n",
            "   ❌ Filtering out: scen17_3rep9\n",
            "   ✅ Including: scen17_4rep1\n",
            "   ❌ Filtering out: scen17_4rep10\n",
            "   ✅ Including: scen17_4rep2\n",
            "   ✅ Including: scen17_4rep3\n",
            "   ❌ Filtering out: scen17_4rep4\n",
            "   ❌ Filtering out: scen17_4rep5\n",
            "   ❌ Filtering out: scen17_4rep6\n",
            "   ❌ Filtering out: scen17_4rep7\n",
            "   ❌ Filtering out: scen17_4rep8\n",
            "   ❌ Filtering out: scen17_4rep9\n",
            "   ✅ Including: scen17_5rep1\n",
            "   ❌ Filtering out: scen17_5rep10\n",
            "   ✅ Including: scen17_5rep2\n",
            "   ✅ Including: scen17_5rep3\n",
            "   ❌ Filtering out: scen17_5rep4\n",
            "   ❌ Filtering out: scen17_5rep5\n",
            "   ❌ Filtering out: scen17_5rep6\n",
            "   ❌ Filtering out: scen17_5rep7\n",
            "   ❌ Filtering out: scen17_5rep8\n",
            "   ❌ Filtering out: scen17_5rep9\n",
            "   ✅ Including: scen18_1rep1\n",
            "   ❌ Filtering out: scen18_1rep10\n",
            "   ✅ Including: scen18_1rep2\n",
            "   ✅ Including: scen18_1rep3\n",
            "   ❌ Filtering out: scen18_1rep4\n",
            "   ❌ Filtering out: scen18_1rep5\n",
            "   ❌ Filtering out: scen18_1rep6\n",
            "   ❌ Filtering out: scen18_1rep7\n",
            "   ❌ Filtering out: scen18_1rep8\n",
            "   ❌ Filtering out: scen18_1rep9\n",
            "   ✅ Including: scen18_2rep1\n",
            "   ❌ Filtering out: scen18_2rep10\n",
            "   ✅ Including: scen18_2rep2\n",
            "   ✅ Including: scen18_2rep3\n",
            "   ❌ Filtering out: scen18_2rep4\n",
            "   ❌ Filtering out: scen18_2rep5\n",
            "   ❌ Filtering out: scen18_2rep6\n",
            "   ❌ Filtering out: scen18_2rep7\n",
            "   ❌ Filtering out: scen18_2rep8\n",
            "   ❌ Filtering out: scen18_2rep9\n",
            "   ✅ Including: scen18_3rep1\n",
            "   ❌ Filtering out: scen18_3rep10\n",
            "   ✅ Including: scen18_3rep2\n",
            "   ✅ Including: scen18_3rep3\n",
            "   ❌ Filtering out: scen18_3rep4\n",
            "   ❌ Filtering out: scen18_3rep5\n",
            "   ❌ Filtering out: scen18_3rep6\n",
            "   ❌ Filtering out: scen18_3rep7\n",
            "   ❌ Filtering out: scen18_3rep8\n",
            "   ❌ Filtering out: scen18_3rep9\n",
            "   ✅ Including: scen18_4rep1\n",
            "   ❌ Filtering out: scen18_4rep10\n",
            "   ✅ Including: scen18_4rep2\n",
            "   ✅ Including: scen18_4rep3\n",
            "   ❌ Filtering out: scen18_4rep4\n",
            "   ❌ Filtering out: scen18_4rep5\n",
            "   ❌ Filtering out: scen18_4rep6\n",
            "   ❌ Filtering out: scen18_4rep7\n",
            "   ❌ Filtering out: scen18_4rep8\n",
            "   ❌ Filtering out: scen18_4rep9\n",
            "   ✅ Including: scen19_1rep1\n",
            "   ❌ Filtering out: scen19_1rep10\n",
            "   ✅ Including: scen19_1rep2\n",
            "   ✅ Including: scen19_1rep3\n",
            "   ❌ Filtering out: scen19_1rep4\n",
            "   ❌ Filtering out: scen19_1rep5\n",
            "   ❌ Filtering out: scen19_1rep6\n",
            "   ❌ Filtering out: scen19_1rep7\n",
            "   ❌ Filtering out: scen19_1rep8\n",
            "   ❌ Filtering out: scen19_1rep9\n",
            "   ✅ Including: scen19_2rep1\n",
            "   ❌ Filtering out: scen19_2rep10\n",
            "   ✅ Including: scen19_2rep2\n",
            "   ✅ Including: scen19_2rep3\n",
            "   ❌ Filtering out: scen19_2rep4\n",
            "   ❌ Filtering out: scen19_2rep5\n",
            "   ❌ Filtering out: scen19_2rep6\n",
            "   ❌ Filtering out: scen19_2rep7\n",
            "   ❌ Filtering out: scen19_2rep8\n",
            "   ❌ Filtering out: scen19_2rep9\n",
            "   ✅ Including: scen19_3rep1\n",
            "   ❌ Filtering out: scen19_3rep10\n",
            "   ✅ Including: scen19_3rep2\n",
            "   ✅ Including: scen19_3rep3\n",
            "   ❌ Filtering out: scen19_3rep4\n",
            "   ❌ Filtering out: scen19_3rep5\n",
            "   ❌ Filtering out: scen19_3rep6\n",
            "   ❌ Filtering out: scen19_3rep7\n",
            "   ❌ Filtering out: scen19_3rep8\n",
            "   ❌ Filtering out: scen19_3rep9\n",
            "   ✅ Including: scen19_4rep1\n",
            "   ❌ Filtering out: scen19_4rep10\n",
            "   ✅ Including: scen19_4rep2\n",
            "   ✅ Including: scen19_4rep3\n",
            "   ❌ Filtering out: scen19_4rep4\n",
            "   ❌ Filtering out: scen19_4rep5\n",
            "   ❌ Filtering out: scen19_4rep6\n",
            "   ❌ Filtering out: scen19_4rep7\n",
            "   ❌ Filtering out: scen19_4rep8\n",
            "   ❌ Filtering out: scen19_4rep9\n",
            "   ✅ Including: scen19_5rep1\n",
            "   ❌ Filtering out: scen19_5rep10\n",
            "   ✅ Including: scen19_5rep2\n",
            "   ✅ Including: scen19_5rep3\n",
            "   ❌ Filtering out: scen19_5rep4\n",
            "   ❌ Filtering out: scen19_5rep5\n",
            "   ❌ Filtering out: scen19_5rep6\n",
            "   ❌ Filtering out: scen19_5rep7\n",
            "   ❌ Filtering out: scen19_5rep8\n",
            "   ❌ Filtering out: scen19_5rep9\n",
            "   ✅ Including: scen1_10rep1\n",
            "   ❌ Filtering out: scen1_10rep10\n",
            "   ✅ Including: scen1_10rep2\n",
            "   ✅ Including: scen1_10rep3\n",
            "   ❌ Filtering out: scen1_10rep4\n",
            "   ❌ Filtering out: scen1_10rep5\n",
            "   ❌ Filtering out: scen1_10rep6\n",
            "   ❌ Filtering out: scen1_10rep7\n",
            "   ❌ Filtering out: scen1_10rep8\n",
            "   ❌ Filtering out: scen1_10rep9\n",
            "   ✅ Including: scen1_1rep1\n",
            "   ❌ Filtering out: scen1_1rep10\n",
            "   ✅ Including: scen1_1rep2\n",
            "   ✅ Including: scen1_1rep3\n",
            "   ❌ Filtering out: scen1_1rep4\n",
            "   ❌ Filtering out: scen1_1rep5\n",
            "   ❌ Filtering out: scen1_1rep6\n",
            "   ❌ Filtering out: scen1_1rep7\n",
            "   ❌ Filtering out: scen1_1rep8\n",
            "   ❌ Filtering out: scen1_1rep9\n",
            "   ✅ Including: scen1_2rep1\n",
            "   ❌ Filtering out: scen1_2rep10\n",
            "   ✅ Including: scen1_2rep2\n",
            "   ✅ Including: scen1_2rep3\n",
            "   ❌ Filtering out: scen1_2rep4\n",
            "   ❌ Filtering out: scen1_2rep5\n",
            "   ❌ Filtering out: scen1_2rep6\n",
            "   ❌ Filtering out: scen1_2rep7\n",
            "   ❌ Filtering out: scen1_2rep8\n",
            "   ❌ Filtering out: scen1_2rep9\n",
            "   ✅ Including: scen1_3rep1\n",
            "   ❌ Filtering out: scen1_3rep10\n",
            "   ✅ Including: scen1_3rep2\n",
            "   ✅ Including: scen1_3rep3\n",
            "   ❌ Filtering out: scen1_3rep4\n",
            "   ❌ Filtering out: scen1_3rep5\n",
            "   ❌ Filtering out: scen1_3rep6\n",
            "   ❌ Filtering out: scen1_3rep7\n",
            "   ❌ Filtering out: scen1_3rep8\n",
            "   ❌ Filtering out: scen1_3rep9\n",
            "   ✅ Including: scen1_4rep1\n",
            "   ❌ Filtering out: scen1_4rep10\n",
            "   ✅ Including: scen1_4rep2\n",
            "   ✅ Including: scen1_4rep3\n",
            "   ❌ Filtering out: scen1_4rep4\n",
            "   ❌ Filtering out: scen1_4rep5\n",
            "   ❌ Filtering out: scen1_4rep6\n",
            "   ❌ Filtering out: scen1_4rep7\n",
            "   ❌ Filtering out: scen1_4rep8\n",
            "   ❌ Filtering out: scen1_4rep9\n",
            "   ✅ Including: scen1_5rep1\n",
            "   ❌ Filtering out: scen1_5rep10\n",
            "   ✅ Including: scen1_5rep2\n",
            "   ✅ Including: scen1_5rep3\n",
            "   ❌ Filtering out: scen1_5rep4\n",
            "   ❌ Filtering out: scen1_5rep5\n",
            "   ❌ Filtering out: scen1_5rep6\n",
            "   ❌ Filtering out: scen1_5rep7\n",
            "   ❌ Filtering out: scen1_5rep8\n",
            "   ❌ Filtering out: scen1_5rep9\n",
            "   ✅ Including: scen1_6rep1\n",
            "   ❌ Filtering out: scen1_6rep10\n",
            "   ✅ Including: scen1_6rep2\n",
            "   ✅ Including: scen1_6rep3\n",
            "   ❌ Filtering out: scen1_6rep4\n",
            "   ❌ Filtering out: scen1_6rep5\n",
            "   ❌ Filtering out: scen1_6rep6\n",
            "   ❌ Filtering out: scen1_6rep7\n",
            "   ❌ Filtering out: scen1_6rep8\n",
            "   ❌ Filtering out: scen1_6rep9\n",
            "   ✅ Including: scen1_7rep1\n",
            "   ❌ Filtering out: scen1_7rep10\n",
            "   ✅ Including: scen1_7rep2\n",
            "   ✅ Including: scen1_7rep3\n",
            "   ❌ Filtering out: scen1_7rep4\n",
            "   ❌ Filtering out: scen1_7rep5\n",
            "   ❌ Filtering out: scen1_7rep6\n",
            "   ❌ Filtering out: scen1_7rep7\n",
            "   ❌ Filtering out: scen1_7rep8\n",
            "   ❌ Filtering out: scen1_7rep9\n",
            "   ✅ Including: scen1_8rep1\n",
            "   ❌ Filtering out: scen1_8rep10\n",
            "   ✅ Including: scen1_8rep2\n",
            "   ✅ Including: scen1_8rep3\n",
            "   ❌ Filtering out: scen1_8rep4\n",
            "   ❌ Filtering out: scen1_8rep5\n",
            "   ❌ Filtering out: scen1_8rep6\n",
            "   ❌ Filtering out: scen1_8rep7\n",
            "   ❌ Filtering out: scen1_8rep8\n",
            "   ❌ Filtering out: scen1_8rep9\n",
            "   ✅ Including: scen1_9rep1\n",
            "   ❌ Filtering out: scen1_9rep10\n",
            "   ✅ Including: scen1_9rep2\n",
            "   ✅ Including: scen1_9rep3\n",
            "   ❌ Filtering out: scen1_9rep4\n",
            "   ❌ Filtering out: scen1_9rep5\n",
            "   ❌ Filtering out: scen1_9rep6\n",
            "   ❌ Filtering out: scen1_9rep7\n",
            "   ❌ Filtering out: scen1_9rep8\n",
            "   ❌ Filtering out: scen1_9rep9\n",
            "   ✅ Including: scen20_2rep1\n",
            "   ❌ Filtering out: scen20_2rep10\n",
            "   ✅ Including: scen20_2rep2\n",
            "   ✅ Including: scen20_2rep3\n",
            "   ❌ Filtering out: scen20_2rep4\n",
            "   ❌ Filtering out: scen20_2rep5\n",
            "   ❌ Filtering out: scen20_2rep6\n",
            "   ❌ Filtering out: scen20_2rep7\n",
            "   ❌ Filtering out: scen20_2rep8\n",
            "   ❌ Filtering out: scen20_2rep9\n",
            "   ✅ Including: scen20_3rep1\n",
            "   ❌ Filtering out: scen20_3rep10\n",
            "   ✅ Including: scen20_3rep2\n",
            "   ✅ Including: scen20_3rep3\n",
            "   ❌ Filtering out: scen20_3rep4\n",
            "   ❌ Filtering out: scen20_3rep5\n",
            "   ❌ Filtering out: scen20_3rep6\n",
            "   ❌ Filtering out: scen20_3rep7\n",
            "   ❌ Filtering out: scen20_3rep8\n",
            "   ❌ Filtering out: scen20_3rep9\n",
            "   ✅ Including: scen20_4rep1\n",
            "   ❌ Filtering out: scen20_4rep10\n",
            "   ✅ Including: scen20_4rep2\n",
            "   ✅ Including: scen20_4rep3\n",
            "   ❌ Filtering out: scen20_4rep4\n",
            "   ❌ Filtering out: scen20_4rep5\n",
            "   ❌ Filtering out: scen20_4rep6\n",
            "   ❌ Filtering out: scen20_4rep7\n",
            "   ❌ Filtering out: scen20_4rep8\n",
            "   ❌ Filtering out: scen20_4rep9\n",
            "   ✅ Including: scen20_5rep1\n",
            "   ❌ Filtering out: scen20_5rep10\n",
            "   ✅ Including: scen20_5rep2\n",
            "   ✅ Including: scen20_5rep3\n",
            "   ❌ Filtering out: scen20_5rep4\n",
            "   ❌ Filtering out: scen20_5rep5\n",
            "   ❌ Filtering out: scen20_5rep6\n",
            "   ❌ Filtering out: scen20_5rep7\n",
            "   ❌ Filtering out: scen20_5rep8\n",
            "   ❌ Filtering out: scen20_5rep9\n",
            "   ✅ Including: scen21_1rep1\n",
            "   ❌ Filtering out: scen21_1rep10\n",
            "   ✅ Including: scen21_1rep2\n",
            "   ✅ Including: scen21_1rep3\n",
            "   ❌ Filtering out: scen21_1rep4\n",
            "   ❌ Filtering out: scen21_1rep5\n",
            "   ❌ Filtering out: scen21_1rep6\n",
            "   ❌ Filtering out: scen21_1rep7\n",
            "   ❌ Filtering out: scen21_1rep8\n",
            "   ❌ Filtering out: scen21_1rep9\n",
            "   ✅ Including: scen21_2rep1\n",
            "   ❌ Filtering out: scen21_2rep10\n",
            "   ✅ Including: scen21_2rep2\n",
            "   ✅ Including: scen21_2rep3\n",
            "   ❌ Filtering out: scen21_2rep4\n",
            "   ❌ Filtering out: scen21_2rep5\n",
            "   ❌ Filtering out: scen21_2rep6\n",
            "   ❌ Filtering out: scen21_2rep7\n",
            "   ❌ Filtering out: scen21_2rep8\n",
            "   ❌ Filtering out: scen21_2rep9\n",
            "   ✅ Including: scen21_3rep1\n",
            "   ❌ Filtering out: scen21_3rep10\n",
            "   ✅ Including: scen21_3rep2\n",
            "   ✅ Including: scen21_3rep3\n",
            "   ❌ Filtering out: scen21_3rep4\n",
            "   ❌ Filtering out: scen21_3rep5\n",
            "   ❌ Filtering out: scen21_3rep6\n",
            "   ❌ Filtering out: scen21_3rep7\n",
            "   ❌ Filtering out: scen21_3rep8\n",
            "   ❌ Filtering out: scen21_3rep9\n",
            "   ✅ Including: scen21_4rep1\n",
            "   ❌ Filtering out: scen21_4rep10\n",
            "   ✅ Including: scen21_4rep2\n",
            "   ✅ Including: scen21_4rep3\n",
            "   ❌ Filtering out: scen21_4rep4\n",
            "   ❌ Filtering out: scen21_4rep5\n",
            "   ❌ Filtering out: scen21_4rep6\n",
            "   ❌ Filtering out: scen21_4rep7\n",
            "   ❌ Filtering out: scen21_4rep8\n",
            "   ❌ Filtering out: scen21_4rep9\n",
            "   ✅ Including: scen2_10rep1\n",
            "   ❌ Filtering out: scen2_10rep10\n",
            "   ✅ Including: scen2_10rep2\n",
            "   ✅ Including: scen2_10rep3\n",
            "   ❌ Filtering out: scen2_10rep4\n",
            "   ❌ Filtering out: scen2_10rep5\n",
            "   ❌ Filtering out: scen2_10rep6\n",
            "   ❌ Filtering out: scen2_10rep7\n",
            "   ❌ Filtering out: scen2_10rep8\n",
            "   ❌ Filtering out: scen2_10rep9\n",
            "   ✅ Including: scen2_2rep1\n",
            "   ❌ Filtering out: scen2_2rep10\n",
            "   ✅ Including: scen2_2rep2\n",
            "   ✅ Including: scen2_2rep3\n",
            "   ❌ Filtering out: scen2_2rep4\n",
            "   ❌ Filtering out: scen2_2rep5\n",
            "   ❌ Filtering out: scen2_2rep6\n",
            "   ❌ Filtering out: scen2_2rep7\n",
            "   ❌ Filtering out: scen2_2rep8\n",
            "   ❌ Filtering out: scen2_2rep9\n",
            "   ✅ Including: scen2_3rep1\n",
            "   ❌ Filtering out: scen2_3rep10\n",
            "   ✅ Including: scen2_3rep2\n",
            "   ✅ Including: scen2_3rep3\n",
            "   ❌ Filtering out: scen2_3rep4\n",
            "   ❌ Filtering out: scen2_3rep5\n",
            "   ❌ Filtering out: scen2_3rep6\n",
            "   ❌ Filtering out: scen2_3rep7\n",
            "   ❌ Filtering out: scen2_3rep8\n",
            "   ❌ Filtering out: scen2_3rep9\n",
            "   ✅ Including: scen2_4rep1\n",
            "   ❌ Filtering out: scen2_4rep10\n",
            "   ✅ Including: scen2_4rep2\n",
            "   ✅ Including: scen2_4rep3\n",
            "   ❌ Filtering out: scen2_4rep4\n",
            "   ❌ Filtering out: scen2_4rep5\n",
            "   ❌ Filtering out: scen2_4rep6\n",
            "   ❌ Filtering out: scen2_4rep7\n",
            "   ❌ Filtering out: scen2_4rep8\n",
            "   ❌ Filtering out: scen2_4rep9\n",
            "   ✅ Including: scen2_5rep1\n",
            "   ❌ Filtering out: scen2_5rep10\n",
            "   ✅ Including: scen2_5rep2\n",
            "   ✅ Including: scen2_5rep3\n",
            "   ❌ Filtering out: scen2_5rep4\n",
            "   ❌ Filtering out: scen2_5rep5\n",
            "   ❌ Filtering out: scen2_5rep6\n",
            "   ❌ Filtering out: scen2_5rep7\n",
            "   ❌ Filtering out: scen2_5rep8\n",
            "   ❌ Filtering out: scen2_5rep9\n",
            "   ✅ Including: scen2_6rep1\n",
            "   ❌ Filtering out: scen2_6rep10\n",
            "   ✅ Including: scen2_6rep2\n",
            "   ✅ Including: scen2_6rep3\n",
            "   ❌ Filtering out: scen2_6rep4\n",
            "   ❌ Filtering out: scen2_6rep5\n",
            "   ❌ Filtering out: scen2_6rep6\n",
            "   ❌ Filtering out: scen2_6rep7\n",
            "   ❌ Filtering out: scen2_6rep8\n",
            "   ❌ Filtering out: scen2_6rep9\n",
            "   ✅ Including: scen2_7rep1\n",
            "   ❌ Filtering out: scen2_7rep10\n",
            "   ✅ Including: scen2_7rep2\n",
            "   ✅ Including: scen2_7rep3\n",
            "   ❌ Filtering out: scen2_7rep4\n",
            "   ❌ Filtering out: scen2_7rep5\n",
            "   ❌ Filtering out: scen2_7rep6\n",
            "   ❌ Filtering out: scen2_7rep7\n",
            "   ❌ Filtering out: scen2_7rep8\n",
            "   ❌ Filtering out: scen2_7rep9\n",
            "   ✅ Including: scen2_8rep1\n",
            "   ❌ Filtering out: scen2_8rep10\n",
            "   ✅ Including: scen2_8rep2\n",
            "   ✅ Including: scen2_8rep3\n",
            "   ❌ Filtering out: scen2_8rep4\n",
            "   ❌ Filtering out: scen2_8rep5\n",
            "   ❌ Filtering out: scen2_8rep6\n",
            "   ❌ Filtering out: scen2_8rep7\n",
            "   ❌ Filtering out: scen2_8rep8\n",
            "   ❌ Filtering out: scen2_8rep9\n",
            "   ✅ Including: scen2_9rep1\n",
            "   ❌ Filtering out: scen2_9rep10\n",
            "   ✅ Including: scen2_9rep2\n",
            "   ✅ Including: scen2_9rep3\n",
            "   ❌ Filtering out: scen2_9rep4\n",
            "   ❌ Filtering out: scen2_9rep5\n",
            "   ❌ Filtering out: scen2_9rep6\n",
            "   ❌ Filtering out: scen2_9rep7\n",
            "   ❌ Filtering out: scen2_9rep8\n",
            "   ❌ Filtering out: scen2_9rep9\n",
            "   ✅ Including: scen3_1rep1\n",
            "   ❌ Filtering out: scen3_1rep10\n",
            "   ✅ Including: scen3_1rep2\n",
            "   ✅ Including: scen3_1rep3\n",
            "   ❌ Filtering out: scen3_1rep4\n",
            "   ❌ Filtering out: scen3_1rep5\n",
            "   ❌ Filtering out: scen3_1rep6\n",
            "   ❌ Filtering out: scen3_1rep7\n",
            "   ❌ Filtering out: scen3_1rep8\n",
            "   ❌ Filtering out: scen3_1rep9\n",
            "   ✅ Including: scen3_2rep1\n",
            "   ❌ Filtering out: scen3_2rep10\n",
            "   ✅ Including: scen3_2rep2\n",
            "   ✅ Including: scen3_2rep3\n",
            "   ❌ Filtering out: scen3_2rep4\n",
            "   ❌ Filtering out: scen3_2rep5\n",
            "   ❌ Filtering out: scen3_2rep6\n",
            "   ❌ Filtering out: scen3_2rep7\n",
            "   ❌ Filtering out: scen3_2rep8\n",
            "   ❌ Filtering out: scen3_2rep9\n",
            "   ✅ Including: scen3_3rep1\n",
            "   ❌ Filtering out: scen3_3rep10\n",
            "   ✅ Including: scen3_3rep2\n",
            "   ✅ Including: scen3_3rep3\n",
            "   ❌ Filtering out: scen3_3rep4\n",
            "   ❌ Filtering out: scen3_3rep5\n",
            "   ❌ Filtering out: scen3_3rep6\n",
            "   ❌ Filtering out: scen3_3rep7\n",
            "   ❌ Filtering out: scen3_3rep8\n",
            "   ❌ Filtering out: scen3_3rep9\n",
            "   ✅ Including: scen3_4rep1\n",
            "   ❌ Filtering out: scen3_4rep10\n",
            "   ✅ Including: scen3_4rep2\n",
            "   ✅ Including: scen3_4rep3\n",
            "   ❌ Filtering out: scen3_4rep4\n",
            "   ❌ Filtering out: scen3_4rep5\n",
            "   ❌ Filtering out: scen3_4rep6\n",
            "   ❌ Filtering out: scen3_4rep7\n",
            "   ❌ Filtering out: scen3_4rep8\n",
            "   ❌ Filtering out: scen3_4rep9\n",
            "   ✅ Including: scen3_5rep1\n",
            "   ❌ Filtering out: scen3_5rep10\n",
            "   ✅ Including: scen3_5rep2\n",
            "   ✅ Including: scen3_5rep3\n",
            "   ❌ Filtering out: scen3_5rep4\n",
            "   ❌ Filtering out: scen3_5rep5\n",
            "   ❌ Filtering out: scen3_5rep6\n",
            "   ❌ Filtering out: scen3_5rep7\n",
            "   ❌ Filtering out: scen3_5rep8\n",
            "   ❌ Filtering out: scen3_5rep9\n",
            "   ✅ Including: scen3_6rep1\n",
            "   ❌ Filtering out: scen3_6rep10\n",
            "   ✅ Including: scen3_6rep2\n",
            "   ✅ Including: scen3_6rep3\n",
            "   ❌ Filtering out: scen3_6rep4\n",
            "   ❌ Filtering out: scen3_6rep5\n",
            "   ❌ Filtering out: scen3_6rep6\n",
            "   ❌ Filtering out: scen3_6rep7\n",
            "   ❌ Filtering out: scen3_6rep8\n",
            "   ❌ Filtering out: scen3_6rep9\n",
            "   ✅ Including: scen3_7rep1\n",
            "   ❌ Filtering out: scen3_7rep10\n",
            "   ✅ Including: scen3_7rep2\n",
            "   ✅ Including: scen3_7rep3\n",
            "   ❌ Filtering out: scen3_7rep4\n",
            "   ❌ Filtering out: scen3_7rep5\n",
            "   ❌ Filtering out: scen3_7rep6\n",
            "   ❌ Filtering out: scen3_7rep7\n",
            "   ❌ Filtering out: scen3_7rep8\n",
            "   ❌ Filtering out: scen3_7rep9\n",
            "   ✅ Including: scen3_8rep1\n",
            "   ❌ Filtering out: scen3_8rep10\n",
            "   ✅ Including: scen3_8rep2\n",
            "   ✅ Including: scen3_8rep3\n",
            "   ❌ Filtering out: scen3_8rep4\n",
            "   ❌ Filtering out: scen3_8rep5\n",
            "   ❌ Filtering out: scen3_8rep6\n",
            "   ❌ Filtering out: scen3_8rep7\n",
            "   ❌ Filtering out: scen3_8rep8\n",
            "   ❌ Filtering out: scen3_8rep9\n",
            "   ✅ Including: scen3_9rep1\n",
            "   ❌ Filtering out: scen3_9rep10\n",
            "   ✅ Including: scen3_9rep2\n",
            "   ✅ Including: scen3_9rep3\n",
            "   ❌ Filtering out: scen3_9rep4\n",
            "   ❌ Filtering out: scen3_9rep5\n",
            "   ❌ Filtering out: scen3_9rep6\n",
            "   ❌ Filtering out: scen3_9rep7\n",
            "   ❌ Filtering out: scen3_9rep8\n",
            "   ❌ Filtering out: scen3_9rep9\n",
            "   ✅ Including: scen4_1rep1\n",
            "   ❌ Filtering out: scen4_1rep10\n",
            "   ✅ Including: scen4_1rep2\n",
            "   ✅ Including: scen4_1rep3\n",
            "   ❌ Filtering out: scen4_1rep4\n",
            "   ❌ Filtering out: scen4_1rep5\n",
            "   ❌ Filtering out: scen4_1rep6\n",
            "   ❌ Filtering out: scen4_1rep7\n",
            "   ❌ Filtering out: scen4_1rep8\n",
            "   ❌ Filtering out: scen4_1rep9\n",
            "   ✅ Including: scen4_2rep1\n",
            "   ❌ Filtering out: scen4_2rep10\n",
            "   ✅ Including: scen4_2rep2\n",
            "   ✅ Including: scen4_2rep3\n",
            "   ❌ Filtering out: scen4_2rep4\n",
            "   ❌ Filtering out: scen4_2rep5\n",
            "   ❌ Filtering out: scen4_2rep6\n",
            "   ❌ Filtering out: scen4_2rep7\n",
            "   ❌ Filtering out: scen4_2rep8\n",
            "   ❌ Filtering out: scen4_2rep9\n",
            "   ✅ Including: scen4_3rep1\n",
            "   ❌ Filtering out: scen4_3rep10\n",
            "   ✅ Including: scen4_3rep2\n",
            "   ✅ Including: scen4_3rep3\n",
            "   ❌ Filtering out: scen4_3rep4\n",
            "   ❌ Filtering out: scen4_3rep5\n",
            "   ❌ Filtering out: scen4_3rep6\n",
            "   ❌ Filtering out: scen4_3rep7\n",
            "   ❌ Filtering out: scen4_3rep8\n",
            "   ❌ Filtering out: scen4_3rep9\n",
            "   ✅ Including: scen4_4rep1\n",
            "   ❌ Filtering out: scen4_4rep10\n",
            "   ✅ Including: scen4_4rep2\n",
            "   ✅ Including: scen4_4rep3\n",
            "   ❌ Filtering out: scen4_4rep4\n",
            "   ❌ Filtering out: scen4_4rep5\n",
            "   ❌ Filtering out: scen4_4rep6\n",
            "   ❌ Filtering out: scen4_4rep7\n",
            "   ❌ Filtering out: scen4_4rep8\n",
            "   ❌ Filtering out: scen4_4rep9\n",
            "   ✅ Including: scen4_5rep1\n",
            "   ❌ Filtering out: scen4_5rep10\n",
            "   ✅ Including: scen4_5rep2\n",
            "   ✅ Including: scen4_5rep3\n",
            "   ❌ Filtering out: scen4_5rep4\n",
            "   ❌ Filtering out: scen4_5rep5\n",
            "   ❌ Filtering out: scen4_5rep6\n",
            "   ❌ Filtering out: scen4_5rep7\n",
            "   ❌ Filtering out: scen4_5rep8\n",
            "   ❌ Filtering out: scen4_5rep9\n",
            "   ✅ Including: scen4_6rep1\n",
            "   ❌ Filtering out: scen4_6rep10\n",
            "   ✅ Including: scen4_6rep2\n",
            "   ✅ Including: scen4_6rep3\n",
            "   ❌ Filtering out: scen4_6rep4\n",
            "   ❌ Filtering out: scen4_6rep5\n",
            "   ❌ Filtering out: scen4_6rep6\n",
            "   ❌ Filtering out: scen4_6rep7\n",
            "   ❌ Filtering out: scen4_6rep8\n",
            "   ❌ Filtering out: scen4_6rep9\n",
            "   ✅ Including: scen4_7rep1\n",
            "   ❌ Filtering out: scen4_7rep10\n",
            "   ✅ Including: scen4_7rep2\n",
            "   ✅ Including: scen4_7rep3\n",
            "   ❌ Filtering out: scen4_7rep4\n",
            "   ❌ Filtering out: scen4_7rep5\n",
            "   ❌ Filtering out: scen4_7rep6\n",
            "   ❌ Filtering out: scen4_7rep7\n",
            "   ❌ Filtering out: scen4_7rep8\n",
            "   ❌ Filtering out: scen4_7rep9\n",
            "   ✅ Including: scen4_8rep1\n",
            "   ❌ Filtering out: scen4_8rep10\n",
            "   ✅ Including: scen4_8rep2\n",
            "   ✅ Including: scen4_8rep3\n",
            "   ❌ Filtering out: scen4_8rep4\n",
            "   ❌ Filtering out: scen4_8rep5\n",
            "   ❌ Filtering out: scen4_8rep6\n",
            "   ❌ Filtering out: scen4_8rep7\n",
            "   ❌ Filtering out: scen4_8rep8\n",
            "   ❌ Filtering out: scen4_8rep9\n",
            "   ✅ Including: scen4_9rep1\n",
            "   ❌ Filtering out: scen4_9rep10\n",
            "   ✅ Including: scen4_9rep2\n",
            "   ✅ Including: scen4_9rep3\n",
            "   ❌ Filtering out: scen4_9rep4\n",
            "   ❌ Filtering out: scen4_9rep5\n",
            "   ❌ Filtering out: scen4_9rep6\n",
            "   ❌ Filtering out: scen4_9rep7\n",
            "   ❌ Filtering out: scen4_9rep8\n",
            "   ❌ Filtering out: scen4_9rep9\n",
            "   ✅ Including: scen5_2rep1\n",
            "   ❌ Filtering out: scen5_2rep10\n",
            "   ✅ Including: scen5_2rep2\n",
            "   ✅ Including: scen5_2rep3\n",
            "   ❌ Filtering out: scen5_2rep4\n",
            "   ❌ Filtering out: scen5_2rep5\n",
            "   ❌ Filtering out: scen5_2rep6\n",
            "   ❌ Filtering out: scen5_2rep7\n",
            "   ❌ Filtering out: scen5_2rep8\n",
            "   ❌ Filtering out: scen5_2rep9\n",
            "   ✅ Including: scen5_3rep1\n",
            "   ❌ Filtering out: scen5_3rep10\n",
            "   ✅ Including: scen5_3rep2\n",
            "   ✅ Including: scen5_3rep3\n",
            "   ❌ Filtering out: scen5_3rep4\n",
            "   ❌ Filtering out: scen5_3rep5\n",
            "   ❌ Filtering out: scen5_3rep6\n",
            "   ❌ Filtering out: scen5_3rep7\n",
            "   ❌ Filtering out: scen5_3rep8\n",
            "   ❌ Filtering out: scen5_3rep9\n",
            "   ✅ Including: scen5_4rep1\n",
            "   ❌ Filtering out: scen5_4rep10\n",
            "   ✅ Including: scen5_4rep2\n",
            "   ✅ Including: scen5_4rep3\n",
            "   ❌ Filtering out: scen5_4rep4\n",
            "   ❌ Filtering out: scen5_4rep5\n",
            "   ❌ Filtering out: scen5_4rep6\n",
            "   ❌ Filtering out: scen5_4rep7\n",
            "   ❌ Filtering out: scen5_4rep8\n",
            "   ❌ Filtering out: scen5_4rep9\n",
            "   ✅ Including: scen5_5rep1\n",
            "   ❌ Filtering out: scen5_5rep10\n",
            "   ✅ Including: scen5_5rep2\n",
            "   ✅ Including: scen5_5rep3\n",
            "   ❌ Filtering out: scen5_5rep4\n",
            "   ❌ Filtering out: scen5_5rep5\n",
            "   ❌ Filtering out: scen5_5rep6\n",
            "   ❌ Filtering out: scen5_5rep7\n",
            "   ❌ Filtering out: scen5_5rep8\n",
            "   ❌ Filtering out: scen5_5rep9\n",
            "   ✅ Including: scen5_6rep1\n",
            "   ❌ Filtering out: scen5_6rep10\n",
            "   ✅ Including: scen5_6rep2\n",
            "   ✅ Including: scen5_6rep3\n",
            "   ❌ Filtering out: scen5_6rep4\n",
            "   ❌ Filtering out: scen5_6rep5\n",
            "   ❌ Filtering out: scen5_6rep6\n",
            "   ❌ Filtering out: scen5_6rep7\n",
            "   ❌ Filtering out: scen5_6rep8\n",
            "   ❌ Filtering out: scen5_6rep9\n",
            "   ✅ Including: scen5_7rep1\n",
            "   ❌ Filtering out: scen5_7rep10\n",
            "   ✅ Including: scen5_7rep2\n",
            "   ✅ Including: scen5_7rep3\n",
            "   ❌ Filtering out: scen5_7rep4\n",
            "   ❌ Filtering out: scen5_7rep5\n",
            "   ❌ Filtering out: scen5_7rep6\n",
            "   ❌ Filtering out: scen5_7rep7\n",
            "   ❌ Filtering out: scen5_7rep8\n",
            "   ❌ Filtering out: scen5_7rep9\n",
            "   ✅ Including: scen5_8rep1\n",
            "   ❌ Filtering out: scen5_8rep10\n",
            "   ✅ Including: scen5_8rep2\n",
            "   ✅ Including: scen5_8rep3\n",
            "   ❌ Filtering out: scen5_8rep4\n",
            "   ❌ Filtering out: scen5_8rep5\n",
            "   ❌ Filtering out: scen5_8rep6\n",
            "   ❌ Filtering out: scen5_8rep7\n",
            "   ❌ Filtering out: scen5_8rep8\n",
            "   ❌ Filtering out: scen5_8rep9\n",
            "   ✅ Including: scen5_9rep1\n",
            "   ❌ Filtering out: scen5_9rep10\n",
            "   ✅ Including: scen5_9rep2\n",
            "   ✅ Including: scen5_9rep3\n",
            "   ❌ Filtering out: scen5_9rep4\n",
            "   ❌ Filtering out: scen5_9rep5\n",
            "   ❌ Filtering out: scen5_9rep6\n",
            "   ❌ Filtering out: scen5_9rep7\n",
            "   ❌ Filtering out: scen5_9rep8\n",
            "   ❌ Filtering out: scen5_9rep9\n",
            "   ✅ Including: scen6_1rep1\n",
            "   ❌ Filtering out: scen6_1rep10\n",
            "   ✅ Including: scen6_1rep2\n",
            "   ✅ Including: scen6_1rep3\n",
            "   ❌ Filtering out: scen6_1rep4\n",
            "   ❌ Filtering out: scen6_1rep5\n",
            "   ❌ Filtering out: scen6_1rep6\n",
            "   ❌ Filtering out: scen6_1rep7\n",
            "   ❌ Filtering out: scen6_1rep8\n",
            "   ❌ Filtering out: scen6_1rep9\n",
            "   ✅ Including: scen6_2rep1\n",
            "   ❌ Filtering out: scen6_2rep10\n",
            "   ✅ Including: scen6_2rep2\n",
            "   ✅ Including: scen6_2rep3\n",
            "   ❌ Filtering out: scen6_2rep4\n",
            "   ❌ Filtering out: scen6_2rep5\n",
            "   ❌ Filtering out: scen6_2rep6\n",
            "   ❌ Filtering out: scen6_2rep7\n",
            "   ❌ Filtering out: scen6_2rep8\n",
            "   ❌ Filtering out: scen6_2rep9\n",
            "   ✅ Including: scen6_3rep1\n",
            "   ❌ Filtering out: scen6_3rep10\n",
            "   ✅ Including: scen6_3rep2\n",
            "   ✅ Including: scen6_3rep3\n",
            "   ❌ Filtering out: scen6_3rep4\n",
            "   ❌ Filtering out: scen6_3rep5\n",
            "   ❌ Filtering out: scen6_3rep6\n",
            "   ❌ Filtering out: scen6_3rep7\n",
            "   ❌ Filtering out: scen6_3rep8\n",
            "   ❌ Filtering out: scen6_3rep9\n",
            "   ✅ Including: scen6_4rep1\n",
            "   ❌ Filtering out: scen6_4rep10\n",
            "   ✅ Including: scen6_4rep2\n",
            "   ✅ Including: scen6_4rep3\n",
            "   ❌ Filtering out: scen6_4rep4\n",
            "   ❌ Filtering out: scen6_4rep5\n",
            "   ❌ Filtering out: scen6_4rep6\n",
            "   ❌ Filtering out: scen6_4rep7\n",
            "   ❌ Filtering out: scen6_4rep8\n",
            "   ❌ Filtering out: scen6_4rep9\n",
            "   ✅ Including: scen6_5rep1\n",
            "   ❌ Filtering out: scen6_5rep10\n",
            "   ✅ Including: scen6_5rep2\n",
            "   ✅ Including: scen6_5rep3\n",
            "   ❌ Filtering out: scen6_5rep4\n",
            "   ❌ Filtering out: scen6_5rep5\n",
            "   ❌ Filtering out: scen6_5rep6\n",
            "   ❌ Filtering out: scen6_5rep7\n",
            "   ❌ Filtering out: scen6_5rep8\n",
            "   ❌ Filtering out: scen6_5rep9\n",
            "   ✅ Including: scen6_6rep1\n",
            "   ❌ Filtering out: scen6_6rep10\n",
            "   ✅ Including: scen6_6rep2\n",
            "   ✅ Including: scen6_6rep3\n",
            "   ❌ Filtering out: scen6_6rep4\n",
            "   ❌ Filtering out: scen6_6rep5\n",
            "   ❌ Filtering out: scen6_6rep6\n",
            "   ❌ Filtering out: scen6_6rep7\n",
            "   ❌ Filtering out: scen6_6rep8\n",
            "   ❌ Filtering out: scen6_6rep9\n",
            "   ✅ Including: scen6_7rep1\n",
            "   ❌ Filtering out: scen6_7rep10\n",
            "   ✅ Including: scen6_7rep2\n",
            "   ✅ Including: scen6_7rep3\n",
            "   ❌ Filtering out: scen6_7rep4\n",
            "   ❌ Filtering out: scen6_7rep5\n",
            "   ❌ Filtering out: scen6_7rep6\n",
            "   ❌ Filtering out: scen6_7rep7\n",
            "   ❌ Filtering out: scen6_7rep8\n",
            "   ❌ Filtering out: scen6_7rep9\n",
            "   ✅ Including: scen6_8rep1\n",
            "   ❌ Filtering out: scen6_8rep10\n",
            "   ✅ Including: scen6_8rep2\n",
            "   ✅ Including: scen6_8rep3\n",
            "   ❌ Filtering out: scen6_8rep4\n",
            "   ❌ Filtering out: scen6_8rep5\n",
            "   ❌ Filtering out: scen6_8rep6\n",
            "   ❌ Filtering out: scen6_8rep7\n",
            "   ❌ Filtering out: scen6_8rep8\n",
            "   ❌ Filtering out: scen6_8rep9\n",
            "   ✅ Including: scen7_1rep1\n",
            "   ❌ Filtering out: scen7_1rep10\n",
            "   ✅ Including: scen7_1rep2\n",
            "   ✅ Including: scen7_1rep3\n",
            "   ❌ Filtering out: scen7_1rep4\n",
            "   ❌ Filtering out: scen7_1rep5\n",
            "   ❌ Filtering out: scen7_1rep6\n",
            "   ❌ Filtering out: scen7_1rep7\n",
            "   ❌ Filtering out: scen7_1rep8\n",
            "   ❌ Filtering out: scen7_1rep9\n",
            "   ✅ Including: scen7_2rep1\n",
            "   ❌ Filtering out: scen7_2rep10\n",
            "   ✅ Including: scen7_2rep2\n",
            "   ✅ Including: scen7_2rep3\n",
            "   ❌ Filtering out: scen7_2rep4\n",
            "   ❌ Filtering out: scen7_2rep5\n",
            "   ❌ Filtering out: scen7_2rep6\n",
            "   ❌ Filtering out: scen7_2rep7\n",
            "   ❌ Filtering out: scen7_2rep8\n",
            "   ❌ Filtering out: scen7_2rep9\n",
            "   ✅ Including: scen7_3rep1\n",
            "   ❌ Filtering out: scen7_3rep10\n",
            "   ✅ Including: scen7_3rep2\n",
            "   ✅ Including: scen7_3rep3\n",
            "   ❌ Filtering out: scen7_3rep4\n",
            "   ❌ Filtering out: scen7_3rep5\n",
            "   ❌ Filtering out: scen7_3rep6\n",
            "   ❌ Filtering out: scen7_3rep7\n",
            "   ❌ Filtering out: scen7_3rep8\n",
            "   ❌ Filtering out: scen7_3rep9\n",
            "   ✅ Including: scen7_4rep1\n",
            "   ❌ Filtering out: scen7_4rep10\n",
            "   ✅ Including: scen7_4rep2\n",
            "   ✅ Including: scen7_4rep3\n",
            "   ❌ Filtering out: scen7_4rep4\n",
            "   ❌ Filtering out: scen7_4rep5\n",
            "   ❌ Filtering out: scen7_4rep6\n",
            "   ❌ Filtering out: scen7_4rep7\n",
            "   ❌ Filtering out: scen7_4rep8\n",
            "   ❌ Filtering out: scen7_4rep9\n",
            "   ✅ Including: scen7_5rep1\n",
            "   ❌ Filtering out: scen7_5rep10\n",
            "   ✅ Including: scen7_5rep2\n",
            "   ✅ Including: scen7_5rep3\n",
            "   ❌ Filtering out: scen7_5rep4\n",
            "   ❌ Filtering out: scen7_5rep5\n",
            "   ❌ Filtering out: scen7_5rep6\n",
            "   ❌ Filtering out: scen7_5rep7\n",
            "   ❌ Filtering out: scen7_5rep8\n",
            "   ❌ Filtering out: scen7_5rep9\n",
            "   ✅ Including: scen7_6rep1\n",
            "   ❌ Filtering out: scen7_6rep10\n",
            "   ✅ Including: scen7_6rep2\n",
            "   ✅ Including: scen7_6rep3\n",
            "   ❌ Filtering out: scen7_6rep4\n",
            "   ❌ Filtering out: scen7_6rep5\n",
            "   ❌ Filtering out: scen7_6rep6\n",
            "   ❌ Filtering out: scen7_6rep7\n",
            "   ❌ Filtering out: scen7_6rep8\n",
            "   ❌ Filtering out: scen7_6rep9\n",
            "   ✅ Including: scen7_7rep1\n",
            "   ❌ Filtering out: scen7_7rep10\n",
            "   ✅ Including: scen7_7rep2\n",
            "   ✅ Including: scen7_7rep3\n",
            "   ❌ Filtering out: scen7_7rep4\n",
            "   ❌ Filtering out: scen7_7rep5\n",
            "   ❌ Filtering out: scen7_7rep6\n",
            "   ❌ Filtering out: scen7_7rep7\n",
            "   ❌ Filtering out: scen7_7rep8\n",
            "   ❌ Filtering out: scen7_7rep9\n",
            "   ✅ Including: scen7_8rep1\n",
            "   ❌ Filtering out: scen7_8rep10\n",
            "   ✅ Including: scen7_8rep2\n",
            "   ✅ Including: scen7_8rep3\n",
            "   ❌ Filtering out: scen7_8rep4\n",
            "   ❌ Filtering out: scen7_8rep5\n",
            "   ❌ Filtering out: scen7_8rep6\n",
            "   ❌ Filtering out: scen7_8rep7\n",
            "   ❌ Filtering out: scen7_8rep8\n",
            "   ❌ Filtering out: scen7_8rep9\n",
            "   ✅ Including: scen8_2rep1\n",
            "   ❌ Filtering out: scen8_2rep10\n",
            "   ✅ Including: scen8_2rep2\n",
            "   ✅ Including: scen8_2rep3\n",
            "   ❌ Filtering out: scen8_2rep4\n",
            "   ❌ Filtering out: scen8_2rep5\n",
            "   ❌ Filtering out: scen8_2rep6\n",
            "   ❌ Filtering out: scen8_2rep7\n",
            "   ❌ Filtering out: scen8_2rep8\n",
            "   ❌ Filtering out: scen8_2rep9\n",
            "   ✅ Including: scen8_3rep1\n",
            "   ❌ Filtering out: scen8_3rep10\n",
            "   ✅ Including: scen8_3rep2\n",
            "   ✅ Including: scen8_3rep3\n",
            "   ❌ Filtering out: scen8_3rep4\n",
            "   ❌ Filtering out: scen8_3rep5\n",
            "   ❌ Filtering out: scen8_3rep6\n",
            "   ❌ Filtering out: scen8_3rep7\n",
            "   ❌ Filtering out: scen8_3rep8\n",
            "   ❌ Filtering out: scen8_3rep9\n",
            "   ✅ Including: scen8_4rep1\n",
            "   ❌ Filtering out: scen8_4rep10\n",
            "   ✅ Including: scen8_4rep2\n",
            "   ✅ Including: scen8_4rep3\n",
            "   ❌ Filtering out: scen8_4rep4\n",
            "   ❌ Filtering out: scen8_4rep5\n",
            "   ❌ Filtering out: scen8_4rep6\n",
            "   ❌ Filtering out: scen8_4rep7\n",
            "   ❌ Filtering out: scen8_4rep8\n",
            "   ❌ Filtering out: scen8_4rep9\n",
            "   ✅ Including: scen8_5rep1\n",
            "   ❌ Filtering out: scen8_5rep10\n",
            "   ✅ Including: scen8_5rep2\n",
            "   ✅ Including: scen8_5rep3\n",
            "   ❌ Filtering out: scen8_5rep4\n",
            "   ❌ Filtering out: scen8_5rep5\n",
            "   ❌ Filtering out: scen8_5rep6\n",
            "   ❌ Filtering out: scen8_5rep7\n",
            "   ❌ Filtering out: scen8_5rep8\n",
            "   ❌ Filtering out: scen8_5rep9\n",
            "   ✅ Including: scen8_6rep1\n",
            "   ❌ Filtering out: scen8_6rep10\n",
            "   ✅ Including: scen8_6rep2\n",
            "   ✅ Including: scen8_6rep3\n",
            "   ❌ Filtering out: scen8_6rep4\n",
            "   ❌ Filtering out: scen8_6rep5\n",
            "   ❌ Filtering out: scen8_6rep6\n",
            "   ❌ Filtering out: scen8_6rep7\n",
            "   ❌ Filtering out: scen8_6rep8\n",
            "   ❌ Filtering out: scen8_6rep9\n",
            "   ✅ Including: scen8_7rep1\n",
            "   ❌ Filtering out: scen8_7rep10\n",
            "   ✅ Including: scen8_7rep2\n",
            "   ✅ Including: scen8_7rep3\n",
            "   ❌ Filtering out: scen8_7rep4\n",
            "   ❌ Filtering out: scen8_7rep5\n",
            "   ❌ Filtering out: scen8_7rep6\n",
            "   ❌ Filtering out: scen8_7rep7\n",
            "   ❌ Filtering out: scen8_7rep8\n",
            "   ❌ Filtering out: scen8_7rep9\n",
            "   ✅ Including: scen8_8rep1\n",
            "   ❌ Filtering out: scen8_8rep10\n",
            "   ✅ Including: scen8_8rep2\n",
            "   ✅ Including: scen8_8rep3\n",
            "   ❌ Filtering out: scen8_8rep4\n",
            "   ❌ Filtering out: scen8_8rep5\n",
            "   ❌ Filtering out: scen8_8rep6\n",
            "   ❌ Filtering out: scen8_8rep7\n",
            "   ❌ Filtering out: scen8_8rep8\n",
            "   ❌ Filtering out: scen8_8rep9\n",
            "   ✅ Including: scen9_1rep1\n",
            "   ❌ Filtering out: scen9_1rep10\n",
            "   ✅ Including: scen9_1rep2\n",
            "   ✅ Including: scen9_1rep3\n",
            "   ❌ Filtering out: scen9_1rep4\n",
            "   ❌ Filtering out: scen9_1rep5\n",
            "   ❌ Filtering out: scen9_1rep6\n",
            "   ❌ Filtering out: scen9_1rep7\n",
            "   ❌ Filtering out: scen9_1rep8\n",
            "   ❌ Filtering out: scen9_1rep9\n",
            "   ✅ Including: scen9_2rep1\n",
            "   ❌ Filtering out: scen9_2rep10\n",
            "   ✅ Including: scen9_2rep2\n",
            "   ✅ Including: scen9_2rep3\n",
            "   ❌ Filtering out: scen9_2rep4\n",
            "   ❌ Filtering out: scen9_2rep5\n",
            "   ❌ Filtering out: scen9_2rep6\n",
            "   ❌ Filtering out: scen9_2rep7\n",
            "   ❌ Filtering out: scen9_2rep8\n",
            "   ❌ Filtering out: scen9_2rep9\n",
            "   ✅ Including: scen9_3rep1\n",
            "   ❌ Filtering out: scen9_3rep10\n",
            "   ✅ Including: scen9_3rep2\n",
            "   ✅ Including: scen9_3rep3\n",
            "   ❌ Filtering out: scen9_3rep4\n",
            "   ❌ Filtering out: scen9_3rep5\n",
            "   ❌ Filtering out: scen9_3rep6\n",
            "   ❌ Filtering out: scen9_3rep7\n",
            "   ❌ Filtering out: scen9_3rep8\n",
            "   ❌ Filtering out: scen9_3rep9\n",
            "   ✅ Including: scen9_4rep1\n",
            "   ❌ Filtering out: scen9_4rep10\n",
            "   ✅ Including: scen9_4rep2\n",
            "   ✅ Including: scen9_4rep3\n",
            "   ❌ Filtering out: scen9_4rep4\n",
            "   ❌ Filtering out: scen9_4rep5\n",
            "   ❌ Filtering out: scen9_4rep6\n",
            "   ❌ Filtering out: scen9_4rep7\n",
            "   ❌ Filtering out: scen9_4rep8\n",
            "   ❌ Filtering out: scen9_4rep9\n",
            "   ✅ Including: scen9_5rep1\n",
            "   ❌ Filtering out: scen9_5rep10\n",
            "   ✅ Including: scen9_5rep2\n",
            "   ✅ Including: scen9_5rep3\n",
            "   ❌ Filtering out: scen9_5rep4\n",
            "   ❌ Filtering out: scen9_5rep5\n",
            "   ❌ Filtering out: scen9_5rep6\n",
            "   ❌ Filtering out: scen9_5rep7\n",
            "   ❌ Filtering out: scen9_5rep8\n",
            "   ❌ Filtering out: scen9_5rep9\n",
            "   ✅ Including: scen9_6rep1\n",
            "   ❌ Filtering out: scen9_6rep10\n",
            "   ✅ Including: scen9_6rep2\n",
            "   ✅ Including: scen9_6rep3\n",
            "   ❌ Filtering out: scen9_6rep4\n",
            "   ❌ Filtering out: scen9_6rep5\n",
            "   ❌ Filtering out: scen9_6rep6\n",
            "   ❌ Filtering out: scen9_6rep7\n",
            "   ❌ Filtering out: scen9_6rep8\n",
            "   ❌ Filtering out: scen9_6rep9\n",
            "   ✅ Including: scen9_7rep1\n",
            "   ❌ Filtering out: scen9_7rep10\n",
            "   ✅ Including: scen9_7rep2\n",
            "   ✅ Including: scen9_7rep3\n",
            "   ❌ Filtering out: scen9_7rep4\n",
            "   ❌ Filtering out: scen9_7rep5\n",
            "   ❌ Filtering out: scen9_7rep6\n",
            "   ❌ Filtering out: scen9_7rep7\n",
            "   ❌ Filtering out: scen9_7rep8\n",
            "   ❌ Filtering out: scen9_7rep9\n",
            "\n",
            "📊 Filtering results:\n",
            "   ✅ Included scenarios: 408\n",
            "   ❌ Filtered out scenarios: 952\n",
            "   📋 Filtered out scenarios:\n",
            "      - scen10_1rep10\n",
            "      - scen10_1rep4\n",
            "      - scen10_1rep5\n",
            "      - scen10_1rep6\n",
            "      - scen10_1rep7\n",
            "      - scen10_1rep8\n",
            "      - scen10_1rep9\n",
            "      - scen10_2rep10\n",
            "      - scen10_2rep4\n",
            "      - scen10_2rep5\n",
            "      ... and 942 more\n",
            "\n",
            "📄 Scanning 408 filtered folders for CSV files...\n",
            "   📊 scen10_1rep1: 61 CSV files\n",
            "   📊 scen10_1rep2: 61 CSV files\n",
            "   📊 scen10_1rep3: 61 CSV files\n",
            "   📊 scen10_2rep1: 61 CSV files\n",
            "   📊 scen10_2rep2: 61 CSV files\n",
            "   📊 scen10_2rep3: 61 CSV files\n",
            "   📊 scen10_3rep1: 61 CSV files\n",
            "   📊 scen10_3rep2: 61 CSV files\n",
            "   📊 scen10_3rep3: 61 CSV files\n",
            "   📊 scen10_4rep1: 61 CSV files\n",
            "   📊 scen10_4rep2: 61 CSV files\n",
            "   📊 scen10_4rep3: 61 CSV files\n",
            "   📊 scen10_5rep1: 61 CSV files\n",
            "   📊 scen10_5rep2: 61 CSV files\n",
            "   📊 scen10_5rep3: 61 CSV files\n",
            "   📊 scen10_6rep1: 61 CSV files\n",
            "   📊 scen10_6rep2: 61 CSV files\n",
            "   📊 scen10_6rep3: 61 CSV files\n",
            "   📊 scen10_7rep1: 61 CSV files\n",
            "   📊 scen10_7rep2: 61 CSV files\n",
            "   📊 scen10_7rep3: 61 CSV files\n",
            "   📊 scen11_2rep1: 61 CSV files\n",
            "   📊 scen11_2rep2: 61 CSV files\n",
            "   📊 scen11_2rep3: 61 CSV files\n",
            "   📊 scen11_3rep1: 61 CSV files\n",
            "   📊 scen11_3rep2: 61 CSV files\n",
            "   📊 scen11_3rep3: 61 CSV files\n",
            "   📊 scen11_4rep1: 61 CSV files\n",
            "   📊 scen11_4rep2: 61 CSV files\n",
            "   📊 scen11_4rep3: 61 CSV files\n",
            "   📊 scen11_5rep1: 61 CSV files\n",
            "   📊 scen11_5rep2: 61 CSV files\n",
            "   📊 scen11_5rep3: 61 CSV files\n",
            "   📊 scen11_6rep1: 61 CSV files\n",
            "   📊 scen11_6rep2: 61 CSV files\n",
            "   📊 scen11_6rep3: 61 CSV files\n",
            "   📊 scen11_7rep1: 61 CSV files\n",
            "   📊 scen11_7rep2: 61 CSV files\n",
            "   📊 scen11_7rep3: 61 CSV files\n",
            "   📊 scen12_1rep1: 61 CSV files\n",
            "   📊 scen12_1rep2: 61 CSV files\n",
            "   📊 scen12_1rep3: 61 CSV files\n",
            "   📊 scen12_2rep1: 61 CSV files\n",
            "   📊 scen12_2rep2: 61 CSV files\n",
            "   📊 scen12_2rep3: 61 CSV files\n",
            "   📊 scen12_3rep1: 61 CSV files\n",
            "   📊 scen12_3rep2: 61 CSV files\n",
            "   📊 scen12_3rep3: 61 CSV files\n",
            "   📊 scen12_4rep1: 61 CSV files\n",
            "   📊 scen12_4rep2: 61 CSV files\n",
            "   📊 scen12_4rep3: 61 CSV files\n",
            "   📊 scen12_5rep1: 61 CSV files\n",
            "   📊 scen12_5rep2: 61 CSV files\n",
            "   📊 scen12_5rep3: 61 CSV files\n",
            "   📊 scen12_6rep1: 61 CSV files\n",
            "   📊 scen12_6rep2: 61 CSV files\n",
            "   📊 scen12_6rep3: 61 CSV files\n",
            "   📊 scen13_1rep1: 61 CSV files\n",
            "   📊 scen13_1rep2: 61 CSV files\n",
            "   📊 scen13_1rep3: 61 CSV files\n",
            "   📊 scen13_2rep1: 61 CSV files\n",
            "   📊 scen13_2rep2: 61 CSV files\n",
            "   📊 scen13_2rep3: 61 CSV files\n",
            "   📊 scen13_3rep1: 61 CSV files\n",
            "   📊 scen13_3rep2: 61 CSV files\n",
            "   📊 scen13_3rep3: 61 CSV files\n",
            "   📊 scen13_4rep1: 61 CSV files\n",
            "   📊 scen13_4rep2: 61 CSV files\n",
            "   📊 scen13_4rep3: 61 CSV files\n",
            "   📊 scen13_5rep1: 61 CSV files\n",
            "   📊 scen13_5rep2: 61 CSV files\n",
            "   📊 scen13_5rep3: 61 CSV files\n",
            "   📊 scen13_6rep1: 61 CSV files\n",
            "   📊 scen13_6rep2: 61 CSV files\n",
            "   📊 scen13_6rep3: 61 CSV files\n",
            "   📊 scen14_2rep1: 61 CSV files\n",
            "   📊 scen14_2rep2: 61 CSV files\n",
            "   📊 scen14_2rep3: 61 CSV files\n",
            "   📊 scen14_3rep1: 61 CSV files\n",
            "   📊 scen14_3rep2: 61 CSV files\n",
            "   📊 scen14_3rep3: 61 CSV files\n",
            "   📊 scen14_4rep1: 61 CSV files\n",
            "   📊 scen14_4rep2: 61 CSV files\n",
            "   📊 scen14_4rep3: 61 CSV files\n",
            "   📊 scen14_5rep1: 61 CSV files\n",
            "   📊 scen14_5rep2: 61 CSV files\n",
            "   📊 scen14_5rep3: 61 CSV files\n",
            "   📊 scen14_6rep1: 61 CSV files\n",
            "   📊 scen14_6rep2: 61 CSV files\n",
            "   📊 scen14_6rep3: 61 CSV files\n",
            "   📊 scen15_1rep1: 61 CSV files\n",
            "   📊 scen15_1rep2: 61 CSV files\n",
            "   📊 scen15_1rep3: 61 CSV files\n",
            "   📊 scen15_2rep1: 61 CSV files\n",
            "   📊 scen15_2rep2: 61 CSV files\n",
            "   📊 scen15_2rep3: 61 CSV files\n",
            "   📊 scen15_3rep1: 61 CSV files\n",
            "   📊 scen15_3rep2: 61 CSV files\n",
            "   📊 scen15_3rep3: 61 CSV files\n",
            "   📊 scen15_4rep1: 61 CSV files\n",
            "   📊 scen15_4rep2: 61 CSV files\n",
            "   📊 scen15_4rep3: 61 CSV files\n",
            "   📊 scen15_5rep1: 61 CSV files\n",
            "   📊 scen15_5rep2: 61 CSV files\n",
            "   📊 scen15_5rep3: 61 CSV files\n",
            "   📊 scen16_1rep1: 61 CSV files\n",
            "   📊 scen16_1rep2: 61 CSV files\n",
            "   📊 scen16_1rep3: 61 CSV files\n",
            "   📊 scen16_2rep1: 61 CSV files\n",
            "   📊 scen16_2rep2: 61 CSV files\n",
            "   📊 scen16_2rep3: 61 CSV files\n",
            "   📊 scen16_3rep1: 61 CSV files\n",
            "   📊 scen16_3rep2: 61 CSV files\n",
            "   📊 scen16_3rep3: 61 CSV files\n",
            "   📊 scen16_4rep1: 61 CSV files\n",
            "   📊 scen16_4rep2: 61 CSV files\n",
            "   📊 scen16_4rep3: 61 CSV files\n",
            "   📊 scen16_5rep1: 61 CSV files\n",
            "   📊 scen16_5rep2: 61 CSV files\n",
            "   📊 scen16_5rep3: 61 CSV files\n",
            "   📊 scen17_2rep1: 61 CSV files\n",
            "   📊 scen17_2rep2: 61 CSV files\n",
            "   📊 scen17_2rep3: 61 CSV files\n",
            "   📊 scen17_3rep1: 61 CSV files\n",
            "   📊 scen17_3rep2: 61 CSV files\n",
            "   📊 scen17_3rep3: 61 CSV files\n",
            "   📊 scen17_4rep1: 61 CSV files\n",
            "   📊 scen17_4rep2: 61 CSV files\n",
            "   📊 scen17_4rep3: 61 CSV files\n",
            "   📊 scen17_5rep1: 61 CSV files\n",
            "   📊 scen17_5rep2: 61 CSV files\n",
            "   📊 scen17_5rep3: 61 CSV files\n",
            "   📊 scen18_1rep1: 61 CSV files\n",
            "   📊 scen18_1rep2: 61 CSV files\n",
            "   📊 scen18_1rep3: 61 CSV files\n",
            "   📊 scen18_2rep1: 61 CSV files\n",
            "   📊 scen18_2rep2: 61 CSV files\n",
            "   📊 scen18_2rep3: 61 CSV files\n",
            "   📊 scen18_3rep1: 61 CSV files\n",
            "   📊 scen18_3rep2: 61 CSV files\n",
            "   📊 scen18_3rep3: 61 CSV files\n",
            "   📊 scen18_4rep1: 61 CSV files\n",
            "   📊 scen18_4rep2: 61 CSV files\n",
            "   📊 scen18_4rep3: 61 CSV files\n",
            "   📊 scen19_1rep1: 61 CSV files\n",
            "   📊 scen19_1rep2: 61 CSV files\n",
            "   📊 scen19_1rep3: 61 CSV files\n",
            "   📊 scen19_2rep1: 61 CSV files\n",
            "   📊 scen19_2rep2: 61 CSV files\n",
            "   📊 scen19_2rep3: 61 CSV files\n",
            "   📊 scen19_3rep1: 61 CSV files\n",
            "   📊 scen19_3rep2: 61 CSV files\n",
            "   📊 scen19_3rep3: 61 CSV files\n",
            "   📊 scen19_4rep1: 61 CSV files\n",
            "   📊 scen19_4rep2: 61 CSV files\n",
            "   📊 scen19_4rep3: 61 CSV files\n",
            "   📊 scen19_5rep1: 61 CSV files\n",
            "   📊 scen19_5rep2: 61 CSV files\n",
            "   📊 scen19_5rep3: 61 CSV files\n",
            "   📊 scen1_10rep1: 61 CSV files\n",
            "   📊 scen1_10rep2: 61 CSV files\n",
            "   📊 scen1_10rep3: 61 CSV files\n",
            "   📊 scen1_1rep1: 61 CSV files\n",
            "   📊 scen1_1rep2: 61 CSV files\n",
            "   📊 scen1_1rep3: 61 CSV files\n",
            "   📊 scen1_2rep1: 61 CSV files\n",
            "   📊 scen1_2rep2: 61 CSV files\n",
            "   📊 scen1_2rep3: 61 CSV files\n",
            "   📊 scen1_3rep1: 61 CSV files\n",
            "   📊 scen1_3rep2: 61 CSV files\n",
            "   📊 scen1_3rep3: 61 CSV files\n",
            "   📊 scen1_4rep1: 61 CSV files\n",
            "   📊 scen1_4rep2: 61 CSV files\n",
            "   📊 scen1_4rep3: 61 CSV files\n",
            "   📊 scen1_5rep1: 61 CSV files\n",
            "   📊 scen1_5rep2: 61 CSV files\n",
            "   📊 scen1_5rep3: 61 CSV files\n",
            "   📊 scen1_6rep1: 61 CSV files\n",
            "   📊 scen1_6rep2: 61 CSV files\n",
            "   📊 scen1_6rep3: 61 CSV files\n",
            "   📊 scen1_7rep1: 61 CSV files\n",
            "   📊 scen1_7rep2: 61 CSV files\n",
            "   📊 scen1_7rep3: 61 CSV files\n",
            "   📊 scen1_8rep1: 61 CSV files\n",
            "   📊 scen1_8rep2: 61 CSV files\n",
            "   📊 scen1_8rep3: 61 CSV files\n",
            "   📊 scen1_9rep1: 61 CSV files\n",
            "   📊 scen1_9rep2: 61 CSV files\n",
            "   📊 scen1_9rep3: 61 CSV files\n",
            "   📊 scen20_2rep1: 61 CSV files\n",
            "   📊 scen20_2rep2: 61 CSV files\n",
            "   📊 scen20_2rep3: 61 CSV files\n",
            "   📊 scen20_3rep1: 61 CSV files\n",
            "   📊 scen20_3rep2: 61 CSV files\n",
            "   📊 scen20_3rep3: 61 CSV files\n",
            "   📊 scen20_4rep1: 61 CSV files\n",
            "   📊 scen20_4rep2: 61 CSV files\n",
            "   📊 scen20_4rep3: 61 CSV files\n",
            "   📊 scen20_5rep1: 61 CSV files\n",
            "   📊 scen20_5rep2: 61 CSV files\n",
            "   📊 scen20_5rep3: 61 CSV files\n",
            "   📊 scen21_1rep1: 61 CSV files\n",
            "   📊 scen21_1rep2: 61 CSV files\n",
            "   📊 scen21_1rep3: 61 CSV files\n",
            "   📊 scen21_2rep1: 61 CSV files\n",
            "   📊 scen21_2rep2: 61 CSV files\n",
            "   📊 scen21_2rep3: 61 CSV files\n",
            "   📊 scen21_3rep1: 61 CSV files\n",
            "   📊 scen21_3rep2: 61 CSV files\n",
            "   📊 scen21_3rep3: 61 CSV files\n",
            "   📊 scen21_4rep1: 61 CSV files\n",
            "   📊 scen21_4rep2: 61 CSV files\n",
            "   📊 scen21_4rep3: 61 CSV files\n",
            "   📊 scen2_10rep1: 61 CSV files\n",
            "   📊 scen2_10rep2: 61 CSV files\n",
            "   📊 scen2_10rep3: 61 CSV files\n",
            "   📊 scen2_2rep1: 61 CSV files\n",
            "   📊 scen2_2rep2: 61 CSV files\n",
            "   📊 scen2_2rep3: 61 CSV files\n",
            "   📊 scen2_3rep1: 61 CSV files\n",
            "   📊 scen2_3rep2: 61 CSV files\n",
            "   📊 scen2_3rep3: 61 CSV files\n",
            "   📊 scen2_4rep1: 61 CSV files\n",
            "   📊 scen2_4rep2: 61 CSV files\n",
            "   📊 scen2_4rep3: 61 CSV files\n",
            "   📊 scen2_5rep1: 61 CSV files\n",
            "   📊 scen2_5rep2: 61 CSV files\n",
            "   📊 scen2_5rep3: 61 CSV files\n",
            "   📊 scen2_6rep1: 61 CSV files\n",
            "   📊 scen2_6rep2: 61 CSV files\n",
            "   📊 scen2_6rep3: 61 CSV files\n",
            "   📊 scen2_7rep1: 61 CSV files\n",
            "   📊 scen2_7rep2: 61 CSV files\n",
            "   📊 scen2_7rep3: 61 CSV files\n",
            "   📊 scen2_8rep1: 61 CSV files\n",
            "   📊 scen2_8rep2: 61 CSV files\n",
            "   📊 scen2_8rep3: 61 CSV files\n",
            "   📊 scen2_9rep1: 61 CSV files\n",
            "   📊 scen2_9rep2: 61 CSV files\n",
            "   📊 scen2_9rep3: 61 CSV files\n",
            "   📊 scen3_1rep1: 61 CSV files\n",
            "   📊 scen3_1rep2: 61 CSV files\n",
            "   📊 scen3_1rep3: 61 CSV files\n",
            "   📊 scen3_2rep1: 61 CSV files\n",
            "   📊 scen3_2rep2: 61 CSV files\n",
            "   📊 scen3_2rep3: 61 CSV files\n",
            "   📊 scen3_3rep1: 61 CSV files\n",
            "   📊 scen3_3rep2: 61 CSV files\n",
            "   📊 scen3_3rep3: 61 CSV files\n",
            "   📊 scen3_4rep1: 61 CSV files\n",
            "   📊 scen3_4rep2: 61 CSV files\n",
            "   📊 scen3_4rep3: 61 CSV files\n",
            "   📊 scen3_5rep1: 61 CSV files\n",
            "   📊 scen3_5rep2: 61 CSV files\n",
            "   📊 scen3_5rep3: 61 CSV files\n",
            "   📊 scen3_6rep1: 61 CSV files\n",
            "   📊 scen3_6rep2: 61 CSV files\n",
            "   📊 scen3_6rep3: 61 CSV files\n",
            "   📊 scen3_7rep1: 61 CSV files\n",
            "   📊 scen3_7rep2: 61 CSV files\n",
            "   📊 scen3_7rep3: 61 CSV files\n",
            "   📊 scen3_8rep1: 61 CSV files\n",
            "   📊 scen3_8rep2: 61 CSV files\n",
            "   📊 scen3_8rep3: 61 CSV files\n",
            "   📊 scen3_9rep1: 61 CSV files\n",
            "   📊 scen3_9rep2: 61 CSV files\n",
            "   📊 scen3_9rep3: 61 CSV files\n",
            "   📊 scen4_1rep1: 61 CSV files\n",
            "   📊 scen4_1rep2: 61 CSV files\n",
            "   📊 scen4_1rep3: 61 CSV files\n",
            "   📊 scen4_2rep1: 61 CSV files\n",
            "   📊 scen4_2rep2: 61 CSV files\n",
            "   📊 scen4_2rep3: 61 CSV files\n",
            "   📊 scen4_3rep1: 61 CSV files\n",
            "   📊 scen4_3rep2: 61 CSV files\n",
            "   📊 scen4_3rep3: 61 CSV files\n",
            "   📊 scen4_4rep1: 61 CSV files\n",
            "   📊 scen4_4rep2: 61 CSV files\n",
            "   📊 scen4_4rep3: 61 CSV files\n",
            "   📊 scen4_5rep1: 61 CSV files\n",
            "   📊 scen4_5rep2: 61 CSV files\n",
            "   📊 scen4_5rep3: 61 CSV files\n",
            "   📊 scen4_6rep1: 61 CSV files\n",
            "   📊 scen4_6rep2: 61 CSV files\n",
            "   📊 scen4_6rep3: 61 CSV files\n",
            "   📊 scen4_7rep1: 61 CSV files\n",
            "   📊 scen4_7rep2: 61 CSV files\n",
            "   📊 scen4_7rep3: 61 CSV files\n",
            "   📊 scen4_8rep1: 61 CSV files\n",
            "   📊 scen4_8rep2: 61 CSV files\n",
            "   📊 scen4_8rep3: 61 CSV files\n",
            "   📊 scen4_9rep1: 61 CSV files\n",
            "   📊 scen4_9rep2: 61 CSV files\n",
            "   📊 scen4_9rep3: 61 CSV files\n",
            "   📊 scen5_2rep1: 61 CSV files\n",
            "   📊 scen5_2rep2: 61 CSV files\n",
            "   📊 scen5_2rep3: 61 CSV files\n",
            "   📊 scen5_3rep1: 61 CSV files\n",
            "   📊 scen5_3rep2: 61 CSV files\n",
            "   📊 scen5_3rep3: 61 CSV files\n",
            "   📊 scen5_4rep1: 61 CSV files\n",
            "   📊 scen5_4rep2: 61 CSV files\n",
            "   📊 scen5_4rep3: 61 CSV files\n",
            "   📊 scen5_5rep1: 61 CSV files\n",
            "   📊 scen5_5rep2: 61 CSV files\n",
            "   📊 scen5_5rep3: 61 CSV files\n",
            "   📊 scen5_6rep1: 61 CSV files\n",
            "   📊 scen5_6rep2: 61 CSV files\n",
            "   📊 scen5_6rep3: 61 CSV files\n",
            "   📊 scen5_7rep1: 61 CSV files\n",
            "   📊 scen5_7rep2: 61 CSV files\n",
            "   📊 scen5_7rep3: 61 CSV files\n",
            "   📊 scen5_8rep1: 61 CSV files\n",
            "   📊 scen5_8rep2: 61 CSV files\n",
            "   📊 scen5_8rep3: 61 CSV files\n",
            "   📊 scen5_9rep1: 61 CSV files\n",
            "   📊 scen5_9rep2: 61 CSV files\n",
            "   📊 scen5_9rep3: 61 CSV files\n",
            "   📊 scen6_1rep1: 61 CSV files\n",
            "   📊 scen6_1rep2: 61 CSV files\n",
            "   📊 scen6_1rep3: 61 CSV files\n",
            "   📊 scen6_2rep1: 61 CSV files\n",
            "   📊 scen6_2rep2: 61 CSV files\n",
            "   📊 scen6_2rep3: 61 CSV files\n",
            "   📊 scen6_3rep1: 61 CSV files\n",
            "   📊 scen6_3rep2: 61 CSV files\n",
            "   📊 scen6_3rep3: 61 CSV files\n",
            "   📊 scen6_4rep1: 61 CSV files\n",
            "   📊 scen6_4rep2: 61 CSV files\n",
            "   📊 scen6_4rep3: 61 CSV files\n",
            "   📊 scen6_5rep1: 61 CSV files\n",
            "   📊 scen6_5rep2: 61 CSV files\n",
            "   📊 scen6_5rep3: 61 CSV files\n",
            "   📊 scen6_6rep1: 61 CSV files\n",
            "   📊 scen6_6rep2: 61 CSV files\n",
            "   📊 scen6_6rep3: 61 CSV files\n",
            "   📊 scen6_7rep1: 61 CSV files\n",
            "   📊 scen6_7rep2: 61 CSV files\n",
            "   📊 scen6_7rep3: 61 CSV files\n",
            "   📊 scen6_8rep1: 61 CSV files\n",
            "   📊 scen6_8rep2: 61 CSV files\n",
            "   📊 scen6_8rep3: 61 CSV files\n",
            "   📊 scen7_1rep1: 61 CSV files\n",
            "   📊 scen7_1rep2: 61 CSV files\n",
            "   📊 scen7_1rep3: 61 CSV files\n",
            "   📊 scen7_2rep1: 61 CSV files\n",
            "   📊 scen7_2rep2: 61 CSV files\n",
            "   📊 scen7_2rep3: 61 CSV files\n",
            "   📊 scen7_3rep1: 61 CSV files\n",
            "   📊 scen7_3rep2: 61 CSV files\n",
            "   📊 scen7_3rep3: 61 CSV files\n",
            "   📊 scen7_4rep1: 61 CSV files\n",
            "   📊 scen7_4rep2: 61 CSV files\n",
            "   📊 scen7_4rep3: 61 CSV files\n",
            "   📊 scen7_5rep1: 61 CSV files\n",
            "   📊 scen7_5rep2: 61 CSV files\n",
            "   📊 scen7_5rep3: 61 CSV files\n",
            "   📊 scen7_6rep1: 61 CSV files\n",
            "   📊 scen7_6rep2: 61 CSV files\n",
            "   📊 scen7_6rep3: 61 CSV files\n",
            "   📊 scen7_7rep1: 61 CSV files\n",
            "   📊 scen7_7rep2: 61 CSV files\n",
            "   📊 scen7_7rep3: 61 CSV files\n",
            "   📊 scen7_8rep1: 61 CSV files\n",
            "   📊 scen7_8rep2: 61 CSV files\n",
            "   📊 scen7_8rep3: 61 CSV files\n",
            "   📊 scen8_2rep1: 61 CSV files\n",
            "   📊 scen8_2rep2: 61 CSV files\n",
            "   📊 scen8_2rep3: 61 CSV files\n",
            "   📊 scen8_3rep1: 61 CSV files\n",
            "   📊 scen8_3rep2: 61 CSV files\n",
            "   📊 scen8_3rep3: 61 CSV files\n",
            "   📊 scen8_4rep1: 61 CSV files\n",
            "   📊 scen8_4rep2: 61 CSV files\n",
            "   📊 scen8_4rep3: 61 CSV files\n",
            "   📊 scen8_5rep1: 61 CSV files\n",
            "   📊 scen8_5rep2: 61 CSV files\n",
            "   📊 scen8_5rep3: 61 CSV files\n",
            "   📊 scen8_6rep1: 61 CSV files\n",
            "   📊 scen8_6rep2: 61 CSV files\n",
            "   📊 scen8_6rep3: 61 CSV files\n",
            "   📊 scen8_7rep1: 61 CSV files\n",
            "   📊 scen8_7rep2: 61 CSV files\n",
            "   📊 scen8_7rep3: 61 CSV files\n",
            "   📊 scen8_8rep1: 61 CSV files\n",
            "   📊 scen8_8rep2: 61 CSV files\n",
            "   📊 scen8_8rep3: 61 CSV files\n",
            "   📊 scen9_1rep1: 61 CSV files\n",
            "   📊 scen9_1rep2: 61 CSV files\n",
            "   📊 scen9_1rep3: 61 CSV files\n",
            "   📊 scen9_2rep1: 61 CSV files\n",
            "   📊 scen9_2rep2: 61 CSV files\n",
            "   📊 scen9_2rep3: 61 CSV files\n",
            "   📊 scen9_3rep1: 61 CSV files\n",
            "   📊 scen9_3rep2: 61 CSV files\n",
            "   📊 scen9_3rep3: 61 CSV files\n",
            "   📊 scen9_4rep1: 61 CSV files\n",
            "   📊 scen9_4rep2: 61 CSV files\n",
            "   📊 scen9_4rep3: 61 CSV files\n",
            "   📊 scen9_5rep1: 61 CSV files\n",
            "   📊 scen9_5rep2: 61 CSV files\n",
            "   📊 scen9_5rep3: 61 CSV files\n",
            "   📊 scen9_6rep1: 61 CSV files\n",
            "   📊 scen9_6rep2: 61 CSV files\n",
            "   📊 scen9_6rep3: 61 CSV files\n",
            "   📊 scen9_7rep1: 61 CSV files\n",
            "   📊 scen9_7rep2: 61 CSV files\n",
            "   📊 scen9_7rep3: 61 CSV files\n",
            "\n",
            "📊 CSV files by scenario (rep1/rep2/rep3 only):\n",
            "   scen10_1rep1: 61 files\n",
            "   scen10_1rep2: 61 files\n",
            "   scen10_1rep3: 61 files\n",
            "   scen10_2rep1: 61 files\n",
            "   scen10_2rep2: 61 files\n",
            "   scen10_2rep3: 61 files\n",
            "   scen10_3rep1: 61 files\n",
            "   scen10_3rep2: 61 files\n",
            "   scen10_3rep3: 61 files\n",
            "   scen10_4rep1: 61 files\n",
            "   scen10_4rep2: 61 files\n",
            "   scen10_4rep3: 61 files\n",
            "   scen10_5rep1: 61 files\n",
            "   scen10_5rep2: 61 files\n",
            "   scen10_5rep3: 61 files\n",
            "   scen10_6rep1: 61 files\n",
            "   scen10_6rep2: 61 files\n",
            "   scen10_6rep3: 61 files\n",
            "   scen10_7rep1: 61 files\n",
            "   scen10_7rep2: 61 files\n",
            "   scen10_7rep3: 61 files\n",
            "   scen11_2rep1: 61 files\n",
            "   scen11_2rep2: 61 files\n",
            "   scen11_2rep3: 61 files\n",
            "   scen11_3rep1: 61 files\n",
            "   scen11_3rep2: 61 files\n",
            "   scen11_3rep3: 61 files\n",
            "   scen11_4rep1: 61 files\n",
            "   scen11_4rep2: 61 files\n",
            "   scen11_4rep3: 61 files\n",
            "   scen11_5rep1: 61 files\n",
            "   scen11_5rep2: 61 files\n",
            "   scen11_5rep3: 61 files\n",
            "   scen11_6rep1: 61 files\n",
            "   scen11_6rep2: 61 files\n",
            "   scen11_6rep3: 61 files\n",
            "   scen11_7rep1: 61 files\n",
            "   scen11_7rep2: 61 files\n",
            "   scen11_7rep3: 61 files\n",
            "   scen12_1rep1: 61 files\n",
            "   scen12_1rep2: 61 files\n",
            "   scen12_1rep3: 61 files\n",
            "   scen12_2rep1: 61 files\n",
            "   scen12_2rep2: 61 files\n",
            "   scen12_2rep3: 61 files\n",
            "   scen12_3rep1: 61 files\n",
            "   scen12_3rep2: 61 files\n",
            "   scen12_3rep3: 61 files\n",
            "   scen12_4rep1: 61 files\n",
            "   scen12_4rep2: 61 files\n",
            "   scen12_4rep3: 61 files\n",
            "   scen12_5rep1: 61 files\n",
            "   scen12_5rep2: 61 files\n",
            "   scen12_5rep3: 61 files\n",
            "   scen12_6rep1: 61 files\n",
            "   scen12_6rep2: 61 files\n",
            "   scen12_6rep3: 61 files\n",
            "   scen13_1rep1: 61 files\n",
            "   scen13_1rep2: 61 files\n",
            "   scen13_1rep3: 61 files\n",
            "   scen13_2rep1: 61 files\n",
            "   scen13_2rep2: 61 files\n",
            "   scen13_2rep3: 61 files\n",
            "   scen13_3rep1: 61 files\n",
            "   scen13_3rep2: 61 files\n",
            "   scen13_3rep3: 61 files\n",
            "   scen13_4rep1: 61 files\n",
            "   scen13_4rep2: 61 files\n",
            "   scen13_4rep3: 61 files\n",
            "   scen13_5rep1: 61 files\n",
            "   scen13_5rep2: 61 files\n",
            "   scen13_5rep3: 61 files\n",
            "   scen13_6rep1: 61 files\n",
            "   scen13_6rep2: 61 files\n",
            "   scen13_6rep3: 61 files\n",
            "   scen14_2rep1: 61 files\n",
            "   scen14_2rep2: 61 files\n",
            "   scen14_2rep3: 61 files\n",
            "   scen14_3rep1: 61 files\n",
            "   scen14_3rep2: 61 files\n",
            "   scen14_3rep3: 61 files\n",
            "   scen14_4rep1: 61 files\n",
            "   scen14_4rep2: 61 files\n",
            "   scen14_4rep3: 61 files\n",
            "   scen14_5rep1: 61 files\n",
            "   scen14_5rep2: 61 files\n",
            "   scen14_5rep3: 61 files\n",
            "   scen14_6rep1: 61 files\n",
            "   scen14_6rep2: 61 files\n",
            "   scen14_6rep3: 61 files\n",
            "   scen15_1rep1: 61 files\n",
            "   scen15_1rep2: 61 files\n",
            "   scen15_1rep3: 61 files\n",
            "   scen15_2rep1: 61 files\n",
            "   scen15_2rep2: 61 files\n",
            "   scen15_2rep3: 61 files\n",
            "   scen15_3rep1: 61 files\n",
            "   scen15_3rep2: 61 files\n",
            "   scen15_3rep3: 61 files\n",
            "   scen15_4rep1: 61 files\n",
            "   scen15_4rep2: 61 files\n",
            "   scen15_4rep3: 61 files\n",
            "   scen15_5rep1: 61 files\n",
            "   scen15_5rep2: 61 files\n",
            "   scen15_5rep3: 61 files\n",
            "   scen16_1rep1: 61 files\n",
            "   scen16_1rep2: 61 files\n",
            "   scen16_1rep3: 61 files\n",
            "   scen16_2rep1: 61 files\n",
            "   scen16_2rep2: 61 files\n",
            "   scen16_2rep3: 61 files\n",
            "   scen16_3rep1: 61 files\n",
            "   scen16_3rep2: 61 files\n",
            "   scen16_3rep3: 61 files\n",
            "   scen16_4rep1: 61 files\n",
            "   scen16_4rep2: 61 files\n",
            "   scen16_4rep3: 61 files\n",
            "   scen16_5rep1: 61 files\n",
            "   scen16_5rep2: 61 files\n",
            "   scen16_5rep3: 61 files\n",
            "   scen17_2rep1: 61 files\n",
            "   scen17_2rep2: 61 files\n",
            "   scen17_2rep3: 61 files\n",
            "   scen17_3rep1: 61 files\n",
            "   scen17_3rep2: 61 files\n",
            "   scen17_3rep3: 61 files\n",
            "   scen17_4rep1: 61 files\n",
            "   scen17_4rep2: 61 files\n",
            "   scen17_4rep3: 61 files\n",
            "   scen17_5rep1: 61 files\n",
            "   scen17_5rep2: 61 files\n",
            "   scen17_5rep3: 61 files\n",
            "   scen18_1rep1: 61 files\n",
            "   scen18_1rep2: 61 files\n",
            "   scen18_1rep3: 61 files\n",
            "   scen18_2rep1: 61 files\n",
            "   scen18_2rep2: 61 files\n",
            "   scen18_2rep3: 61 files\n",
            "   scen18_3rep1: 61 files\n",
            "   scen18_3rep2: 61 files\n",
            "   scen18_3rep3: 61 files\n",
            "   scen18_4rep1: 61 files\n",
            "   scen18_4rep2: 61 files\n",
            "   scen18_4rep3: 61 files\n",
            "   scen19_1rep1: 61 files\n",
            "   scen19_1rep2: 61 files\n",
            "   scen19_1rep3: 61 files\n",
            "   scen19_2rep1: 61 files\n",
            "   scen19_2rep2: 61 files\n",
            "   scen19_2rep3: 61 files\n",
            "   scen19_3rep1: 61 files\n",
            "   scen19_3rep2: 61 files\n",
            "   scen19_3rep3: 61 files\n",
            "   scen19_4rep1: 61 files\n",
            "   scen19_4rep2: 61 files\n",
            "   scen19_4rep3: 61 files\n",
            "   scen19_5rep1: 61 files\n",
            "   scen19_5rep2: 61 files\n",
            "   scen19_5rep3: 61 files\n",
            "   scen1_10rep1: 61 files\n",
            "   scen1_10rep2: 61 files\n",
            "   scen1_10rep3: 61 files\n",
            "   scen1_1rep1: 61 files\n",
            "   scen1_1rep2: 61 files\n",
            "   scen1_1rep3: 61 files\n",
            "   scen1_2rep1: 61 files\n",
            "   scen1_2rep2: 61 files\n",
            "   scen1_2rep3: 61 files\n",
            "   scen1_3rep1: 61 files\n",
            "   scen1_3rep2: 61 files\n",
            "   scen1_3rep3: 61 files\n",
            "   scen1_4rep1: 61 files\n",
            "   scen1_4rep2: 61 files\n",
            "   scen1_4rep3: 61 files\n",
            "   scen1_5rep1: 61 files\n",
            "   scen1_5rep2: 61 files\n",
            "   scen1_5rep3: 61 files\n",
            "   scen1_6rep1: 61 files\n",
            "   scen1_6rep2: 61 files\n",
            "   scen1_6rep3: 61 files\n",
            "   scen1_7rep1: 61 files\n",
            "   scen1_7rep2: 61 files\n",
            "   scen1_7rep3: 61 files\n",
            "   scen1_8rep1: 61 files\n",
            "   scen1_8rep2: 61 files\n",
            "   scen1_8rep3: 61 files\n",
            "   scen1_9rep1: 61 files\n",
            "   scen1_9rep2: 61 files\n",
            "   scen1_9rep3: 61 files\n",
            "   scen20_2rep1: 61 files\n",
            "   scen20_2rep2: 61 files\n",
            "   scen20_2rep3: 61 files\n",
            "   scen20_3rep1: 61 files\n",
            "   scen20_3rep2: 61 files\n",
            "   scen20_3rep3: 61 files\n",
            "   scen20_4rep1: 61 files\n",
            "   scen20_4rep2: 61 files\n",
            "   scen20_4rep3: 61 files\n",
            "   scen20_5rep1: 61 files\n",
            "   scen20_5rep2: 61 files\n",
            "   scen20_5rep3: 61 files\n",
            "   scen21_1rep1: 61 files\n",
            "   scen21_1rep2: 61 files\n",
            "   scen21_1rep3: 61 files\n",
            "   scen21_2rep1: 61 files\n",
            "   scen21_2rep2: 61 files\n",
            "   scen21_2rep3: 61 files\n",
            "   scen21_3rep1: 61 files\n",
            "   scen21_3rep2: 61 files\n",
            "   scen21_3rep3: 61 files\n",
            "   scen21_4rep1: 61 files\n",
            "   scen21_4rep2: 61 files\n",
            "   scen21_4rep3: 61 files\n",
            "   scen2_10rep1: 61 files\n",
            "   scen2_10rep2: 61 files\n",
            "   scen2_10rep3: 61 files\n",
            "   scen2_2rep1: 61 files\n",
            "   scen2_2rep2: 61 files\n",
            "   scen2_2rep3: 61 files\n",
            "   scen2_3rep1: 61 files\n",
            "   scen2_3rep2: 61 files\n",
            "   scen2_3rep3: 61 files\n",
            "   scen2_4rep1: 61 files\n",
            "   scen2_4rep2: 61 files\n",
            "   scen2_4rep3: 61 files\n",
            "   scen2_5rep1: 61 files\n",
            "   scen2_5rep2: 61 files\n",
            "   scen2_5rep3: 61 files\n",
            "   scen2_6rep1: 61 files\n",
            "   scen2_6rep2: 61 files\n",
            "   scen2_6rep3: 61 files\n",
            "   scen2_7rep1: 61 files\n",
            "   scen2_7rep2: 61 files\n",
            "   scen2_7rep3: 61 files\n",
            "   scen2_8rep1: 61 files\n",
            "   scen2_8rep2: 61 files\n",
            "   scen2_8rep3: 61 files\n",
            "   scen2_9rep1: 61 files\n",
            "   scen2_9rep2: 61 files\n",
            "   scen2_9rep3: 61 files\n",
            "   scen3_1rep1: 61 files\n",
            "   scen3_1rep2: 61 files\n",
            "   scen3_1rep3: 61 files\n",
            "   scen3_2rep1: 61 files\n",
            "   scen3_2rep2: 61 files\n",
            "   scen3_2rep3: 61 files\n",
            "   scen3_3rep1: 61 files\n",
            "   scen3_3rep2: 61 files\n",
            "   scen3_3rep3: 61 files\n",
            "   scen3_4rep1: 61 files\n",
            "   scen3_4rep2: 61 files\n",
            "   scen3_4rep3: 61 files\n",
            "   scen3_5rep1: 61 files\n",
            "   scen3_5rep2: 61 files\n",
            "   scen3_5rep3: 61 files\n",
            "   scen3_6rep1: 61 files\n",
            "   scen3_6rep2: 61 files\n",
            "   scen3_6rep3: 61 files\n",
            "   scen3_7rep1: 61 files\n",
            "   scen3_7rep2: 61 files\n",
            "   scen3_7rep3: 61 files\n",
            "   scen3_8rep1: 61 files\n",
            "   scen3_8rep2: 61 files\n",
            "   scen3_8rep3: 61 files\n",
            "   scen3_9rep1: 61 files\n",
            "   scen3_9rep2: 61 files\n",
            "   scen3_9rep3: 61 files\n",
            "   scen4_1rep1: 61 files\n",
            "   scen4_1rep2: 61 files\n",
            "   scen4_1rep3: 61 files\n",
            "   scen4_2rep1: 61 files\n",
            "   scen4_2rep2: 61 files\n",
            "   scen4_2rep3: 61 files\n",
            "   scen4_3rep1: 61 files\n",
            "   scen4_3rep2: 61 files\n",
            "   scen4_3rep3: 61 files\n",
            "   scen4_4rep1: 61 files\n",
            "   scen4_4rep2: 61 files\n",
            "   scen4_4rep3: 61 files\n",
            "   scen4_5rep1: 61 files\n",
            "   scen4_5rep2: 61 files\n",
            "   scen4_5rep3: 61 files\n",
            "   scen4_6rep1: 61 files\n",
            "   scen4_6rep2: 61 files\n",
            "   scen4_6rep3: 61 files\n",
            "   scen4_7rep1: 61 files\n",
            "   scen4_7rep2: 61 files\n",
            "   scen4_7rep3: 61 files\n",
            "   scen4_8rep1: 61 files\n",
            "   scen4_8rep2: 61 files\n",
            "   scen4_8rep3: 61 files\n",
            "   scen4_9rep1: 61 files\n",
            "   scen4_9rep2: 61 files\n",
            "   scen4_9rep3: 61 files\n",
            "   scen5_2rep1: 61 files\n",
            "   scen5_2rep2: 61 files\n",
            "   scen5_2rep3: 61 files\n",
            "   scen5_3rep1: 61 files\n",
            "   scen5_3rep2: 61 files\n",
            "   scen5_3rep3: 61 files\n",
            "   scen5_4rep1: 61 files\n",
            "   scen5_4rep2: 61 files\n",
            "   scen5_4rep3: 61 files\n",
            "   scen5_5rep1: 61 files\n",
            "   scen5_5rep2: 61 files\n",
            "   scen5_5rep3: 61 files\n",
            "   scen5_6rep1: 61 files\n",
            "   scen5_6rep2: 61 files\n",
            "   scen5_6rep3: 61 files\n",
            "   scen5_7rep1: 61 files\n",
            "   scen5_7rep2: 61 files\n",
            "   scen5_7rep3: 61 files\n",
            "   scen5_8rep1: 61 files\n",
            "   scen5_8rep2: 61 files\n",
            "   scen5_8rep3: 61 files\n",
            "   scen5_9rep1: 61 files\n",
            "   scen5_9rep2: 61 files\n",
            "   scen5_9rep3: 61 files\n",
            "   scen6_1rep1: 61 files\n",
            "   scen6_1rep2: 61 files\n",
            "   scen6_1rep3: 61 files\n",
            "   scen6_2rep1: 61 files\n",
            "   scen6_2rep2: 61 files\n",
            "   scen6_2rep3: 61 files\n",
            "   scen6_3rep1: 61 files\n",
            "   scen6_3rep2: 61 files\n",
            "   scen6_3rep3: 61 files\n",
            "   scen6_4rep1: 61 files\n",
            "   scen6_4rep2: 61 files\n",
            "   scen6_4rep3: 61 files\n",
            "   scen6_5rep1: 61 files\n",
            "   scen6_5rep2: 61 files\n",
            "   scen6_5rep3: 61 files\n",
            "   scen6_6rep1: 61 files\n",
            "   scen6_6rep2: 61 files\n",
            "   scen6_6rep3: 61 files\n",
            "   scen6_7rep1: 61 files\n",
            "   scen6_7rep2: 61 files\n",
            "   scen6_7rep3: 61 files\n",
            "   scen6_8rep1: 61 files\n",
            "   scen6_8rep2: 61 files\n",
            "   scen6_8rep3: 61 files\n",
            "   scen7_1rep1: 61 files\n",
            "   scen7_1rep2: 61 files\n",
            "   scen7_1rep3: 61 files\n",
            "   scen7_2rep1: 61 files\n",
            "   scen7_2rep2: 61 files\n",
            "   scen7_2rep3: 61 files\n",
            "   scen7_3rep1: 61 files\n",
            "   scen7_3rep2: 61 files\n",
            "   scen7_3rep3: 61 files\n",
            "   scen7_4rep1: 61 files\n",
            "   scen7_4rep2: 61 files\n",
            "   scen7_4rep3: 61 files\n",
            "   scen7_5rep1: 61 files\n",
            "   scen7_5rep2: 61 files\n",
            "   scen7_5rep3: 61 files\n",
            "   scen7_6rep1: 61 files\n",
            "   scen7_6rep2: 61 files\n",
            "   scen7_6rep3: 61 files\n",
            "   scen7_7rep1: 61 files\n",
            "   scen7_7rep2: 61 files\n",
            "   scen7_7rep3: 61 files\n",
            "   scen7_8rep1: 61 files\n",
            "   scen7_8rep2: 61 files\n",
            "   scen7_8rep3: 61 files\n",
            "   scen8_2rep1: 61 files\n",
            "   scen8_2rep2: 61 files\n",
            "   scen8_2rep3: 61 files\n",
            "   scen8_3rep1: 61 files\n",
            "   scen8_3rep2: 61 files\n",
            "   scen8_3rep3: 61 files\n",
            "   scen8_4rep1: 61 files\n",
            "   scen8_4rep2: 61 files\n",
            "   scen8_4rep3: 61 files\n",
            "   scen8_5rep1: 61 files\n",
            "   scen8_5rep2: 61 files\n",
            "   scen8_5rep3: 61 files\n",
            "   scen8_6rep1: 61 files\n",
            "   scen8_6rep2: 61 files\n",
            "   scen8_6rep3: 61 files\n",
            "   scen8_7rep1: 61 files\n",
            "   scen8_7rep2: 61 files\n",
            "   scen8_7rep3: 61 files\n",
            "   scen8_8rep1: 61 files\n",
            "   scen8_8rep2: 61 files\n",
            "   scen8_8rep3: 61 files\n",
            "   scen9_1rep1: 61 files\n",
            "   scen9_1rep2: 61 files\n",
            "   scen9_1rep3: 61 files\n",
            "   scen9_2rep1: 61 files\n",
            "   scen9_2rep2: 61 files\n",
            "   scen9_2rep3: 61 files\n",
            "   scen9_3rep1: 61 files\n",
            "   scen9_3rep2: 61 files\n",
            "   scen9_3rep3: 61 files\n",
            "   scen9_4rep1: 61 files\n",
            "   scen9_4rep2: 61 files\n",
            "   scen9_4rep3: 61 files\n",
            "   scen9_5rep1: 61 files\n",
            "   scen9_5rep2: 61 files\n",
            "   scen9_5rep3: 61 files\n",
            "   scen9_6rep1: 61 files\n",
            "   scen9_6rep2: 61 files\n",
            "   scen9_6rep3: 61 files\n",
            "   scen9_7rep1: 61 files\n",
            "   scen9_7rep2: 61 files\n",
            "   scen9_7rep3: 61 files\n",
            "\n",
            "✅ Filtered discovery complete:\n",
            "   Total CSV files: 24,888\n",
            "   Only rep1/rep2/rep3: ✅\n",
            "   Discovery time: 224.9s\n",
            "🎯 Will process 24,888 files with rate limiting\n",
            "\n",
            "📡 Step 2: Rate-limited processing...\n",
            "📡 RATE-LIMITED PROCESSING 24,888 files\n",
            "Workers: 4 (API-safe)\n",
            "============================================================\n",
            "🔄 Checking for previous checkpoints...\n",
            "📚 Checkpoint loaded!\n",
            "   Files processed: 8,000\n",
            "   Samples: 50,000\n",
            "   Last batch: 16\n",
            "✅ All files already processed!\n",
            "\n",
            "📊 Step 3: Analyzing dataset...\n",
            "RATE-LIMITED DATASET SUMMARY:\n",
            "   Files processed: 24,888\n",
            "   Total vehicles: 50,000\n",
            "   Label distribution:\n",
            "     aggressive: 32,708 (65.4%)\n",
            "     cooperative: 17,292 (34.6%)\n",
            "\n",
            "🔧 Step 4: Data preparation...\n",
            "   Processed 0/50,000 samples\n",
            "   Processed 10,000/50,000 samples\n",
            "   Processed 20,000/50,000 samples\n",
            "   Processed 30,000/50,000 samples\n",
            "   Processed 40,000/50,000 samples\n",
            "✅ Data prepared: 50,000 samples\n",
            "Train samples: 40,000\n",
            "Test samples: 10,000\n",
            "Label distribution in all_labels: Counter({'aggressive': 32708, 'cooperative': 17292})\n",
            "Unique labels: {'cooperative', 'aggressive'}\n",
            "Total samples: 50000\n",
            "Classes: ['aggressive' 'cooperative']\n",
            "\n",
            "🚀 Step 5: Model training...\n",
            "🧠 Fixed Model Architecture:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional_2\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_2\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
              "│ speed (\u001b[38;5;33mInputLayer\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m1\u001b[0m)     │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ position            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m2\u001b[0m)     │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ acceleration        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m1\u001b[0m)     │          \u001b[38;5;34m0\u001b[0m │ speed[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
              "│ (\u001b[38;5;33mLambda\u001b[0m)            │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ lateral (\u001b[38;5;33mLambda\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m1\u001b[0m)     │          \u001b[38;5;34m0\u001b[0m │ position[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ concatenate_4       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m3\u001b[0m)     │          \u001b[38;5;34m0\u001b[0m │ speed[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],      │\n",
              "│ (\u001b[38;5;33mConcatenate\u001b[0m)       │                   │            │ acceleration[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
              "│                     │                   │            │ lateral[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ conv1d_4 (\u001b[38;5;33mConv1D\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m64\u001b[0m)    │        \u001b[38;5;34m640\u001b[0m │ concatenate_4[\u001b[38;5;34m0\u001b[0m]… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ batch_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m64\u001b[0m)    │        \u001b[38;5;34m256\u001b[0m │ conv1d_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
              "│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dropout_4 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m64\u001b[0m)    │          \u001b[38;5;34m0\u001b[0m │ batch_normalizat… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ conv1d_5 (\u001b[38;5;33mConv1D\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m128\u001b[0m)   │     \u001b[38;5;34m24,704\u001b[0m │ dropout_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ lstm_4 (\u001b[38;5;33mLSTM\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m128\u001b[0m)   │     \u001b[38;5;34m67,584\u001b[0m │ concatenate_4[\u001b[38;5;34m0\u001b[0m]… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ batch_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m128\u001b[0m)   │        \u001b[38;5;34m512\u001b[0m │ conv1d_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
              "│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ batch_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m128\u001b[0m)   │        \u001b[38;5;34m512\u001b[0m │ lstm_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
              "│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ global_max_pooling… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ batch_normalizat… │\n",
              "│ (\u001b[38;5;33mGlobalMaxPooling1…\u001b[0m │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ lstm_5 (\u001b[38;5;33mLSTM\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │     \u001b[38;5;34m49,408\u001b[0m │ batch_normalizat… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ concatenate_5       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m192\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ global_max_pooli… │\n",
              "│ (\u001b[38;5;33mConcatenate\u001b[0m)       │                   │            │ lstm_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_6 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │     \u001b[38;5;34m49,408\u001b[0m │ concatenate_5[\u001b[38;5;34m0\u001b[0m]… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ batch_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │      \u001b[38;5;34m1,024\u001b[0m │ dense_6[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
              "│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dropout_5 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ batch_normalizat… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_7 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │     \u001b[38;5;34m32,896\u001b[0m │ dropout_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_8 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)         │        \u001b[38;5;34m258\u001b[0m │ dense_7[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
              "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
              "│ speed (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)     │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ position            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)     │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ acceleration        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)     │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ speed[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)            │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ lateral (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)     │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ position[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ concatenate_4       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)     │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ speed[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],      │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       │                   │            │ acceleration[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│                     │                   │            │ lateral[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ conv1d_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">640</span> │ concatenate_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ batch_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │ conv1d_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dropout_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalizat… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ conv1d_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)   │     <span style=\"color: #00af00; text-decoration-color: #00af00\">24,704</span> │ dropout_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ lstm_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)   │     <span style=\"color: #00af00; text-decoration-color: #00af00\">67,584</span> │ concatenate_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ batch_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)   │        <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ conv1d_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ batch_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)   │        <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ lstm_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ global_max_pooling… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalizat… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalMaxPooling1…</span> │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ lstm_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │     <span style=\"color: #00af00; text-decoration-color: #00af00\">49,408</span> │ batch_normalizat… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ concatenate_5       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ global_max_pooli… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       │                   │            │ lstm_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">49,408</span> │ concatenate_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ batch_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │ dense_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dropout_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalizat… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">32,896</span> │ dropout_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">258</span> │ dense_7[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
              "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m227,202\u001b[0m (887.51 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">227,202</span> (887.51 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m226,050\u001b[0m (883.01 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">226,050</span> (883.01 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m1,152\u001b[0m (4.50 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,152</span> (4.50 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training with rate-limited dataset...\n",
            "Training samples: 40,000\n",
            "Epoch 1/20\n",
            "\u001b[1m133/133\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 68ms/step - accuracy: 0.6714 - loss: 0.7032 - val_accuracy: 0.4883 - val_loss: 0.7472 - learning_rate: 0.0010\n",
            "Epoch 2/20\n",
            "\u001b[1m133/133\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - accuracy: 0.7131 - loss: 0.6099 - val_accuracy: 0.5175 - val_loss: 0.7290 - learning_rate: 0.0010\n",
            "Epoch 3/20\n",
            "\u001b[1m133/133\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - accuracy: 0.7229 - loss: 0.5888 - val_accuracy: 0.5318 - val_loss: 0.7673 - learning_rate: 0.0010\n",
            "Epoch 4/20\n",
            "\u001b[1m133/133\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - accuracy: 0.7330 - loss: 0.5744 - val_accuracy: 0.5347 - val_loss: 0.7629 - learning_rate: 0.0010\n",
            "Epoch 5/20\n",
            "\u001b[1m133/133\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - accuracy: 0.7357 - loss: 0.5718 - val_accuracy: 0.5360 - val_loss: 0.7784 - learning_rate: 0.0010\n",
            "Epoch 6/20\n",
            "\u001b[1m133/133\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - accuracy: 0.7443 - loss: 0.5634 - val_accuracy: 0.5485 - val_loss: 0.7517 - learning_rate: 5.0000e-04\n",
            "Epoch 7/20\n",
            "\u001b[1m133/133\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - accuracy: 0.7446 - loss: 0.5588 - val_accuracy: 0.5465 - val_loss: 0.7646 - learning_rate: 5.0000e-04\n",
            "✅ Training completed!\n",
            "\n",
            "📊 Evaluation...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:5 out of the last 29 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x7a12782302c0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RATE-LIMITED RESULTS:\n",
            "   Test Accuracy: 0.510\n",
            "   F1 Score (macro): 0.463\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "  aggressive       0.50      0.82      0.62      4912\n",
            " cooperative       0.55      0.21      0.30      5088\n",
            "\n",
            "    accuracy                           0.51     10000\n",
            "   macro avg       0.53      0.52      0.46     10000\n",
            "weighted avg       0.53      0.51      0.46     10000\n",
            "\n",
            "\n",
            "💾 Saving model...\n",
            "❌ Training failed: name 'cl_system' is not defined\n",
            "\n",
            "❌ Training failed - run resume_training() to continue\n",
            "\n",
            "💾 All checkpoints saved to: /content/drive/MyDrive/Training_Checkpoints\n",
            "📡 API-safe trainer - no more 429 errors!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-48-2476193913.py\", line 817, in train_with_rate_limits\n",
            "    cl_system.model = model\n",
            "    ^^^^^^^^^\n",
            "NameError: name 'cl_system' is not defined\n"
          ]
        }
      ]
    }
  ]
}