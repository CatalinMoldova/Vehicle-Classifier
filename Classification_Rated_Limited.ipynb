{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        " \"\"\"\n",
        "# Abu Dhabi Traffic Flow Vehicle Behavior Classification Project\n",
        "\n",
        "## Research Objective:\n",
        "Train a CNN-LSTM hybrid model using actual vehicle labels from Aimsun data:\n",
        "- HDV Aggressive ‚Üí Aggressive behavior\n",
        "- HDV Conventional Gipps Model ‚Üí Normal behavior\n",
        "- HDV Cooperative ‚Üí Cooperative behavior\n",
        "- CAV ‚Üí Autonomous vehicle (excluded from training)\n",
        "\n",
        "## Model Validation:\n",
        "Compare CNN-LSTM predictions with actual vehicle labels to evaluate accuracy and F1 scores. Split the data into 80% training and 20% testing. Get the data from BOX using BOX API\n",
        "\"\"\"\n",
        "print(\"üöó Abu Dhabi Traffic Flow Vehicle Behavior Classification System\")\n",
        "print(\"üìä Training with Actual Vehicle Labels from Data Files\")\n"
      ],
      "metadata": {
        "id": "19aUYFyUEL3B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "CHeck the GPU"
      ],
      "metadata": {
        "id": "ayA6YEbYEfTA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HXIpGdvyEE_T",
        "outputId": "b04a8ce4-1e1c-478d-f975-143888427f99"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sat Jul 26 17:07:17 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   67C    P0             34W /   70W |    7562MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from psutil import virtual_memory\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "if ram_gb < 20:\n",
        "  print('Not using a high-RAM runtime')\n",
        "else:\n",
        "  print('You are using a high-RAM runtime!')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jaOFC8HaEJpm",
        "outputId": "3ca9f5e9-7a3b-4ab3-81b4-a9bd5ba32517"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your runtime has 54.8 gigabytes of available RAM\n",
            "\n",
            "You are using a high-RAM runtime!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "flowchart TD\n",
        "\n",
        "    A[Raw CSV Data] --> B[Data Parsing & Cleaning]\n",
        "    B --> C[Feature Extraction]\n",
        "    C --> D[Data Preparation]\n",
        "    D --> E[Model Training (CNN-LSTM)]\n",
        "    E --> F[Prediction & Continuous Learning]\n",
        "    F --> G[Behavior Output: Aggressive/Cooperative/Normal]\n"
      ],
      "metadata": {
        "id": "iGLsituzEhKB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade tensorflow\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jI05iFqnYAti",
        "outputId": "8f599109-26a4-4e91-a78a-970232752711"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.19.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (5.29.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.14.1)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.73.1)\n",
            "Requirement already satisfied: tensorboard~=2.19.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.19.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.8.0)\n",
            "Requirement already satisfied: numpy<2.2.0,>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.0.2)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.14.0)\n",
            "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.5.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.1.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.7.14)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.8.2)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard~=2.19.0->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install boxsdk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lDM7amQRbYUK",
        "outputId": "789d4f72-0b44-41c7-8acf-ed8c49fdc52a"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: boxsdk in /usr/local/lib/python3.11/dist-packages (3.14.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from boxsdk) (25.3.0)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.11/dist-packages (from boxsdk) (2.5.0)\n",
            "Requirement already satisfied: requests<3,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from boxsdk) (2.32.3)\n",
            "Requirement already satisfied: requests-toolbelt>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from boxsdk) (1.0.0)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.11/dist-packages (from boxsdk) (2.9.0.post0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.4.3->boxsdk) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.4.3->boxsdk) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.4.3->boxsdk) (2025.7.14)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil->boxsdk) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install \"boxsdk[jwt]\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nob-Ws-EbYxC",
        "outputId": "bfd924a1-b907-4f67-d917-2cb36d88c461"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: boxsdk[jwt] in /usr/local/lib/python3.11/dist-packages (3.14.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from boxsdk[jwt]) (25.3.0)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.11/dist-packages (from boxsdk[jwt]) (2.5.0)\n",
            "Requirement already satisfied: requests<3,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from boxsdk[jwt]) (2.32.3)\n",
            "Requirement already satisfied: requests-toolbelt>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from boxsdk[jwt]) (1.0.0)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.11/dist-packages (from boxsdk[jwt]) (2.9.0.post0)\n",
            "Requirement already satisfied: pyjwt>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from boxsdk[jwt]) (2.10.1)\n",
            "Requirement already satisfied: cryptography>=3 in /usr/local/lib/python3.11/dist-packages (from boxsdk[jwt]) (43.0.3)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography>=3->boxsdk[jwt]) (1.17.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.4.3->boxsdk[jwt]) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.4.3->boxsdk[jwt]) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.4.3->boxsdk[jwt]) (2025.7.14)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil->boxsdk[jwt]) (1.17.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography>=3->boxsdk[jwt]) (2.22)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, json, numpy as np, pandas as pd, warnings\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model, load_model\n",
        "from tensorflow.keras.layers import (Input, LSTM, Conv1D, Dense, Dropout, BatchNormalization, Concatenate, GlobalMaxPooling1D, Masking)\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "import joblib\n",
        "import matplotlib.pyplot as plt\n",
        "import io\n",
        "import json\n",
        "import hashlib\n",
        "from tensorflow.keras.layers import Lambda\n",
        "from sklearn.metrics import f1_score, precision_score, recall_score, classification_report\n",
        "from boxsdk import Client, JWTAuth\n",
        "import pandas as pd\n",
        "import random\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.saving import register_keras_serializable\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "auth = JWTAuth.from_settings_file('key.json')\n",
        "client = Client(auth)\n",
        "\n",
        "\n",
        "@register_keras_serializable(package=\"custom_layers\")\n",
        "def compute_accel(speed_tensor):\n",
        "    # ‚Ä¶ your numpy/tf logic ‚Ä¶\n",
        "    return acceleration_tensor\n",
        "\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "print(\"‚úÖ All libraries imported successfully\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z4oDAKdSElhK",
        "outputId": "21266cfc-3c3e-42a7-d661-ee3b94fb283d"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ All libraries imported successfully\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Processing functions"
      ],
      "metadata": {
        "id": "Cg4LXHlyF9tG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_array_string(array_str):\n",
        "    \"\"\"Parse array strings with error handling\"\"\"\n",
        "    try:\n",
        "        if pd.isna(array_str): return []\n",
        "        cleaned = str(array_str).replace('inf', '0').replace('-inf', '0').replace('nan', 'None')\n",
        "        return eval(cleaned)\n",
        "    except:\n",
        "        return []\n",
        "\n",
        "def interpolate_missing_values(sequence):\n",
        "    \"\"\"Handle missing values using linear interpolation\"\"\"\n",
        "    \"\"\"\n",
        "    Interpolate only None or np.nan values in the sequence.\n",
        "    Leave -inf and +inf unchanged.\n",
        "    \"\"\"\n",
        "    if not sequence:\n",
        "        return []\n",
        "    arr = np.array(sequence, dtype=float)\n",
        "    # Identify missing values (None or np.nan)\n",
        "    missing_mask = np.isnan(arr)\n",
        "    # Valid values are finite and not missing\n",
        "    valid_mask = ~missing_mask & np.isfinite(arr)\n",
        "    valid_indices = np.where(valid_mask)[0]\n",
        "\n",
        "    # If all are missing, return zeros except for -inf/+inf\n",
        "    if np.all(missing_mask | ~np.isfinite(arr)):\n",
        "        return [v if np.isinf(v) else 0.0 for v in arr]\n",
        "\n",
        "    # If only one valid value, fill missing with that value (but keep infs)\n",
        "    if len(valid_indices) == 1:\n",
        "        fill_value = arr[valid_indices[0]]\n",
        "        return [\n",
        "            v if not np.isnan(v) else fill_value\n",
        "            for v in arr\n",
        "        ]\n",
        "\n",
        "    # Standard case: interpolate only missing values, leave infs untouched\n",
        "    interp_values = np.copy(arr)\n",
        "    # Indices to interpolate: missing and not inf\n",
        "    interp_indices = np.where(missing_mask & ~np.isinf(arr))[0]\n",
        "    if len(valid_indices) >= 2 and len(interp_indices) > 0:\n",
        "        # Interpolate only where needed\n",
        "        interp_result = np.interp(\n",
        "            interp_indices, valid_indices, arr[valid_mask]\n",
        "        )\n",
        "        interp_values[interp_indices] = interp_result\n",
        "\n",
        "    # Convert to list and return\n",
        "    return interp_values.tolist()\n",
        "\n",
        "def parse_coordinate_string(coord_str):\n",
        "    \"\"\"Parse coordinate data with interpolation\"\"\"\n",
        "    try:\n",
        "        if pd.isna(coord_str): return []\n",
        "        cleaned_str = str(coord_str).replace('inf', '0').replace('-inf', '0').replace('nan', 'None')\n",
        "        coords = eval(cleaned_str)\n",
        "        x_coords = [c[0] for c in coords]\n",
        "        y_coords = [c[1] for c in coords]\n",
        "        return list(zip(\n",
        "            interpolate_missing_values(x_coords),\n",
        "            interpolate_missing_values(y_coords)\n",
        "        ))\n",
        "    except:\n",
        "        return []\n",
        "\n",
        "def calculate_lateral_speeds(front_coords, rear_coords):\n",
        "    \"\"\"Calculate lateral movement speeds\"\"\"\n",
        "    if len(front_coords) < 2: return []\n",
        "    lateral_speeds = []\n",
        "    for i in range(1, len(front_coords)):\n",
        "        try:\n",
        "            dx = front_coords[i][0] - front_coords[i-1][0]\n",
        "            dy = front_coords[i][1] - front_coords[i-1][1]\n",
        "            lateral_speeds.append(np.sqrt(dx**2 + dy**2))\n",
        "        except:\n",
        "            lateral_speeds.append(0)\n",
        "    return interpolate_missing_values(lateral_speeds)\n"
      ],
      "metadata": {
        "id": "BjE3mvMzFv6C"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_vehicle_labels(df):\n",
        "    \"\"\"Extract labels directly from VehTypeName column\"\"\"\n",
        "    def map_vehicle_type(veh_type_name):\n",
        "        veh_type = str(veh_type_name).strip()\n",
        "        if 'CAV' in veh_type:\n",
        "            return 'autonomous'  # Will be excluded from training\n",
        "        elif 'Aggressive' in veh_type:\n",
        "            return 'aggressive'\n",
        "        elif 'Cooperative' in veh_type:\n",
        "            return 'cooperative'\n",
        "        elif 'Conventional' in veh_type or 'Gipps' in veh_type:\n",
        "            return 'normal'\n",
        "        else:\n",
        "            return 'normal'  # Default classification\n",
        "\n",
        "    df['behavior_label'] = df['VehTypeName'].apply(map_vehicle_type)\n",
        "    return df"
      ],
      "metadata": {
        "id": "U_mUvROnQ6K7"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Feature extraction and if any record is more than half empty then it is discarded"
      ],
      "metadata": {
        "id": "f0_t3CzdF_9n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_speed_position_features(vehicle_row, min_sequence_length=5):\n",
        "    \"\"\"\n",
        "    Extract only speed and position sequences from vehicle data row.\n",
        "    Returns None if data is insufficient.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Parse speed sequence\n",
        "        speeds = parse_array_string(vehicle_row['Speeds'])\n",
        "        # Parse front coordinates (positions)\n",
        "        front_coords = parse_coordinate_string(vehicle_row['VehFrontCoords'])\n",
        "\n",
        "        # Validation: Check for at least 30 invalid values in any sequence\n",
        "        def count_invalid(seq):\n",
        "            return sum(\n",
        "                (v is None) or\n",
        "                (isinstance(v, float) and (np.isnan(v) or np.isinf(v)))\n",
        "                for v in seq\n",
        "            )\n",
        "\n",
        "        if count_invalid(speeds) >= 30 or count_invalid(front_coords) >= 30:\n",
        "            return None\n",
        "\n",
        "        # Clean sequences\n",
        "        speeds_clean = interpolate_missing_values(speeds)\n",
        "        positions_clean = front_coords  # Already interpolated by parse_coordinate_string\n",
        "\n",
        "        # Ensure consistent length\n",
        "        min_length = min(len(speeds_clean), len(positions_clean))\n",
        "        if min_length < min_sequence_length:\n",
        "            return None\n",
        "\n",
        "        # Truncate to same length\n",
        "        speeds_clean = speeds_clean[:min_length]\n",
        "        positions_clean = positions_clean[:min_length]\n",
        "\n",
        "        # Convert to arrays\n",
        "        speeds_arr = np.array(speeds_clean).reshape(-1, 1)\n",
        "        positions_arr = np.array(positions_clean).reshape(-1, 2)\n",
        "\n",
        "        return {\n",
        "            'speeds': speeds_arr,          # shape (N, 1)\n",
        "            'positions': positions_arr,    # shape (N, 2)\n",
        "            'sequence_length': min_length\n",
        "        }\n",
        "    except Exception as e:\n",
        "        print(f\"Feature extraction error: {e}\")\n",
        "        return None\n"
      ],
      "metadata": {
        "id": "ngSBzjcLF8rM"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data reading function"
      ],
      "metadata": {
        "id": "47gNKBafEzBN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def read_aimsun_data(file_path, column_map=None):\n",
        "    \"\"\"Read CSV with flexible column mapping\"\"\"\n",
        "    DEFAULT_MAP = {\n",
        "        'vehicle_id': 'VehNr', 'timestep': 'Timestep', 'speed': 'Speeds',\n",
        "        'acceleration': 'Accelerations', 'front_coords': 'VehFrontCoords',\n",
        "        'rear_coords': 'VehRearCoords', 'vehicle_type': 'VehTypeName', 'length': 'Length'\n",
        "    }\n",
        "    col_map = column_map or DEFAULT_MAP\n",
        "    try:\n",
        "        df = pd.read_csv(file_path)\n",
        "        return df.rename(columns={v: k for k, v in col_map.items()})\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading {file_path}: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "def extract_vehicle_table(df, vehicle_id):\n",
        "    \"\"\"Extract time-series table for specific vehicle\"\"\"\n",
        "    vehicle_data = df[df['vehicle_id'] == vehicle_id].copy()\n",
        "    vehicle_data['front_coords'] = vehicle_data['front_coords'].apply(parse_coordinate_string)\n",
        "    vehicle_data['rear_coords'] = vehicle_data['rear_coords'].apply(parse_coordinate_string)\n",
        "    vehicle_data['position'] = vehicle_data.apply(\n",
        "        lambda x: np.mean([x['front_coords'], x['rear_coords']], axis=0), axis=1)\n",
        "    return vehicle_data[['timestep', 'speed', 'acceleration', 'position']]\n",
        "\n",
        "print(\"‚úÖ Data reading functions ready\")\n"
      ],
      "metadata": {
        "id": "jTs8hiznEo0o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a024fb28-44bd-4a02-91ae-e592f65f5bf3"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Data reading functions ready\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def process_labeled_data_speed_position(csv_files, folder_path, max_files=None):\n",
        "    \"\"\"\n",
        "    Process data files using only speed and position features.\n",
        "    \"\"\"\n",
        "    all_features = []\n",
        "    all_labels = []\n",
        "    #vehicle_details = []\n",
        "    processed_count = 0\n",
        "\n",
        "    files_to_process = csv_files[:max_files] if max_files else csv_files\n",
        "\n",
        "    for filename in files_to_process:\n",
        "        try:\n",
        "            print(f\"üìÑ Processing {filename}...\")\n",
        "            file_path = os.path.join(folder_path, filename)\n",
        "            df = pd.read_csv(file_path)\n",
        "\n",
        "            # Extract labels from VehTypeName\n",
        "            df = extract_vehicle_labels(df)\n",
        "\n",
        "            print(f\"   Labels in {filename}:\")\n",
        "            file_labels = df['behavior_label'].value_counts()\n",
        "            for label, count in file_labels.items():\n",
        "                print(f\"     {label}: {count}\")\n",
        "\n",
        "            for idx, row in df.iterrows():\n",
        "                if row['behavior_label'] == 'autonomous':\n",
        "                    continue\n",
        "\n",
        "                features = extract_speed_position_features(row)\n",
        "                if features is not None:\n",
        "                    all_features.append(features)\n",
        "                    all_labels.append(row['behavior_label'])\n",
        "                    processed_count += 1\n",
        "\n",
        "            print(f\"   ‚úÖ Extracted {processed_count} valid vehicles so far\")\n",
        "        except Exception as e:\n",
        "            print(f\"   ‚ùå Error: {e}\")\n",
        "\n",
        "    return all_features, all_labels\n"
      ],
      "metadata": {
        "id": "ms4Ms3QEKjEa"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_speed_position_data(features_list, max_sequence=60):\n",
        "    \"\"\"\n",
        "    Prepare padded speed and position arrays for model input.\n",
        "    Returns:\n",
        "        X_speed: (num_samples, max_sequence, 1)\n",
        "        X_pos: (num_samples, max_sequence, 2)\n",
        "    \"\"\"\n",
        "    X_speed = []\n",
        "    X_pos = []\n",
        "    for features in features_list:\n",
        "        speeds = features['speeds'][:max_sequence]\n",
        "        positions = features['positions'][:max_sequence]\n",
        "        seq_len = min(len(speeds), max_sequence)\n",
        "\n",
        "        # Pad if needed\n",
        "        speed_padded = np.zeros((max_sequence, 1))\n",
        "        pos_padded = np.zeros((max_sequence, 2))\n",
        "        speed_padded[:seq_len] = speeds[:seq_len]\n",
        "        pos_padded[:seq_len] = positions[:seq_len]\n",
        "\n",
        "        X_speed.append(speed_padded)\n",
        "        X_pos.append(pos_padded)\n",
        "    return np.array(X_speed), np.array(X_pos)\n"
      ],
      "metadata": {
        "id": "NjHLMrcCWEjs"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Replace your build_speed_position_model_fixed function with this version:\n",
        "\n",
        "def build_speed_position_model_fixed(sequence_length=60, num_classes=3):\n",
        "    \"\"\"\n",
        "    Fixed model that properly handles multiple classes and computes\n",
        "    acceleration/lateral movement correctly. Fixed cuDNN compatibility.\n",
        "    \"\"\"\n",
        "    # Inputs\n",
        "    speed_input = Input(shape=(sequence_length, 1), name='speed')\n",
        "    position_input = Input(shape=(sequence_length, 2), name='position')\n",
        "\n",
        "    # Compute acceleration (difference between consecutive speeds)\n",
        "    def compute_acceleration(speed_tensor):\n",
        "        # Pad with zeros at the beginning for the first timestep\n",
        "        zeros = tf.zeros_like(speed_tensor[:, :1, :])\n",
        "        accel = speed_tensor[:, 1:, :] - speed_tensor[:, :-1, :]\n",
        "        return tf.concat([zeros, accel], axis=1)\n",
        "\n",
        "    accel = Lambda(compute_acceleration, name='acceleration')(speed_input)\n",
        "\n",
        "    # Compute lateral movement (Euclidean distance between consecutive positions)\n",
        "    def compute_lateral_movement(pos_tensor):\n",
        "        # Calculate displacement vectors\n",
        "        displacement = pos_tensor[:, 1:, :] - pos_tensor[:, :-1, :]\n",
        "        # Calculate magnitudes (lateral speed)\n",
        "        lateral_speed = tf.norm(displacement, axis=-1, keepdims=True)\n",
        "        # Pad with zeros at the beginning\n",
        "        zeros = tf.zeros_like(lateral_speed[:, :1, :])\n",
        "        return tf.concat([zeros, lateral_speed], axis=1)\n",
        "\n",
        "    lateral = Lambda(compute_lateral_movement, name='lateral')(position_input)\n",
        "\n",
        "    # Concatenate all features\n",
        "    features = Concatenate(axis=-1)([speed_input, accel, lateral])  # shape: (batch, seq, 3)\n",
        "\n",
        "    # REMOVE MASKING - this causes cuDNN issues\n",
        "    # masked = Masking(mask_value=0.0)(features)\n",
        "\n",
        "    # CNN branch - work directly on features\n",
        "    cnn = Conv1D(64, 3, activation='relu', padding='same')(features)\n",
        "    cnn = BatchNormalization()(cnn)\n",
        "    cnn = Dropout(0.3)(cnn)\n",
        "    cnn = Conv1D(128, 3, activation='relu', padding='same')(cnn)\n",
        "    cnn = BatchNormalization()(cnn)\n",
        "    cnn_out = GlobalMaxPooling1D()(cnn)\n",
        "\n",
        "    # LSTM branch - disable cuDNN to avoid masking issues\n",
        "    lstm = LSTM(128, return_sequences=True, use_cudnn=False)(features)  # FIX: Disable cuDNN\n",
        "    lstm = BatchNormalization()(lstm)\n",
        "    lstm_out = LSTM(64, use_cudnn=False)(lstm)  # FIX: Disable cuDNN\n",
        "\n",
        "    # Feature fusion\n",
        "    combined = Concatenate()([cnn_out, lstm_out])\n",
        "    x = Dense(256, activation='relu')(combined)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Dropout(0.5)(x)\n",
        "    x = Dense(128, activation='relu')(x)\n",
        "\n",
        "    # FIXED: Output layer should have num_classes neurons, not 1\n",
        "    output = Dense(num_classes, activation='softmax')(x)\n",
        "\n",
        "    model = Model(inputs=[speed_input, position_input], outputs=output)\n",
        "\n",
        "    # Use appropriate optimizer and learning rate\n",
        "    model.compile(\n",
        "        optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
        "        loss='sparse_categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "\n",
        "    print(\"üß† Fixed Model Architecture:\")\n",
        "    model.summary()\n",
        "    return model\n",
        "\n",
        "# Also fix your prepare_speed_position_data function:\n",
        "\n",
        "def prepare_speed_position_data(features_list, max_sequence=60):\n",
        "    \"\"\"\n",
        "    Prepare padded speed and position arrays for model input.\n",
        "    Fixed for cuDNN compatibility.\n",
        "    Returns:\n",
        "        X_speed: (num_samples, max_sequence, 1)\n",
        "        X_pos: (num_samples, max_sequence, 2)\n",
        "    \"\"\"\n",
        "    X_speed = []\n",
        "    X_pos = []\n",
        "    for features in features_list:\n",
        "        speeds = features['speeds'][:max_sequence]\n",
        "        positions = features['positions'][:max_sequence]\n",
        "        seq_len = min(len(speeds), max_sequence)\n",
        "\n",
        "        # Pad if needed - ensure proper dtype\n",
        "        speed_padded = np.zeros((max_sequence, 1), dtype=np.float32)  # FIX: Add dtype\n",
        "        pos_padded = np.zeros((max_sequence, 2), dtype=np.float32)    # FIX: Add dtype\n",
        "        speed_padded[:seq_len] = speeds[:seq_len]\n",
        "        pos_padded[:seq_len] = positions[:seq_len]\n",
        "\n",
        "        X_speed.append(speed_padded)\n",
        "        X_pos.append(pos_padded)\n",
        "    return np.array(X_speed, dtype=np.float32), np.array(X_pos, dtype=np.float32)  # FIX: Ensure dtype"
      ],
      "metadata": {
        "id": "1m9Vc8MLLr4w"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "graph TD\n",
        "\n",
        "    TS[Time Series Input<br/>60 x 3] --> Mask[Masking Layer]\n",
        "    Mask --> CNN1[Conv1D 64<br/>Kernel=3]\n",
        "    CNN1 --> BN1[BatchNorm]\n",
        "    BN1 --> Drop1[Dropout 0.3]\n",
        "    Drop1 --> CNN2[Conv1D 128<br/>Kernel=3]\n",
        "    CNN2 --> BN2[BatchNorm]\n",
        "    BN2 --> GMP[GlobalMaxPooling1D]\n",
        "    \n",
        "    Mask --> LSTM1[LSTM 128<br/>return_sequences=True]\n",
        "    LSTM1 --> BN3[BatchNorm]\n",
        "    BN3 --> LSTM2[LSTM 64]\n",
        "    \n",
        "    S[Static Features<br/>13 dims] --> Dense1[Dense 64]\n",
        "    Dense1 --> BN4[BatchNorm]\n",
        "    \n",
        "    GMP --> Concat[Concatenate]\n",
        "    LSTM2 --> Concat\n",
        "    BN4 --> Concat\n",
        "    \n",
        "    Concat --> Dense2[Dense 256]\n",
        "    Dense2 --> BN5[BatchNorm]\n",
        "    BN5 --> Drop2[Dropout 0.5]\n",
        "    Drop2 --> Dense3[Dense 128]\n",
        "    Dense3 --> Output[Softmax Output<br/>3 Classes]\n"
      ],
      "metadata": {
        "id": "AELTfAu-LuCN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calculate the F1 score"
      ],
      "metadata": {
        "id": "_KFMVLbeKpxk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def classify_vehicle_behavior_speed_position(vehicle_data, model, max_sequence=60):\n",
        "    \"\"\"\n",
        "    Classify vehicle behavior using only speed and position sequences.\n",
        "    \"\"\"\n",
        "    features = extract_speed_position_features(vehicle_data)\n",
        "    if not features:\n",
        "        return \"unknown\"\n",
        "\n",
        "    speeds = features['speeds'][:max_sequence]\n",
        "    positions = features['positions'][:max_sequence]\n",
        "    seq_len = min(len(speeds), max_sequence)\n",
        "\n",
        "    # Pad\n",
        "    speed_padded = np.zeros((1, max_sequence, 1))\n",
        "    pos_padded = np.zeros((1, max_sequence, 2))\n",
        "    speed_padded[0, :seq_len] = speeds\n",
        "    pos_padded[0, :seq_len] = positions\n",
        "\n",
        "    prediction = model.predict([speed_padded, pos_padded])\n",
        "    class_id = np.argmax(prediction)\n",
        "    return {0: 'aggressive', 1: 'cooperative', 2: 'normal'}.get(class_id, 'unknown')\n"
      ],
      "metadata": {
        "id": "Xv_LU62xLhDy"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def analyze_f1_performance(comparisons, actual_labels, predictions):\n",
        "    \"\"\"Detailed F1 score analysis with visualizations\"\"\"\n",
        "\n",
        "    # Classification report\n",
        "    print(\"\\nüìã Detailed Classification Report:\")\n",
        "    print(\"=\" * 60)\n",
        "    print(classification_report(actual_labels, predictions))\n",
        "\n",
        "    # F1 scores by confidence ranges\n",
        "    df_comp = pd.DataFrame(comparisons)\n",
        "    confidence_ranges = [(0.0, 0.5), (0.5, 0.7), (0.7, 0.9), (0.9, 1.0)]\n",
        "\n",
        "    print(\"\\nüìä F1 Score by Confidence Ranges:\")\n",
        "    print(\"-\" * 50)\n",
        "    for low, high in confidence_ranges:\n",
        "        mask = (df_comp['confidence'] >= low) & (df_comp['confidence'] < high)\n",
        "        subset = df_comp[mask]\n",
        "        if len(subset) > 0:\n",
        "            subset_f1 = f1_score(subset['actual'], subset['predicted'], average='macro')\n",
        "            print(f\"Confidence {low:.1f}-{high:.1f}: F1={subset_f1:.3f} (n={len(subset)})\")\n",
        "\n",
        "    # Misclassified vehicles analysis\n",
        "    print(f\"\\n‚ùå Misclassified Vehicles Analysis:\")\n",
        "    misclassified = df_comp[df_comp['correct'] == False]\n",
        "    if not misclassified.empty:\n",
        "        print(f\"Total misclassified: {len(misclassified)}\")\n",
        "        for behavior in ['aggressive', 'cooperative', 'normal']:\n",
        "            behavior_miss = misclassified[misclassified['actual'] == behavior]\n",
        "            if len(behavior_miss) > 0:\n",
        "                print(f\"  {behavior}: {len(behavior_miss)} vehicles\")\n",
        "\n",
        "def plot_f1_results(comparisons, actual_labels, predictions):\n",
        "    \"\"\"Visualize F1 score results\"\"\"\n",
        "\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
        "\n",
        "    # 1. F1 scores by class\n",
        "    classes = sorted(set(actual_labels + predictions))\n",
        "    if len(classes) > 1:\n",
        "        f1_per_class = f1_score(actual_labels, predictions, average=None, labels=classes)\n",
        "        axes[0,0].bar(classes, f1_per_class, color=['#ff9999','#66b3ff','#99ff99'])\n",
        "        axes[0,0].set_title('F1 Score by Vehicle Behavior Class')\n",
        "        axes[0,0].set_ylabel('F1 Score')\n",
        "        axes[0,0].set_ylim(0, 1.1)\n",
        "\n",
        "    # 2. Prediction confidence distribution\n",
        "    df_comp = pd.DataFrame(comparisons)\n",
        "    axes[0,1].hist([df_comp[df_comp['correct']]['confidence'],\n",
        "                   df_comp[~df_comp['correct']]['confidence']],\n",
        "                  bins=20, alpha=0.7, label=['Correct', 'Incorrect'])\n",
        "    axes[0,1].set_title('Prediction Confidence Distribution')\n",
        "    axes[0,1].set_xlabel('Confidence')\n",
        "    axes[0,1].legend()\n",
        "\n",
        "    # 3. Confusion matrix\n",
        "    if len(classes) > 1:\n",
        "        from sklearn.metrics import confusion_matrix\n",
        "        import seaborn as sns\n",
        "        cm = confusion_matrix(actual_labels, predictions, labels=classes)\n",
        "        sns.heatmap(cm, annot=True, fmt='d', xticklabels=classes, yticklabels=classes, ax=axes[1,0])\n",
        "        axes[1,0].set_title('Confusion Matrix')\n",
        "        axes[1,0].set_xlabel('Predicted')\n",
        "        axes[1,0].set_ylabel('Actual')\n",
        "\n",
        "    # 4. F1 vs Accuracy comparison\n",
        "    accuracy = df_comp['correct'].mean()\n",
        "    f1_macro = f1_score(actual_labels, predictions, average='macro') if len(classes) > 1 else accuracy\n",
        "\n",
        "    metrics = ['Accuracy', 'F1-Macro']\n",
        "    values = [accuracy, f1_macro]\n",
        "    axes[1,1].bar(metrics, values, color=['lightblue', 'lightcoral'])\n",
        "    axes[1,1].set_title('Accuracy vs F1 Score')\n",
        "    axes[1,1].set_ylabel('Score')\n",
        "    axes[1,1].set_ylim(0, 1.1)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "print(\"‚úÖ F1 analysis and visualization functions ready\")\n"
      ],
      "metadata": {
        "id": "rxl7qvJNLlyY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7629acde-8a79-43d0-a15a-83594b1a2e4b"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ F1 analysis and visualization functions ready\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def compare_predictions_with_labels_f1_speed_position(model, features_list, vehicle_details, label_encoder, max_sequence=60):\n",
        "    \"\"\"\n",
        "    Compare model predictions with actual vehicle labels using F1 score.\n",
        "    \"\"\"\n",
        "    predictions = []\n",
        "    actual_labels = []\n",
        "    comparisons = []\n",
        "\n",
        "    print(\"üîç Comparing predictions with actual labels (F1 Score Evaluation)...\")\n",
        "    print(\"-\" * 80)\n",
        "    print(f\"{'VehNr':<8} {'Actual Label':<15} {'Predicted':<15} {'Match':<8} {'Confidence':<12}\")\n",
        "    print(\"-\" * 80)\n",
        "\n",
        "    correct_predictions = 0\n",
        "\n",
        "    X_speed, X_pos = prepare_speed_position_data(features_list, max_sequence=max_sequence)\n",
        "\n",
        "    for i, vehicle_info in enumerate(vehicle_details):\n",
        "        # Predict\n",
        "        prediction_probs = model.predict([X_speed[i:i+1], X_pos[i:i+1]], verbose=0)\n",
        "        predicted_class_idx = np.argmax(prediction_probs)\n",
        "        predicted_label = label_encoder.inverse_transform([predicted_class_idx])[0]\n",
        "        confidence = np.max(prediction_probs)\n",
        "        actual_label = vehicle_info['actual_label']\n",
        "        is_correct = predicted_label == actual_label\n",
        "\n",
        "        if is_correct:\n",
        "            correct_predictions += 1\n",
        "\n",
        "        predictions.append(predicted_label)\n",
        "        actual_labels.append(actual_label)\n",
        "        comparison = {\n",
        "            'VehNr': vehicle_info['VehNr'],\n",
        "            'actual': actual_label,\n",
        "            'predicted': predicted_label,\n",
        "            'correct': is_correct,\n",
        "            'confidence': confidence,\n",
        "            'file': vehicle_info['file']\n",
        "        }\n",
        "        comparisons.append(comparison)\n",
        "\n",
        "        if i < 20:\n",
        "            match_symbol = \"‚úÖ\" if is_correct else \"‚ùå\"\n",
        "            print(f\"{vehicle_info['VehNr']:<8} {actual_label:<15} {predicted_label:<15} {match_symbol:<8} {confidence:.3f}\")\n",
        "\n",
        "    # Calculate F1 scores\n",
        "    try:\n",
        "        f1_macro = f1_score(actual_labels, predictions, average='macro')\n",
        "        f1_weighted = f1_score(actual_labels, predictions, average='weighted')\n",
        "        f1_per_class = f1_score(actual_labels, predictions, average=None, labels=label_encoder.classes_)\n",
        "        precision_macro = precision_score(actual_labels, predictions, average='macro')\n",
        "        recall_macro = recall_score(actual_labels, predictions, average='macro')\n",
        "        accuracy = correct_predictions / len(vehicle_details)\n",
        "\n",
        "        print(\"-\" * 80)\n",
        "        print(f\"üéØ Overall Accuracy: {accuracy:.3f} ({correct_predictions}/{len(vehicle_details)})\")\n",
        "        print(f\"üìä F1 Score (Macro): {f1_macro:.3f}\")\n",
        "        print(f\"üìä F1 Score (Weighted): {f1_weighted:.3f}\")\n",
        "        print(f\"üìä Precision (Macro): {precision_macro:.3f}\")\n",
        "        print(f\"üìä Recall (Macro): {recall_macro:.3f}\")\n",
        "\n",
        "        print(\"\\nüè∑Ô∏è F1 Score per Class:\")\n",
        "        for i, class_name in enumerate(label_encoder.classes_):\n",
        "            if i < len(f1_per_class):\n",
        "                print(f\"  {class_name:15s}: {f1_per_class[i]:.3f}\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è Could not calculate F1 scores: {e}\")\n",
        "        f1_macro = f1_weighted = 0.0\n",
        "\n",
        "    return comparisons, accuracy, f1_macro, f1_weighted\n"
      ],
      "metadata": {
        "id": "6kh-oGxmKpJ_"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import io\n",
        "import json\n",
        "import os\n",
        "import hashlib\n",
        "\n",
        "class ContinuousLearningSystem:\n",
        "\n",
        "    def __init__(self, model_path, scaler_path, le_path, data_path, processed_path, box_client=None, box_folder=None):\n",
        "        self.model_path = model_path\n",
        "        self.scaler_path = scaler_path\n",
        "        self.le_path = le_path\n",
        "        self.data_path = data_path\n",
        "        self.processed_path = processed_path\n",
        "        self.processed_hashes = self.load_processed_hashes()\n",
        "        self.box_client = box_client\n",
        "        self.box_folder = box_folder\n",
        "\n",
        "        # Load existing model artifacts if they exist\n",
        "        self.model = self.load_model()\n",
        "        self.scaler = self.load_scaler()\n",
        "        self.label_encoder = self.load_label_encoder()\n",
        "\n",
        "\n",
        "    def load_processed_hashes(self):\n",
        "        try:\n",
        "            if os.path.exists(self.processed_path):\n",
        "                with open(self.processed_path, 'r') as f:\n",
        "                    return set(json.load(f))\n",
        "            else:\n",
        "                return set()\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading processed hashes: {e}\")\n",
        "            return set()\n",
        "    def load_model(self):\n",
        "        try:\n",
        "            if os.path.exists(self.model_path):\n",
        "                print(f\"Loading existing model from {self.model_path}\")\n",
        "                return load_model(self.model_path)\n",
        "            else:\n",
        "                print(\"No existing model found. A new model will be built.\")\n",
        "                return None\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading model from {self.model_path}: {e}\")\n",
        "            return None\n",
        "\n",
        "    def load_scaler(self):\n",
        "        try:\n",
        "            if os.path.exists(self.scaler_path):\n",
        "                print(f\"Loading existing scaler from {self.scaler_path}\")\n",
        "                return joblib.load(self.scaler_path)\n",
        "            else:\n",
        "                print(\"No existing scaler found.\")\n",
        "                return None\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading scaler from {self.scaler_path}: {e}\")\n",
        "            return None\n",
        "\n",
        "    def load_label_encoder(self):\n",
        "        try:\n",
        "            if os.path.exists(self.le_path):\n",
        "                print(f\"Loading existing label encoder from {self.le_path}\")\n",
        "                return joblib.load(self.le_path)\n",
        "            else:\n",
        "                print(\"No existing label encoder found.\")\n",
        "                return None\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading label encoder from {self.le_path}: {e}\")\n",
        "            return None\n",
        "\n",
        "\n",
        "\n",
        "    def save_processed_hashes(self):\n",
        "        try:\n",
        "            with open(self.processed_path, 'w') as f:\n",
        "                json.dump(list(self.processed_hashes), f)\n",
        "        except Exception as e:\n",
        "            print(f\"Error saving processed hashes: {e}\")\n",
        "\n",
        "    def csv_stream_hash(self, csv_stream):\n",
        "        pos = csv_stream.tell()\n",
        "        csv_stream.seek(0)\n",
        "        content = csv_stream.read()\n",
        "        csv_stream.seek(pos)  # Restore position\n",
        "        return hashlib.sha256(content.encode('utf-8')).hexdigest()\n",
        "\n",
        "    def stream_all_csv_files_from_box(self, folder):\n",
        "        \"\"\"Yield (file_name, csv_stream) for every CSV file in Box folder and subfolders.\"\"\"\n",
        "        for item in folder.get_items(limit=2000):\n",
        "            if item.type == 'folder':\n",
        "                yield from self.stream_all_csv_files_from_box(self.box_client.folder(item.id))\n",
        "            elif item.type == 'file' and item.name.endswith('.csv'):\n",
        "                file_content = item.content()\n",
        "                csv_stream = io.StringIO(file_content.decode('utf-8'))\n",
        "                yield item.name, csv_stream\n",
        "\n",
        "    def get_new_box_files(self):\n",
        "        \"\"\"Get new CSV files from Box that have not been processed yet.\"\"\"\n",
        "        new_files = []\n",
        "        if not self.box_client or not self.box_folder:\n",
        "            print(\"Box client or folder not configured.\")\n",
        "            return new_files\n",
        "\n",
        "        for file_name, csv_stream in self.stream_all_csv_files_from_box(self.box_folder):\n",
        "            file_hash = self.csv_stream_hash(csv_stream)\n",
        "            if file_hash not in self.processed_hashes:\n",
        "                new_files.append((file_name, csv_stream, file_hash))\n",
        "        return new_files\n",
        "\n",
        "\n",
        "    def update_model(self):\n",
        "        model = self.model\n",
        "        scaler = self.scaler\n",
        "        label_encoder = self.label_encoder\n",
        "\n",
        "        # fetch all file lists\n",
        "        train_files = [...]    # wherever you get these\n",
        "        test_files  = [...]\n",
        "\n",
        "        # Process existing training & testing data\n",
        "        train_features, train_labels, _ = self.process_labeled_data_streams(train_files)\n",
        "        test_features,  test_labels,  _ = self.process_labeled_data_streams(test_files)\n",
        "\n",
        "        # Check for newly arrived data\n",
        "        new_files = self.get_new_box_files()\n",
        "        if not new_files:\n",
        "            print(\"No new files to process from Box\")\n",
        "            return model, scaler, label_encoder\n",
        "\n",
        "        new_features, new_labels, _ = self.process_labeled_data_streams(new_files)\n",
        "        if not new_features:\n",
        "            print(\"No valid features extracted from new Box files\")\n",
        "            return model, scaler, label_encoder\n",
        "\n",
        "        # (re)fit scaler & encoder if needed\n",
        "        if scaler is None:\n",
        "            print(\"Fitting new scaler on new data\")\n",
        "            static_feats = [f['static'] for f in new_features]\n",
        "            scaler = StandardScaler().fit(static_feats)\n",
        "            self.scaler = scaler\n",
        "\n",
        "        if label_encoder is None:\n",
        "            print(\"Fitting new label encoder on new data\")\n",
        "            label_encoder = LabelEncoder().fit(new_labels)\n",
        "            self.label_encoder = label_encoder\n",
        "\n",
        "        # ‚Äî‚Äî NEW: split incoming new_features/new_labels 80/20 ‚Äî‚Äî #\n",
        "        new_train_feats, new_test_feats, new_train_lbls, new_test_lbls = train_test_split(\n",
        "            new_features,\n",
        "            new_labels,\n",
        "            test_size=0.2,\n",
        "            random_state=42,\n",
        "            stratify=new_labels  # if you want to preserve class balance\n",
        "        )\n",
        "\n",
        "        # Merge splits into your main train/test sets\n",
        "        train_features.extend(new_train_feats)\n",
        "        train_labels.extend(new_train_lbls)\n",
        "        test_features.extend(new_test_feats)\n",
        "        test_labels.extend(new_test_lbls)\n",
        "\n",
        "        # Prepare model inputs\n",
        "        X_ts_train, X_static_train, y_train = self.prepare_new_data(\n",
        "            train_features, train_labels, scaler, label_encoder\n",
        "        )\n",
        "        X_ts_test,  X_static_test,  y_test  = self.prepare_new_data(\n",
        "            test_features, test_labels, scaler, label_encoder\n",
        "        )\n",
        "\n",
        "        # Build model if first time\n",
        "        if model is None:\n",
        "            print(\"Building a new model for initial training.\")\n",
        "            num_features         = X_ts_train.shape[-1] if X_ts_train.size else 3\n",
        "            static_feature_count = X_static_train.shape[-1] if X_static_train.size else 13\n",
        "            num_classes          = len(label_encoder.classes_) if label_encoder else 3\n",
        "\n",
        "            model = build_speed_position_model(\n",
        "                sequence_length=X_ts_train.shape[1],\n",
        "                num_features=num_features,\n",
        "                static_feature_count=static_feature_count,\n",
        "                num_classes=num_classes\n",
        "            )\n",
        "            self.model = model\n",
        "\n",
        "        # Train on the enlarged training set\n",
        "        print(f\"Training model on {len(train_features)} total training samples\")\n",
        "        model.fit(\n",
        "            [X_ts_train, X_static_train],\n",
        "            y_train,\n",
        "            epochs=3,\n",
        "            batch_size=32,\n",
        "            validation_split=0.2\n",
        "        )\n",
        "\n",
        "        # (Optional) Evaluate on the enlarged test set\n",
        "        loss, acc = model.evaluate([X_ts_test, X_static_test], y_test, verbose=0)\n",
        "        print(f\"Test loss: {loss:.4f}, Test accuracy: {acc:.4f}\")\n",
        "\n",
        "        # Save everything\n",
        "        self.save_model()\n",
        "        self.save_scaler()\n",
        "        self.save_label_encoder()\n",
        "\n",
        "        # Mark new files as processed\n",
        "        for _, _, fhash in new_files:\n",
        "            self.processed_hashes.add(fhash)\n",
        "        self.save_processed_hashes()\n",
        "\n",
        "        print(f\"Updated model with {len(new_features)} new samples \"\n",
        "              f\"({len(new_train_feats)}‚Üítrain, {len(new_test_feats)}‚Üítest)\")\n",
        "        return self.model, self.scaler, self.label_encoder\n",
        "\n",
        "    def process_labeled_data_streams(self, file_streams):\n",
        "        \"\"\"Process labeled data from a list of (file_name, csv_stream) tuples.\"\"\"\n",
        "        all_features = []\n",
        "        all_labels = []\n",
        "        vehicle_details = []\n",
        "        processed_count = 0\n",
        "\n",
        "        for file_name, csv_stream, _ in file_streams:\n",
        "            try:\n",
        "                print(f\"üìÑ Processing {file_name} from Box stream...\")\n",
        "                df = pd.read_csv(csv_stream)\n",
        "                df = extract_vehicle_labels(df)\n",
        "                print(f\"   Labels in {file_name}:\")\n",
        "                file_labels = df['behavior_label'].value_counts()\n",
        "                for label, count in file_labels.items():\n",
        "                    print(f\"     {label}: {count}\")\n",
        "                for idx, row in df.iterrows():\n",
        "                    if row['behavior_label'] == 'autonomous':\n",
        "                        continue\n",
        "                    features = extract_speed_position_features(row)\n",
        "                    if features is not None:\n",
        "                        all_features.append(features)\n",
        "                        all_labels.append(row['behavior_label'])\n",
        "                        vehicle_details.append({\n",
        "                            'VehNr': row['VehNr'],\n",
        "                            'VehTypeName': row['VehTypeName'],\n",
        "                            'actual_label': row['behavior_label'],\n",
        "                            'file': file_name\n",
        "                        })\n",
        "                        processed_count += 1\n",
        "                print(f\"   ‚úÖ Extracted {processed_count} valid vehicles so far\")\n",
        "            except Exception as e:\n",
        "                print(f\"   ‚ùå Error processing {file_name}: {e}\")\n",
        "\n",
        "        return all_features, all_labels, vehicle_details\n",
        "\n",
        "\n",
        "    def save_model(self):\n",
        "        try:\n",
        "            if self.model:\n",
        "                self.model.save(self.model_path)\n",
        "                print(f\"Model saved to {self.model_path}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error saving model to {self.model_path}: {e}\")\n",
        "\n",
        "    def save_scaler(self):\n",
        "        try:\n",
        "            if self.scaler:\n",
        "                joblib.dump(self.scaler, self.scaler_path)\n",
        "                print(f\"Scaler saved to {self.scaler_path}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error saving scaler to {self.scaler_path}: {e}\")\n",
        "\n",
        "    def save_label_encoder(self):\n",
        "        try:\n",
        "            if self.label_encoder:\n",
        "                joblib.dump(self.label_encoder, self.le_path)\n",
        "                print(f\"Label encoder saved to {self.le_path}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error saving label encoder to {self.le_path}: {e}\")\n",
        "\n",
        "    def load_all_data_from_box(self):\n",
        "        \"\"\"\n",
        "        Load all vehicle data from Box into a single list.\n",
        "        Each item: {'features': ..., 'label': ..., 'details': {...}}\n",
        "        \"\"\"\n",
        "        all_data = []\n",
        "        file_counter = 0\n",
        "\n",
        "        for file_name, csv_stream in self.stream_all_csv_files_from_box(self.box_folder):\n",
        "            fhash = self.csv_stream_hash(csv_stream)\n",
        "            if fhash in self.processed_hashes:\n",
        "                continue\n",
        "\n",
        "            try:\n",
        "                csv_stream.seek(0)\n",
        "                df = pd.read_csv(csv_stream)\n",
        "                df = extract_vehicle_labels(df)\n",
        "\n",
        "                for _, row in df.iterrows():\n",
        "                    if row['behavior_label'] == 'autonomous':\n",
        "                        continue\n",
        "                    features = extract_speed_position_features(row)\n",
        "                    if features is not None:\n",
        "                        record = {\n",
        "                            'features': features,\n",
        "                            'label': row['behavior_label'],\n",
        "                            'details': {\n",
        "                                'VehNr': row['VehNr'],\n",
        "                                'VehTypeName': row['VehTypeName'],\n",
        "                                'file': file_name\n",
        "                            }\n",
        "                        }\n",
        "                        all_data.append(record)\n",
        "                self.processed_hashes.add(fhash)\n",
        "                file_counter += 1\n",
        "                if file_counter ==500: return all_data\n",
        "                if file_counter % 10 == 0:\n",
        "                    print(f\"Files loaded: {file_counter}\")\n",
        "                    self.save_processed_hashes()\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing {file_name}: {e}\")\n",
        "\n",
        "        self.save_processed_hashes()\n",
        "        return all_data\n"
      ],
      "metadata": {
        "id": "9LhczUrMPr2g"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Extract the data from the folder"
      ],
      "metadata": {
        "id": "nNTWaDAaJBFf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "DRIVE_PATH = '/content/drive/MyDrive/Test_Output'\n",
        "MODEL_PATH = f'{DRIVE_PATH}/traffic_model.keras'\n",
        "SCALER_PATH = f'{DRIVE_PATH}/scaler.joblib'\n",
        "LE_PATH = f'{DRIVE_PATH}/label_encoder.joblib'\n",
        "PROCESSED_PATH = f'{DRIVE_PATH}/processed.json'\n",
        "\n",
        "os.makedirs(DRIVE_PATH, exist_ok=True)\n",
        "\n",
        "print(\"‚úÖ All output and data will be stored in:\", DRIVE_PATH)\n"
      ],
      "metadata": {
        "id": "CEY68Ww-lOur",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b118a883-289c-412f-e988-c83f12e51fe3"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "‚úÖ All output and data will be stored in: /content/drive/MyDrive/Test_Output\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "main_folder_id = '326383492292'\n",
        "main_folder = client.folder(main_folder_id).get()\n",
        "\n",
        "# Find the \"Parsed Time Series Data\" subfolder\n",
        "parsed_folder = None\n",
        "for item in main_folder.get_items(limit=100):\n",
        "    if item.type == 'folder' and item.name == 'Parsed Time Series Data':\n",
        "        parsed_folder = client.folder(item.id)\n",
        "        break\n",
        "\n",
        "if not parsed_folder:\n",
        "    raise Exception(\"Parsed Time Series Data folder not found.\")"
      ],
      "metadata": {
        "id": "7iXMloUoJA1j"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def debug_scenario_composition(scenario_name='scen3_1rep3', check_all_files=True):\n",
        "    \"\"\"\n",
        "    Debug the vehicle composition across all files in a scenario\n",
        "    \"\"\"\n",
        "    print(f\"üîç COMPREHENSIVE SCENARIO ANALYSIS: {scenario_name}\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    # Navigate to the specific folder\n",
        "    main_folder = client.folder(main_folder_id)\n",
        "    parsed_folder = None\n",
        "\n",
        "    for item in main_folder.get_items():\n",
        "        if item.name == 'Parsed Time Series Data':\n",
        "            parsed_folder = client.folder(item.id)\n",
        "            break\n",
        "\n",
        "    target_folder = None\n",
        "    for item in parsed_folder.get_items():\n",
        "        if item.name == scenario_name:\n",
        "            target_folder = client.folder(item.id)\n",
        "            break\n",
        "\n",
        "    if not target_folder:\n",
        "        print(f\"‚ùå Folder {scenario_name} not found\")\n",
        "        return\n",
        "\n",
        "    # Aggregate statistics across all files\n",
        "    total_vehicles = {}\n",
        "    all_vehicle_types = set()\n",
        "    file_count = 0\n",
        "\n",
        "    # Check all CSV files in the folder\n",
        "    for file_item in target_folder.get_items(limit=1000):\n",
        "        if file_item.type == 'file' and file_item.name.endswith('.csv'):\n",
        "            file_count += 1\n",
        "\n",
        "            try:\n",
        "                content = file_item.content()\n",
        "                df = pd.read_csv(io.StringIO(content.decode('utf-8')))\n",
        "\n",
        "                if 'VehTypeName' in df.columns:\n",
        "                    # Count vehicle types in this file\n",
        "                    veh_counts = df['VehTypeName'].value_counts()\n",
        "\n",
        "                    for vtype, count in veh_counts.items():\n",
        "                        vtype_clean = str(vtype).strip()\n",
        "                        all_vehicle_types.add(vtype_clean)\n",
        "\n",
        "                        if vtype_clean not in total_vehicles:\n",
        "                            total_vehicles[vtype_clean] = 0\n",
        "                        total_vehicles[vtype_clean] += count\n",
        "\n",
        "                if not check_all_files and file_count >= 5:  # Sample first 5 files if not checking all\n",
        "                    break\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"‚ùå Error reading {file_item.name}: {e}\")\n",
        "\n",
        "    print(f\"\\nüìä Analyzed {file_count} files\")\n",
        "    print(f\"\\nüöó All Vehicle Types Found:\")\n",
        "    for vtype in sorted(all_vehicle_types):\n",
        "        print(f\"   '{vtype}'\")\n",
        "\n",
        "    # Calculate percentages\n",
        "    grand_total = sum(total_vehicles.values())\n",
        "    print(f\"\\nüìà Vehicle Distribution (Total: {grand_total} vehicles):\")\n",
        "    for vtype, count in sorted(total_vehicles.items(), key=lambda x: x[1], reverse=True):\n",
        "        percentage = (count / grand_total) * 100\n",
        "        print(f\"   {vtype}: {count} ({percentage:.1f}%)\")\n",
        "\n",
        "    # Map to behavior categories\n",
        "    behavior_counts = {\n",
        "        'autonomous': 0,\n",
        "        'aggressive': 0,\n",
        "        'cooperative': 0,\n",
        "        'normal': 0\n",
        "    }\n",
        "\n",
        "    for vtype, count in total_vehicles.items():\n",
        "        if 'CAV' in vtype:\n",
        "            behavior_counts['autonomous'] += count\n",
        "        elif 'Aggressive' in vtype:\n",
        "            behavior_counts['aggressive'] += count\n",
        "        elif 'Cooperative' in vtype:\n",
        "            behavior_counts['cooperative'] += count\n",
        "        elif 'Conventional' in vtype:\n",
        "            behavior_counts['normal'] += count\n",
        "\n",
        "\n",
        "    print(f\"\\nüè∑Ô∏è Behavior Category Distribution:\")\n",
        "    for behavior, count in behavior_counts.items():\n",
        "        percentage = (count / grand_total) * 100 if grand_total > 0 else 0\n",
        "        print(f\"   {behavior}: {count} ({percentage:.1f}%)\")\n",
        "\n",
        "\n",
        "    return all_vehicle_types, total_vehicles\n",
        "\n",
        "# Run the analysis\n",
        "all_types, counts = debug_scenario_composition('scen3_5rep3', check_all_files=True)"
      ],
      "metadata": {
        "id": "qLjkJ6D8_6iY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b404ec4b-8b1d-44ca-ba3e-614960f5f352"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîç COMPREHENSIVE SCENARIO ANALYSIS: scen3_5rep3\n",
            "================================================================================\n",
            "\n",
            "üìä Analyzed 61 files\n",
            "\n",
            "üöó All Vehicle Types Found:\n",
            "   'CAV'\n",
            "   'HDV Aggressive'\n",
            "   'HDV Conventional Gipps Model'\n",
            "   'HDV Cooperative'\n",
            "\n",
            "üìà Vehicle Distribution (Total: 35133 vehicles):\n",
            "   HDV Conventional Gipps Model: 14028 (39.9%)\n",
            "   HDV Cooperative: 8891 (25.3%)\n",
            "   HDV Aggressive: 8671 (24.7%)\n",
            "   CAV: 3543 (10.1%)\n",
            "\n",
            "üè∑Ô∏è Behavior Category Distribution:\n",
            "   autonomous: 3543 (10.1%)\n",
            "   aggressive: 8671 (24.7%)\n",
            "   cooperative: 8891 (25.3%)\n",
            "   normal: 14028 (39.9%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# RATE-LIMITED ULTRA TRAINER - RESPECTS BOX API LIMITS\n",
        "# Fixed version that won't hit rate limits\n",
        "\n",
        "import os\n",
        "import time\n",
        "import gc\n",
        "import pickle\n",
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import io\n",
        "import threading\n",
        "import random\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "from typing import List, Tuple, Optional\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# ====== GPU MEMORY MANAGEMENT ======\n",
        "print(\"üîß Setting up GPU memory management...\")\n",
        "import tensorflow as tf\n",
        "try:\n",
        "    gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "    if gpus:\n",
        "        tf.config.experimental.set_memory_growth(gpus[0], True)\n",
        "        print(f\"‚úÖ GPU memory growth enabled: {gpus[0]}\")\n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è No GPU detected - using CPU\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ö†Ô∏è GPU setup warning: {e}\")\n",
        "\n",
        "# ====== RATE LIMITER CLASS ======\n",
        "class BoxAPIRateLimiter:\n",
        "    \"\"\"\n",
        "    Handles Box API rate limiting and retry logic\n",
        "    \"\"\"\n",
        "    def __init__(self, max_requests_per_second=8, max_retries=5):\n",
        "        self.max_requests_per_second = max_requests_per_second\n",
        "        self.max_retries = max_retries\n",
        "        self.last_request_time = 0\n",
        "        self.request_count = 0\n",
        "        self.window_start = time.time()\n",
        "        self.lock = threading.Lock()\n",
        "\n",
        "    def wait_if_needed(self):\n",
        "        \"\"\"Wait if we're approaching rate limits\"\"\"\n",
        "        with self.lock:\n",
        "            current_time = time.time()\n",
        "\n",
        "            # Reset counter every second\n",
        "            if current_time - self.window_start >= 1.0:\n",
        "                self.request_count = 0\n",
        "                self.window_start = current_time\n",
        "\n",
        "            # If we're at the limit, wait\n",
        "            if self.request_count >= self.max_requests_per_second:\n",
        "                sleep_time = 1.0 - (current_time - self.window_start)\n",
        "                if sleep_time > 0:\n",
        "                    time.sleep(sleep_time)\n",
        "                self.request_count = 0\n",
        "                self.window_start = time.time()\n",
        "\n",
        "            self.request_count += 1\n",
        "\n",
        "    def execute_with_retry(self, func, *args, **kwargs):\n",
        "        \"\"\"Execute function with automatic retry on rate limit\"\"\"\n",
        "        for attempt in range(self.max_retries):\n",
        "            try:\n",
        "                self.wait_if_needed()\n",
        "                return func(*args, **kwargs)\n",
        "\n",
        "            except Exception as e:\n",
        "                error_str = str(e).lower()\n",
        "\n",
        "                # Check if it's a rate limit error\n",
        "                if '429' in error_str or 'rate limit' in error_str or 'too many requests' in error_str:\n",
        "                    wait_time = 2 ** attempt + random.uniform(0, 1)  # Exponential backoff\n",
        "                    print(f\"‚ö†Ô∏è Rate limited (attempt {attempt + 1}), waiting {wait_time:.1f}s...\")\n",
        "                    time.sleep(wait_time)\n",
        "                    continue\n",
        "                else:\n",
        "                    # Not a rate limit error, re-raise\n",
        "                    raise e\n",
        "\n",
        "        # All retries exhausted\n",
        "        raise Exception(f\"Max retries ({self.max_retries}) exceeded for Box API call\")\n",
        "\n",
        "# Global rate limiter\n",
        "rate_limiter = BoxAPIRateLimiter(max_requests_per_second=6)  # Conservative limit\n",
        "\n",
        "# ====== RUNTIME KEEP-ALIVE SYSTEM ======\n",
        "class RuntimeKeepAlive:\n",
        "    def __init__(self):\n",
        "        self.running = False\n",
        "        self.thread = None\n",
        "\n",
        "    def start(self):\n",
        "        if not self.running:\n",
        "            self.running = True\n",
        "            self.thread = threading.Thread(target=self._keep_alive, daemon=True)\n",
        "            self.thread.start()\n",
        "            print(\"üîÑ Runtime keep-alive started\")\n",
        "\n",
        "    def stop(self):\n",
        "        self.running = False\n",
        "        print(\"üõë Runtime keep-alive stopped\")\n",
        "\n",
        "    def _keep_alive(self):\n",
        "        while self.running:\n",
        "            time.sleep(1800)  # 30 minutes\n",
        "            if self.running:\n",
        "                print(f\"üíì Runtime alive at {time.strftime('%H:%M:%S')}\")\n",
        "\n",
        "# Start keep-alive\n",
        "keep_alive = RuntimeKeepAlive()\n",
        "keep_alive.start()\n",
        "\n",
        "# ====== PROGRESS MONITOR ======\n",
        "class ProgressMonitor:\n",
        "    def __init__(self, checkpoint_dir):\n",
        "        self.checkpoint_dir = checkpoint_dir\n",
        "        self.progress_file = f\"{checkpoint_dir}/progress.json\"\n",
        "\n",
        "    def update(self, **kwargs):\n",
        "        try:\n",
        "            progress = {\n",
        "                'timestamp': time.time(),\n",
        "                'time_str': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
        "                **kwargs\n",
        "            }\n",
        "\n",
        "            with open(self.progress_file, 'w') as f:\n",
        "                json.dump(progress, f, indent=2)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è Progress update failed: {e}\")\n",
        "\n",
        "    def show(self):\n",
        "        try:\n",
        "            if os.path.exists(self.progress_file):\n",
        "                with open(self.progress_file, 'r') as f:\n",
        "                    progress = json.load(f)\n",
        "\n",
        "                print(f\"üìä CURRENT PROGRESS:\")\n",
        "                print(f\"   Time: {progress.get('time_str', 'Unknown')}\")\n",
        "                print(f\"   Files processed: {progress.get('files_processed', 0):,}\")\n",
        "                print(f\"   Samples collected: {progress.get('samples_collected', 0):,}\")\n",
        "                print(f\"   Current batch: {progress.get('batch_num', 0)}\")\n",
        "                print(f\"   Phase: {progress.get('phase', 'Unknown')}\")\n",
        "                return progress\n",
        "            else:\n",
        "                print(\"üìä No progress data found\")\n",
        "                return {}\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error reading progress: {e}\")\n",
        "            return {}\n",
        "\n",
        "# ====== RATE-LIMITED TRAINER ======\n",
        "class RateLimitedTrainer:\n",
        "    \"\"\"\n",
        "    Trainer that respects Box API rate limits\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, client, main_folder_id, checkpoint_dir='/content/drive/MyDrive/Training_Checkpoints'):\n",
        "        self.client = client\n",
        "        self.main_folder_id = main_folder_id\n",
        "        self.checkpoint_dir = checkpoint_dir\n",
        "\n",
        "        # RATE-LIMITED SETTINGS (Respects API limits)\n",
        "        self.batch_size = 100           # Smaller batches to reduce API calls\n",
        "        self.max_workers = 4            # Fewer workers to avoid overwhelming API\n",
        "        self.checkpoint_interval = 500   # Checkpoint more frequently\n",
        "        self.memory_cleanup_interval = 300\n",
        "        self.max_rows_per_file = 500    # Balance between speed and data quality\n",
        "        self.max_sequence_length = 50   # Good balance\n",
        "\n",
        "        # Rate limiting\n",
        "        self.rate_limiter = rate_limiter\n",
        "\n",
        "        # Create directories\n",
        "        os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "\n",
        "        # File paths\n",
        "        self.processed_files_log = f\"{checkpoint_dir}/processed_files.json\"\n",
        "        self.features_checkpoint = f\"{checkpoint_dir}/features_checkpoint.pkl\"\n",
        "        self.progress_monitor = ProgressMonitor(checkpoint_dir)\n",
        "\n",
        "        print(f\"üöÄ Rate-Limited Trainer initialized\")\n",
        "        print(f\"   Batch size: {self.batch_size} (API-friendly)\")\n",
        "        print(f\"   Workers: {self.max_workers} (rate-limited)\")\n",
        "        print(f\"   API rate limit: 6 requests/second\")\n",
        "        print(f\"   Retry logic: Enabled\")\n",
        "\n",
        "    def save_checkpoint(self, processed_files, all_features, all_labels, batch_num):\n",
        "        \"\"\"Save training progress\"\"\"\n",
        "        try:\n",
        "            print(f\"üíæ Saving checkpoint at batch {batch_num}...\")\n",
        "\n",
        "            with open(self.processed_files_log, 'w') as f:\n",
        "                json.dump(processed_files, f)\n",
        "\n",
        "            checkpoint_data = {\n",
        "                'features': all_features,\n",
        "                'labels': all_labels,\n",
        "                'batch_num': batch_num,\n",
        "                'timestamp': time.time()\n",
        "            }\n",
        "\n",
        "            with open(self.features_checkpoint, 'wb') as f:\n",
        "                pickle.dump(checkpoint_data, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "            self.progress_monitor.update(\n",
        "                batch_num=batch_num,\n",
        "                files_processed=len(processed_files),\n",
        "                samples_collected=len(all_features),\n",
        "                phase='data_processing'\n",
        "            )\n",
        "\n",
        "            print(f\"‚úÖ Checkpoint saved: {len(processed_files):,} files, {len(all_features):,} samples\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Checkpoint save failed: {e}\")\n",
        "\n",
        "    def load_checkpoint(self):\n",
        "        \"\"\"Load previous training progress\"\"\"\n",
        "        print(\"üîÑ Checking for previous checkpoints...\")\n",
        "\n",
        "        if not os.path.exists(self.processed_files_log):\n",
        "            print(\"No previous checkpoint found. Starting fresh.\")\n",
        "            return [], [], [], 0\n",
        "\n",
        "        try:\n",
        "            with open(self.processed_files_log, 'r') as f:\n",
        "                processed_files = json.load(f)\n",
        "\n",
        "            with open(self.features_checkpoint, 'rb') as f:\n",
        "                checkpoint_data = pickle.load(f)\n",
        "\n",
        "            print(f\"üìö Checkpoint loaded!\")\n",
        "            print(f\"   Files processed: {len(processed_files):,}\")\n",
        "            print(f\"   Samples: {len(checkpoint_data['features']):,}\")\n",
        "            print(f\"   Last batch: {checkpoint_data['batch_num']}\")\n",
        "\n",
        "            return (processed_files, checkpoint_data['features'],\n",
        "                   checkpoint_data['labels'], checkpoint_data['batch_num'])\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error loading checkpoint: {e}\")\n",
        "            return [], [], [], 0\n",
        "\n",
        "    def get_csv_files_rate_limited(self, max_files=80000):\n",
        "        \"\"\"Rate-limited CSV discovery with rep1/rep2/rep3 filtering\"\"\"\n",
        "        print(f\"üì° RATE-LIMITED CSV DISCOVERY (max {max_files:,} files)\")\n",
        "        print(\"üéØ FILTERING: Only rep1, rep2, rep3 scenarios\")\n",
        "        print(\"=\" * 60)\n",
        "\n",
        "        self.progress_monitor.update(phase='file_discovery', files_target=max_files)\n",
        "\n",
        "        # Check cache\n",
        "        \"\"\"\n",
        "        cache_file = f\"{self.checkpoint_dir}/csv_files_cache_filtered.json\"\n",
        "\n",
        "        if os.path.exists(cache_file):\n",
        "            print(\"üìÅ Loading cached filtered file list...\")\n",
        "            try:\n",
        "                with open(cache_file, 'r') as f:\n",
        "                    cached_files = json.load(f)\n",
        "\n",
        "                if len(cached_files) >= max_files:\n",
        "                    print(f\"‚úÖ Using {len(cached_files[:max_files]):,} files from cache (rep1/rep2/rep3 only)\")\n",
        "                    return cached_files[:max_files]\n",
        "            except:\n",
        "                print(\"Cache corrupted, rescanning...\")\n",
        "\n",
        "\n",
        "        \"\"\"\n",
        "        start_time = time.time()\n",
        "        # Navigate with rate limiting\n",
        "        def get_folder_items_safe(folder, limit=1000):\n",
        "            \"\"\"Get folder items with rate limiting\"\"\"\n",
        "            return self.rate_limiter.execute_with_retry(\n",
        "                lambda: list(folder.get_items(limit=limit))\n",
        "            )\n",
        "\n",
        "        # Get main folder\n",
        "        main_folder = self.client.folder(self.main_folder_id)\n",
        "\n",
        "        # Find Parsed Time Series Data\n",
        "        print(\"üîç Finding Parsed Time Series Data folder...\")\n",
        "        parsed_folder = None\n",
        "\n",
        "        main_items = get_folder_items_safe(main_folder, limit=100)\n",
        "        for item in main_items:\n",
        "            if item.name == 'Parsed Time Series Data':\n",
        "                parsed_folder = self.client.folder(item.id)\n",
        "                break\n",
        "\n",
        "        if not parsed_folder:\n",
        "            print(\"‚ùå Parsed Time Series Data folder not found\")\n",
        "            return []\n",
        "\n",
        "        # Get scenario folders with rate limiting and filtering\n",
        "        print(\"üìÅ Getting scenario folders (filtering for rep1/rep2/rep3)...\")\n",
        "        scenario_folders = []\n",
        "        filtered_out_folders = []\n",
        "\n",
        "        parsed_items = get_folder_items_safe(parsed_folder, limit=500)\n",
        "        for item in parsed_items:\n",
        "            if item.type == 'folder':\n",
        "                folder_name = item.name\n",
        "\n",
        "                # FILTER: Only include rep1, rep2, rep3\n",
        "                if (folder_name.endswith('rep1') or\n",
        "                    folder_name.endswith('rep2') or\n",
        "                    folder_name.endswith('rep3')):\n",
        "                    scenario_folders.append((folder_name, item.id))\n",
        "                    print(f\"   ‚úÖ Including: {folder_name}\")\n",
        "                else:\n",
        "                    filtered_out_folders.append(folder_name)\n",
        "                    print(f\"   ‚ùå Filtering out: {folder_name}\")\n",
        "\n",
        "        print(f\"\\nüìä Filtering results:\")\n",
        "        print(f\"   ‚úÖ Included scenarios: {len(scenario_folders)}\")\n",
        "        print(f\"   ‚ùå Filtered out scenarios: {len(filtered_out_folders)}\")\n",
        "\n",
        "        if filtered_out_folders:\n",
        "            print(f\"   üìã Filtered out scenarios:\")\n",
        "            for folder in filtered_out_folders[:10]:  # Show first 10\n",
        "                print(f\"      - {folder}\")\n",
        "            if len(filtered_out_folders) > 10:\n",
        "                print(f\"      ... and {len(filtered_out_folders) - 10} more\")\n",
        "\n",
        "        if not scenario_folders:\n",
        "            print(\"‚ùå No rep1/rep2/rep3 scenarios found!\")\n",
        "            return []\n",
        "\n",
        "        # Rate-limited folder scanning\n",
        "        all_csv_files = []\n",
        "\n",
        "        def scan_folder_safe(folder_info):\n",
        "            \"\"\"Scan folder with rate limiting\"\"\"\n",
        "            folder_name, folder_id = folder_info\n",
        "            csv_files = []\n",
        "\n",
        "            try:\n",
        "                folder = self.client.folder(folder_id)\n",
        "\n",
        "                # Rate-limited file listing\n",
        "                items = self.rate_limiter.execute_with_retry(\n",
        "                    lambda: list(folder.get_items(limit=2000))\n",
        "                )\n",
        "\n",
        "                for item in items:\n",
        "                    if item.type == 'file' and item.name.endswith('.csv'):\n",
        "                        csv_files.append({\n",
        "                            'name': item.name,\n",
        "                            'id': item.id,\n",
        "                            'scenario': folder_name\n",
        "                        })\n",
        "\n",
        "                print(f\"   üìä {folder_name}: {len(csv_files)} CSV files\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"‚ö†Ô∏è Folder {folder_name} failed: {e}\")\n",
        "\n",
        "            return csv_files\n",
        "\n",
        "        # Sequential processing to respect rate limits\n",
        "        print(f\"\\nüìÑ Scanning {len(scenario_folders)} filtered folders for CSV files...\")\n",
        "        for folder_info in scenario_folders:\n",
        "            csv_files = scan_folder_safe(folder_info)\n",
        "            all_csv_files.extend(csv_files)\n",
        "\n",
        "            if len(all_csv_files) % 1000 == 0:\n",
        "                elapsed = time.time() - start_time\n",
        "                print(f\"   Found {len(all_csv_files):,} CSV files ({elapsed:.1f}s)\")\n",
        "\n",
        "            if len(all_csv_files) >= max_files:\n",
        "                break\n",
        "\n",
        "            # Small delay between folders to be extra safe\n",
        "            time.sleep(0.1)\n",
        "\n",
        "        # Analyze scenario distribution\n",
        "        scenario_counts = {}\n",
        "        for file_info in all_csv_files:\n",
        "            scenario = file_info['scenario']\n",
        "            scenario_counts[scenario] = scenario_counts.get(scenario, 0) + 1\n",
        "\n",
        "        print(f\"\\nüìä CSV files by scenario (rep1/rep2/rep3 only):\")\n",
        "        for scenario, count in sorted(scenario_counts.items()):\n",
        "            print(f\"   {scenario}: {count:,} files\")\n",
        "\n",
        "        # Cache results\n",
        "        try:\n",
        "            with open(cache_file, 'w') as f:\n",
        "                json.dump(all_csv_files[:max_files], f)\n",
        "            print(f\"üíæ Cached {len(all_csv_files):,} filtered files\")\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "        elapsed = time.time() - start_time\n",
        "        print(f\"\\n‚úÖ Filtered discovery complete:\")\n",
        "        print(f\"   Total CSV files: {len(all_csv_files):,}\")\n",
        "        print(f\"   Only rep1/rep2/rep3: ‚úÖ\")\n",
        "        print(f\"   Discovery time: {elapsed:.1f}s\")\n",
        "\n",
        "        return all_csv_files[:max_files]\n",
        "\n",
        "    def process_single_file_safe(self, file_info):\n",
        "        \"\"\"Process single file with rate limiting\"\"\"\n",
        "        try:\n",
        "            file_id = file_info['id']\n",
        "            filename = file_info['name']\n",
        "\n",
        "            # Rate-limited file download\n",
        "            def download_file():\n",
        "                file_obj = self.client.file(file_id)\n",
        "                return file_obj.content()\n",
        "\n",
        "            content = self.rate_limiter.execute_with_retry(download_file)\n",
        "\n",
        "            # Fast CSV parsing\n",
        "            df = pd.read_csv(\n",
        "                io.StringIO(content.decode('utf-8')),\n",
        "                usecols=[ 'VehTypeName', 'Speeds', 'VehFrontCoords'],\n",
        "                nrows=self.max_rows_per_file,\n",
        "                dtype={'VehTypeName': 'str'},\n",
        "                low_memory=False,\n",
        "                engine='c'\n",
        "            )\n",
        "\n",
        "\n",
        "            # DEBUG: Check what vehicle types we have\n",
        "            if self.debug_first_file and filename == self.debug_first_file:\n",
        "                print(f\"\\nüîç DEBUG: Analyzing {filename}\")\n",
        "                print(f\"Total rows in file: {len(df)}\")\n",
        "                print(f\"Unique VehTypeName values:\")\n",
        "                for vtype in df['VehTypeName'].unique():\n",
        "                    print(f\"  - '{vtype}'\")\n",
        "                self.debug_first_file = None  # Only debug once\n",
        "\n",
        "            if len(df) == 0:\n",
        "                return [], [], filename\n",
        "\n",
        "            # Fast label mapping\n",
        "            def fast_map(veh_type):\n",
        "                s = str(veh_type)\n",
        "                if 'CAV' in s: return 'autonomous'\n",
        "                elif 'Aggressive' in s: return 'aggressive'\n",
        "                elif 'Cooperative' in s: return 'cooperative'\n",
        "                elif 'Conventional' in s: return 'normal'\n",
        "\n",
        "            df['behavior_label'] = df['VehTypeName'].apply(fast_map)\n",
        "\n",
        "\n",
        "            # DEBUG: Check label distribution after mapping\n",
        "            if not hasattr(self, 'file_count'):\n",
        "                self.file_count = 0\n",
        "            self.file_count += 1\n",
        "\n",
        "            if self.file_count <= 5:  # Debug first 5 files\n",
        "                print(f\"\\nüìä File {self.file_count}: {filename}\")\n",
        "                label_counts = df['behavior_label'].value_counts()\n",
        "                print(f\"Labels after mapping:\")\n",
        "                for label, count in label_counts.items():\n",
        "                    print(f\"  {label}: {count}\")\n",
        "\n",
        "            # Filter and process\n",
        "            valid_df = df[\n",
        "                (df['behavior_label'] != 'autonomous') &\n",
        "                (df['Speeds'].notna()) &\n",
        "                (df['VehFrontCoords'].notna())\n",
        "            ]\n",
        "\n",
        "\n",
        "            # DEBUG: Check what happened after filtering\n",
        "            if self.file_count <= 5:\n",
        "                if len(valid_df) > 0:\n",
        "                    valid_label_counts = valid_df['behavior_label'].value_counts()\n",
        "                    print(f\"Labels after filtering:\")\n",
        "                    for label, count in valid_label_counts.items():\n",
        "                        print(f\"  {label}: {count}\")\n",
        "                else:\n",
        "                    print(f\"  No valid rows after filtering!\")\n",
        "\n",
        "            # Check if we lost any normal vehicles\n",
        "            if 'normal' in label_counts and ('normal' not in valid_label_counts or valid_label_counts.get('normal', 0) == 0):\n",
        "                print(f\"  ‚ö†Ô∏è WARNING: Lost all 'normal' vehicles during filtering!\")\n",
        "\n",
        "            if len(valid_df) == 0:\n",
        "                return [], [], filename\n",
        "\n",
        "            # Feature extraction\n",
        "            features_list = []\n",
        "            labels_list = []\n",
        "\n",
        "            for _, row in valid_df.iterrows():\n",
        "                try:\n",
        "                    speeds_str = str(row['Speeds']).replace('inf', '0').replace('nan', '0').replace('-inf', '0')\n",
        "                    coords_str = str(row['VehFrontCoords']).replace('inf', '0').replace('nan', '0').replace('-inf', '0')\n",
        "\n",
        "                    speeds = eval(speeds_str)\n",
        "                    coords = eval(coords_str)\n",
        "\n",
        "                    if len(speeds) >= 5 and len(coords) >= 5:\n",
        "                        max_len = self.max_sequence_length\n",
        "                        speeds = speeds[:max_len]\n",
        "                        coords = coords[:max_len]\n",
        "\n",
        "                        min_len = min(len(speeds), len(coords))\n",
        "\n",
        "                        features = {\n",
        "                            'speeds': np.array(speeds[:min_len], dtype=np.float32).reshape(-1, 1),\n",
        "                            'positions': np.array(coords[:min_len], dtype=np.float32),\n",
        "                            'sequence_length': min_len\n",
        "                        }\n",
        "\n",
        "                        features_list.append(features)\n",
        "                        labels_list.append(row['behavior_label'])\n",
        "\n",
        "                except:\n",
        "                    continue\n",
        "\n",
        "            return features_list, labels_list, filename\n",
        "\n",
        "        except Exception as e:\n",
        "            return [], [], file_info.get('name', 'unknown')\n",
        "\n",
        "    def process_files_rate_limited(self, csv_files):\n",
        "        \"\"\"Process files with proper rate limiting\"\"\"\n",
        "        print(f\"üì° RATE-LIMITED PROCESSING {len(csv_files):,} files\")\n",
        "        print(f\"Workers: {self.max_workers} (API-safe)\")\n",
        "        print(\"=\" * 60)\n",
        "\n",
        "        # Load checkpoint\n",
        "        processed_files, all_features, all_labels, start_batch = self.load_checkpoint()\n",
        "        processed_files_set = set(processed_files)\n",
        "\n",
        "        # Filter remaining files\n",
        "        remaining_files = [f for f in csv_files if f['name'] not in processed_files_set]\n",
        "\n",
        "        if not remaining_files:\n",
        "            print(\"‚úÖ All files already processed!\")\n",
        "            return all_features, all_labels\n",
        "\n",
        "        print(f\"üìä Processing {len(remaining_files):,} remaining files\")\n",
        "\n",
        "        batch_size = self.batch_size\n",
        "        total_batches = (len(remaining_files) + batch_size - 1) // batch_size\n",
        "\n",
        "        start_time = time.time()\n",
        "        files_processed = len(processed_files)\n",
        "\n",
        "        for batch_idx in range(start_batch, total_batches):\n",
        "            batch_start = batch_idx * batch_size\n",
        "            batch_end = min(batch_start + batch_size, len(remaining_files))\n",
        "            batch_files = remaining_files[batch_start:batch_end]\n",
        "\n",
        "            print(f\"\\nüì¶ RATE-LIMITED BATCH {batch_idx + 1}/{total_batches}\")\n",
        "            print(f\"   Files {batch_start + 1:,}-{batch_end:,}\")\n",
        "\n",
        "            batch_start_time = time.time()\n",
        "\n",
        "            # Rate-limited parallel processing\n",
        "            with ThreadPoolExecutor(max_workers=self.max_workers) as executor:\n",
        "                future_to_file = {\n",
        "                    executor.submit(self.process_single_file_safe, file_info): file_info\n",
        "                    for file_info in batch_files\n",
        "                }\n",
        "\n",
        "                batch_features = []\n",
        "                batch_labels = []\n",
        "                batch_processed = []\n",
        "\n",
        "                for future in as_completed(future_to_file):\n",
        "                    try:\n",
        "                        features_list, labels_list, filename = future.result(timeout=60)\n",
        "\n",
        "                        if features_list:\n",
        "                            batch_features.extend(features_list)\n",
        "                            batch_labels.extend(labels_list)\n",
        "\n",
        "                        batch_processed.append(filename)\n",
        "                        files_processed += 1\n",
        "\n",
        "                    except Exception as e:\n",
        "                        print(f\"‚ö†Ô∏è File processing failed: {e}\")\n",
        "                        continue\n",
        "\n",
        "            # Add batch results\n",
        "            all_features.extend(batch_features)\n",
        "            all_labels.extend(batch_labels)\n",
        "            processed_files.extend(batch_processed)\n",
        "\n",
        "            batch_time = time.time() - batch_start_time\n",
        "            total_time = time.time() - start_time\n",
        "\n",
        "            # Progress stats\n",
        "            rate = files_processed / total_time if total_time > 0 else 0\n",
        "            eta = (len(remaining_files) - files_processed) / rate if rate > 0 else 0\n",
        "\n",
        "            print(f\"   ‚úÖ Batch complete: {len(batch_features):,} samples ({batch_time:.1f}s)\")\n",
        "            print(f\"   üìä Progress: {files_processed:,}/{len(remaining_files):,} files\")\n",
        "            print(f\"   üì° Rate: {rate:.1f} files/sec (API-safe)\")\n",
        "            print(f\"   ‚è±Ô∏è ETA: {eta/60:.1f} min\")\n",
        "            print(f\"   üíæ Total samples: {len(all_features):,}\")\n",
        "\n",
        "            # Update progress\n",
        "            self.progress_monitor.update(\n",
        "                batch_num=batch_idx + 1,\n",
        "                files_processed=files_processed,\n",
        "                samples_collected=len(all_features),\n",
        "                processing_rate=rate,\n",
        "                eta_minutes=eta/60\n",
        "            )\n",
        "\n",
        "            # Checkpoint more frequently\n",
        "            if (batch_idx + 1) % (self.checkpoint_interval // batch_size) == 0:\n",
        "                self.save_checkpoint(processed_files, all_features, all_labels, batch_idx + 1)\n",
        "\n",
        "            # Memory cleanup\n",
        "            if (batch_idx + 1) % (self.memory_cleanup_interval // batch_size) == 0:\n",
        "                print(\"üßπ Memory cleanup...\")\n",
        "                del batch_features, batch_labels\n",
        "                gc.collect()\n",
        "\n",
        "        # Final checkpoint\n",
        "        self.save_checkpoint(processed_files, all_features, all_labels, total_batches)\n",
        "\n",
        "        total_time = time.time() - start_time\n",
        "        print(f\"\\n‚úÖ RATE-LIMITED PROCESSING COMPLETE\")\n",
        "        print(f\"   Files processed: {files_processed:,}\")\n",
        "        print(f\"   Total samples: {len(all_features):,}\")\n",
        "        print(f\"   Total time: {total_time:.1f}s ({total_time/60:.1f} min)\")\n",
        "        print(f\"   Safe rate: {files_processed/total_time:.1f} files/sec\")\n",
        "\n",
        "        return all_features, all_labels\n",
        "\n",
        "    def train_with_rate_limits(self, max_files=80000):\n",
        "        \"\"\"Train with proper rate limiting\"\"\"\n",
        "        print(\"üì° RATE-LIMITED 80K FILE TRAINING\")\n",
        "        print(\"=\" * 70)\n",
        "\n",
        "        # Initialize debug flags\n",
        "        self.debug_first_file = True  # Will be set to first filename\n",
        "        self.file_count = 0\n",
        "        self.unexpected_types = set()\n",
        "\n",
        "\n",
        "\n",
        "        training_start = time.time()\n",
        "\n",
        "        self.progress_monitor.update(phase='starting', max_files=max_files)\n",
        "\n",
        "        # Step 1: Rate-limited file discovery\n",
        "        print(\"üîç Step 1: Rate-limited file discovery...\")\n",
        "        csv_files = self.get_csv_files_rate_limited(max_files=max_files)\n",
        "\n",
        "        if not csv_files:\n",
        "            print(\"‚ùå No CSV files found\")\n",
        "            return None\n",
        "\n",
        "        print(f\"üéØ Will process {len(csv_files):,} files with rate limiting\")\n",
        "\n",
        "        # Step 2: Rate-limited processing\n",
        "        print(\"\\nüì° Step 2: Rate-limited processing...\")\n",
        "        all_features, all_labels = self.process_files_rate_limited(csv_files)\n",
        "\n",
        "        if not all_features:\n",
        "            print(\"‚ùå No features extracted\")\n",
        "            return None\n",
        "\n",
        "\n",
        "        # Step 3: Data analysis\n",
        "        print(f\"\\nüìä Step 3: Analyzing dataset...\")\n",
        "        from collections import Counter\n",
        "        label_distribution = Counter(all_labels)\n",
        "\n",
        "        print(f\"RATE-LIMITED DATASET SUMMARY:\")\n",
        "        print(f\"   Files processed: {len(csv_files):,}\")\n",
        "        print(f\"   Total vehicles: {len(all_features):,}\")\n",
        "        print(f\"   Label distribution:\")\n",
        "        for label, count in label_distribution.items():\n",
        "            percentage = (count / len(all_labels)) * 100\n",
        "            print(f\"     {label}: {count:,} ({percentage:.1f}%)\")\n",
        "\n",
        "        self.progress_monitor.update(\n",
        "            phase='data_preparation',\n",
        "            total_samples=len(all_features),\n",
        "            label_distribution=dict(label_distribution)\n",
        "        )\n",
        "\n",
        "        # Step 4: Data preparation\n",
        "        print(f\"\\nüîß Step 4: Data preparation...\")\n",
        "\n",
        "        def prepare_data_safe(features_list, max_sequence=50):\n",
        "            total_samples = len(features_list)\n",
        "\n",
        "            X_speed = np.zeros((total_samples, max_sequence, 1), dtype=np.float32)\n",
        "            X_pos = np.zeros((total_samples, max_sequence, 2), dtype=np.float32)\n",
        "\n",
        "            for i, features in enumerate(features_list):\n",
        "                try:\n",
        "                    speeds = features['speeds'][:max_sequence]\n",
        "                    positions = features['positions'][:max_sequence]\n",
        "                    seq_len = min(len(speeds), len(positions), max_sequence)\n",
        "\n",
        "                    X_speed[i, :seq_len, 0] = speeds[:seq_len, 0]\n",
        "                    X_pos[i, :seq_len, :] = positions[:seq_len, :]\n",
        "\n",
        "                except:\n",
        "                    continue\n",
        "\n",
        "                if i % 10000 == 0:\n",
        "                    print(f\"   Processed {i:,}/{total_samples:,} samples\")\n",
        "\n",
        "            return X_speed, X_pos\n",
        "\n",
        "        X_speed, X_pos = prepare_data_safe(all_features, max_sequence=self.max_sequence_length)\n",
        "\n",
        "        # Clean up\n",
        "        del all_features\n",
        "        gc.collect()\n",
        "\n",
        "        print(f\"‚úÖ Data prepared: {X_speed.shape[0]:,} samples\")\n",
        "\n",
        "        # Continue with training (same as before)\n",
        "        split_idx = int(0.8 * len(all_labels))\n",
        "\n",
        "        X_speed_train = X_speed[:split_idx]\n",
        "        X_speed_test = X_speed[split_idx:]\n",
        "        X_pos_train = X_pos[:split_idx]\n",
        "        X_pos_test = X_pos[split_idx:]\n",
        "\n",
        "        train_labels = all_labels[:split_idx]\n",
        "        test_labels = all_labels[split_idx:]\n",
        "\n",
        "        print(f\"Train samples: {len(train_labels):,}\")\n",
        "        print(f\"Test samples: {len(test_labels):,}\")\n",
        "\n",
        "        # Label encoding\n",
        "        from sklearn.preprocessing import LabelEncoder\n",
        "        label_encoder = LabelEncoder()\n",
        "        label_encoder.fit(all_labels)\n",
        "\n",
        "        y_train = label_encoder.transform(train_labels)\n",
        "        y_test = label_encoder.transform(test_labels)\n",
        "\n",
        "\n",
        "        from collections import Counter\n",
        "        print(f\"Label distribution in all_labels: {Counter(all_labels)}\")\n",
        "        print(f\"Unique labels: {set(all_labels)}\")\n",
        "        print(f\"Total samples: {len(all_labels)}\")\n",
        "\n",
        "        print(f\"Classes: {label_encoder.classes_}\")\n",
        "\n",
        "        self.progress_monitor.update(\n",
        "            phase='model_training',\n",
        "            train_samples=len(y_train),\n",
        "            test_samples=len(y_test),\n",
        "            classes=list(label_encoder.classes_)\n",
        "        )\n",
        "\n",
        "        # Model training\n",
        "        print(f\"\\nüöÄ Step 5: Model training...\")\n",
        "\n",
        "        num_classes = len(label_encoder.classes_)\n",
        "        model = build_speed_position_model_fixed(\n",
        "            sequence_length=self.max_sequence_length,\n",
        "            num_classes=num_classes\n",
        "        )\n",
        "\n",
        "        print(f\"Training with rate-limited dataset...\")\n",
        "        print(f\"Training samples: {len(y_train):,}\")\n",
        "\n",
        "        try:\n",
        "            callbacks = [\n",
        "                tf.keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True),\n",
        "                tf.keras.callbacks.ReduceLROnPlateau(factor=0.5, patience=3),\n",
        "            ]\n",
        "\n",
        "            history = model.fit(\n",
        "                [X_speed_train, X_pos_train], y_train,\n",
        "                epochs=20,\n",
        "                batch_size=256,\n",
        "                validation_split=0.15,\n",
        "                callbacks=callbacks,\n",
        "                verbose=1\n",
        "            )\n",
        "\n",
        "            print(\"‚úÖ Training completed!\")\n",
        "\n",
        "            # Evaluation\n",
        "            print(f\"\\nüìä Evaluation...\")\n",
        "\n",
        "            y_pred_probs = model.predict([X_speed_test, X_pos_test], batch_size=512, verbose=0)\n",
        "            y_pred = np.argmax(y_pred_probs, axis=1)\n",
        "\n",
        "            accuracy = np.mean(y_test == y_pred)\n",
        "\n",
        "            from sklearn.metrics import classification_report, f1_score\n",
        "            f1_macro = f1_score(y_test, y_pred, average='macro')\n",
        "\n",
        "            print(f\"RATE-LIMITED RESULTS:\")\n",
        "            print(f\"   Test Accuracy: {accuracy:.3f}\")\n",
        "            print(f\"   F1 Score (macro): {f1_macro:.3f}\")\n",
        "\n",
        "            print(f\"\\nClassification Report:\")\n",
        "            print(classification_report(y_test, y_pred, target_names=label_encoder.classes_))\n",
        "\n",
        "            # Save model\n",
        "            print(f\"\\nüíæ Saving model...\")\n",
        "\n",
        "            model.save(f\"{self.checkpoint_dir}/rate_limited_80k_model.keras\")\n",
        "\n",
        "            cl_system.model = model\n",
        "            cl_system.label_encoder = label_encoder\n",
        "            cl_system.save_model()\n",
        "            cl_system.save_label_encoder()\n",
        "\n",
        "            total_time = time.time() - training_start\n",
        "\n",
        "            self.progress_monitor.update(\n",
        "                phase='completed',\n",
        "                total_time=total_time,\n",
        "                final_accuracy=accuracy,\n",
        "                f1_macro=f1_macro\n",
        "            )\n",
        "\n",
        "            print(f\"\\nüéâ RATE-LIMITED TRAINING COMPLETE!\")\n",
        "            print(f\"=\" * 60)\n",
        "            print(f\"Files processed: {len(csv_files):,}\")\n",
        "            print(f\"Total vehicles: {len(all_labels):,}\")\n",
        "            print(f\"Training time: {total_time:.1f}s ({total_time/3600:.1f} hours)\")\n",
        "            print(f\"Final accuracy: {accuracy:.3f}\")\n",
        "            print(f\"API-safe: No rate limit errors\")\n",
        "\n",
        "            return {\n",
        "                'model': model,\n",
        "                'label_encoder': label_encoder,\n",
        "                'history': history,\n",
        "                'accuracy': accuracy,\n",
        "                'f1_macro': f1_macro,\n",
        "                'files_processed': len(csv_files),\n",
        "                'total_vehicles': len(all_labels),\n",
        "                'training_time': total_time,\n",
        "                'label_distribution': label_distribution\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Training failed: {e}\")\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "            return None\n",
        "\n",
        "        # At the end, check how many normal vehicles made it through\n",
        "        normal_extracted = sum(1 for label in labels_list if label == 'normal')\n",
        "        print(f\"   Normal vehicles: {normal_in_valid} in valid_df -> {normal_extracted} extracted\")\n",
        "\n",
        "\n",
        "        # DEBUG: Print summary of what we found\n",
        "        print(f\"\\nüîç DEBUG SUMMARY:\")\n",
        "        print(f\"Total files processed: {files_processed}\")\n",
        "        print(f\"Total samples collected: {len(all_features)}\")\n",
        "\n",
        "        # Check label distribution\n",
        "        from collections import Counter\n",
        "        label_counts = Counter(all_labels)\n",
        "        print(f\"Final label distribution:\")\n",
        "        for label, count in label_counts.items():\n",
        "            print(f\"  {label}: {count}\")\n",
        "\n",
        "        if hasattr(self, 'unexpected_types') and self.unexpected_types:\n",
        "            print(f\"\\nUnexpected vehicle types found:\")\n",
        "            for vtype in sorted(self.unexpected_types):\n",
        "                print(f\"  - '{vtype}'\")\n",
        "\n",
        "    def debug_process_single_file_ultra_fast(self, file_info):\n",
        "      \"\"\"Debug version with label tracking\"\"\"\n",
        "      try:\n",
        "          file_id = file_info['id']\n",
        "          filename = file_info['name']\n",
        "\n",
        "          # Get file and download\n",
        "          file_obj = self.client.file(file_id)\n",
        "          content = file_obj.content()\n",
        "\n",
        "          # Ultra-fast CSV parsing\n",
        "          df = pd.read_csv(\n",
        "              io.StringIO(content.decode('utf-8')),\n",
        "              usecols=['VehNr', 'VehTypeName', 'Speeds', 'VehFrontCoords'],\n",
        "              nrows=self.max_rows_per_file,\n",
        "              dtype={'VehNr': 'str', 'VehTypeName': 'str'},\n",
        "              low_memory=False,\n",
        "              engine='c'\n",
        "          )\n",
        "\n",
        "          if len(df) == 0:\n",
        "              return [], [], filename, {}\n",
        "\n",
        "          # Debug: Check unique VehTypeName values\n",
        "          unique_types = df['VehTypeName'].unique()\n",
        "\n",
        "          # Ultra-fast label mapping with debug info\n",
        "          def ultra_fast_map(veh_type):\n",
        "              s = str(veh_type)\n",
        "              if 'CAV' in s: return 'autonomous'\n",
        "              elif 'Aggressive' in s: return 'aggressive'\n",
        "              elif 'Cooperative' in s: return 'cooperative'\n",
        "              elif 'Conventional' in s or 'Gipps' in s or 'Normal' in s: return 'normal'\n",
        "              else: return 'normal'\n",
        "\n",
        "          df['behavior_label'] = df['VehTypeName'].apply(ultra_fast_map)\n",
        "\n",
        "          # Debug: Count labels before filtering\n",
        "          label_counts_before = df['behavior_label'].value_counts().to_dict()\n",
        "\n",
        "          # Filter and process\n",
        "          valid_df = df[\n",
        "              (df['behavior_label'] != 'autonomous') &\n",
        "              (df['Speeds'].notna()) &\n",
        "              (df['VehFrontCoords'].notna())\n",
        "          ]\n",
        "\n",
        "          # Debug: Count labels after filtering\n",
        "          label_counts_after = valid_df['behavior_label'].value_counts().to_dict()\n",
        "\n",
        "          debug_info = {\n",
        "              'unique_types': list(unique_types),\n",
        "              'labels_before_filter': label_counts_before,\n",
        "              'labels_after_filter': label_counts_after\n",
        "          }\n",
        "\n",
        "          if len(valid_df) == 0:\n",
        "              return [], [], filename, debug_info\n",
        "\n",
        "          # Rest of processing...\n",
        "          features_list = []\n",
        "          labels_list = []\n",
        "\n",
        "          for _, row in valid_df.iterrows():\n",
        "              try:\n",
        "                  speeds_str = str(row['Speeds']).replace('inf', '0').replace('nan', '0').replace('-inf', '0')\n",
        "                  coords_str = str(row['VehFrontCoords']).replace('inf', '0').replace('nan', '0').replace('-inf', '0')\n",
        "\n",
        "                  speeds = eval(speeds_str)\n",
        "                  coords = eval(coords_str)\n",
        "\n",
        "                  if len(speeds) >= 3 and len(coords) >= 3:\n",
        "                      max_len = self.max_sequence_length\n",
        "                      speeds = speeds[:max_len]\n",
        "                      coords = coords[:max_len]\n",
        "\n",
        "                      min_len = min(len(speeds), len(coords))\n",
        "\n",
        "                      features = {\n",
        "                          'speeds': np.array(speeds[:min_len], dtype=np.float32).reshape(-1, 1),\n",
        "                          'positions': np.array(coords[:min_len], dtype=np.float32),\n",
        "                          'sequence_length': min_len\n",
        "                      }\n",
        "\n",
        "                      features_list.append(features)\n",
        "                      labels_list.append(row['behavior_label'])\n",
        "\n",
        "              except:\n",
        "                  continue\n",
        "\n",
        "          return features_list, labels_list, filename, debug_info\n",
        "\n",
        "      except Exception as e:\n",
        "          print(f\"Error processing file: {e}\")\n",
        "          return [], [], file_info.get('name', 'unknown'), {}\n",
        "# ====== MONITORING FUNCTIONS ======\n",
        "def show_progress():\n",
        "    \"\"\"Show current training progress\"\"\"\n",
        "    try:\n",
        "        monitor = ProgressMonitor('/content/drive/MyDrive/Training_Checkpoints')\n",
        "        return monitor.show()\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error showing progress: {e}\")\n",
        "\n",
        "def resume_training():\n",
        "    \"\"\"Resume training from checkpoint\"\"\"\n",
        "    print(\"üîÑ RESUMING RATE-LIMITED TRAINING FROM CHECKPOINT\")\n",
        "    trainer = RateLimitedTrainer(client, main_folder_id)\n",
        "    return trainer.train_with_rate_limits(max_files=80000)\n",
        "\n",
        "# ====== MAIN EXECUTION ======\n",
        "print(\"üì° RATE-LIMITED 80K TRAINER READY\")\n",
        "print(\"=\" * 70)\n",
        "print(\"üîß Rate limiting features:\")\n",
        "print(\"   ‚úÖ 6 requests/second limit\")\n",
        "print(\"   ‚úÖ Exponential backoff retry\")\n",
        "print(\"   ‚úÖ 4 workers (API-safe)\")\n",
        "print(\"   ‚úÖ 100 files per batch\")\n",
        "print(\"   ‚úÖ Connection pool management\")\n",
        "print(\"   ‚úÖ Auto-retry on 429 errors\")\n",
        "print()\n",
        "print(\"üìã Available commands:\")\n",
        "print(\"   show_progress() - Check current progress\")\n",
        "print(\"   resume_training() - Resume from checkpoint\")\n",
        "\n",
        "# Create and start rate-limited trainer\n",
        "print(\"\\nüì° STARTING RATE-LIMITED 80K FILE TRAINING\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "trainer = RateLimitedTrainer(client, main_folder_id)\n",
        "\n",
        "print(f\"Rate-Limited Configuration:\")\n",
        "print(f\"   Max files: 80,000\")\n",
        "print(f\"   Workers: 4 (API-safe)\")\n",
        "print(f\"   Rate limit: 6 requests/second\")\n",
        "print(f\"   Batch size: 100 files\")\n",
        "print(f\"   Auto-retry: Enabled\")\n",
        "print(f\"   Checkpoints: Every 500 files\")\n",
        "\n",
        "# Start rate-limited training\n",
        "result = trainer.train_with_rate_limits(max_files=80000)\n",
        "\n",
        "if result:\n",
        "    print(f\"\\nüéâ RATE-LIMITED SUCCESS!\")\n",
        "    print(f\"   Files processed: {result['files_processed']:,}\")\n",
        "    print(f\"   Total vehicles: {result['total_vehicles']:,}\")\n",
        "    print(f\"   Training time: {result['training_time']/3600:.1f} hours\")\n",
        "    print(f\"   Final accuracy: {result['accuracy']:.3f}\")\n",
        "    print(f\"   No API errors: ‚úÖ\")\n",
        "\n",
        "    # Stop keep-alive\n",
        "    keep_alive.stop()\n",
        "else:\n",
        "    print(f\"\\n‚ùå Training failed - run resume_training() to continue\")\n",
        "\n",
        "print(f\"\\nüíæ All checkpoints saved to: /content/drive/MyDrive/Training_Checkpoints\")\n",
        "print(f\"üì° API-safe trainer - no more 429 errors!\")"
      ],
      "metadata": {
        "id": "jIVGbR0q6eCa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "dde0c3ec-075a-4846-9a18-47e07c51d0bb"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîß Setting up GPU memory management...\n",
            "‚úÖ GPU memory growth enabled: PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')\n",
            "üîÑ Runtime keep-alive started\n",
            "üì° RATE-LIMITED 80K TRAINER READY\n",
            "======================================================================\n",
            "üîß Rate limiting features:\n",
            "   ‚úÖ 6 requests/second limit\n",
            "   ‚úÖ Exponential backoff retry\n",
            "   ‚úÖ 4 workers (API-safe)\n",
            "   ‚úÖ 100 files per batch\n",
            "   ‚úÖ Connection pool management\n",
            "   ‚úÖ Auto-retry on 429 errors\n",
            "\n",
            "üìã Available commands:\n",
            "   show_progress() - Check current progress\n",
            "   resume_training() - Resume from checkpoint\n",
            "\n",
            "üì° STARTING RATE-LIMITED 80K FILE TRAINING\n",
            "======================================================================\n",
            "üöÄ Rate-Limited Trainer initialized\n",
            "   Batch size: 100 (API-friendly)\n",
            "   Workers: 4 (rate-limited)\n",
            "   API rate limit: 6 requests/second\n",
            "   Retry logic: Enabled\n",
            "Rate-Limited Configuration:\n",
            "   Max files: 80,000\n",
            "   Workers: 4 (API-safe)\n",
            "   Rate limit: 6 requests/second\n",
            "   Batch size: 100 files\n",
            "   Auto-retry: Enabled\n",
            "   Checkpoints: Every 500 files\n",
            "üì° RATE-LIMITED 80K FILE TRAINING\n",
            "======================================================================\n",
            "üîç Step 1: Rate-limited file discovery...\n",
            "üì° RATE-LIMITED CSV DISCOVERY (max 80,000 files)\n",
            "üéØ FILTERING: Only rep1, rep2, rep3 scenarios\n",
            "============================================================\n",
            "üîç Finding Parsed Time Series Data folder...\n",
            "üìÅ Getting scenario folders (filtering for rep1/rep2/rep3)...\n",
            "   ‚úÖ Including: scen10_1rep1\n",
            "   ‚ùå Filtering out: scen10_1rep10\n",
            "   ‚úÖ Including: scen10_1rep2\n",
            "   ‚úÖ Including: scen10_1rep3\n",
            "   ‚ùå Filtering out: scen10_1rep4\n",
            "   ‚ùå Filtering out: scen10_1rep5\n",
            "   ‚ùå Filtering out: scen10_1rep6\n",
            "   ‚ùå Filtering out: scen10_1rep7\n",
            "   ‚ùå Filtering out: scen10_1rep8\n",
            "   ‚ùå Filtering out: scen10_1rep9\n",
            "   ‚úÖ Including: scen10_2rep1\n",
            "   ‚ùå Filtering out: scen10_2rep10\n",
            "   ‚úÖ Including: scen10_2rep2\n",
            "   ‚úÖ Including: scen10_2rep3\n",
            "   ‚ùå Filtering out: scen10_2rep4\n",
            "   ‚ùå Filtering out: scen10_2rep5\n",
            "   ‚ùå Filtering out: scen10_2rep6\n",
            "   ‚ùå Filtering out: scen10_2rep7\n",
            "   ‚ùå Filtering out: scen10_2rep8\n",
            "   ‚ùå Filtering out: scen10_2rep9\n",
            "   ‚úÖ Including: scen10_3rep1\n",
            "   ‚ùå Filtering out: scen10_3rep10\n",
            "   ‚úÖ Including: scen10_3rep2\n",
            "   ‚úÖ Including: scen10_3rep3\n",
            "   ‚ùå Filtering out: scen10_3rep4\n",
            "   ‚ùå Filtering out: scen10_3rep5\n",
            "   ‚ùå Filtering out: scen10_3rep6\n",
            "   ‚ùå Filtering out: scen10_3rep7\n",
            "   ‚ùå Filtering out: scen10_3rep8\n",
            "   ‚ùå Filtering out: scen10_3rep9\n",
            "   ‚úÖ Including: scen10_4rep1\n",
            "   ‚ùå Filtering out: scen10_4rep10\n",
            "   ‚úÖ Including: scen10_4rep2\n",
            "   ‚úÖ Including: scen10_4rep3\n",
            "   ‚ùå Filtering out: scen10_4rep4\n",
            "   ‚ùå Filtering out: scen10_4rep5\n",
            "   ‚ùå Filtering out: scen10_4rep6\n",
            "   ‚ùå Filtering out: scen10_4rep7\n",
            "   ‚ùå Filtering out: scen10_4rep8\n",
            "   ‚ùå Filtering out: scen10_4rep9\n",
            "   ‚úÖ Including: scen10_5rep1\n",
            "   ‚ùå Filtering out: scen10_5rep10\n",
            "   ‚úÖ Including: scen10_5rep2\n",
            "   ‚úÖ Including: scen10_5rep3\n",
            "   ‚ùå Filtering out: scen10_5rep4\n",
            "   ‚ùå Filtering out: scen10_5rep5\n",
            "   ‚ùå Filtering out: scen10_5rep6\n",
            "   ‚ùå Filtering out: scen10_5rep7\n",
            "   ‚ùå Filtering out: scen10_5rep8\n",
            "   ‚ùå Filtering out: scen10_5rep9\n",
            "   ‚úÖ Including: scen10_6rep1\n",
            "   ‚ùå Filtering out: scen10_6rep10\n",
            "   ‚úÖ Including: scen10_6rep2\n",
            "   ‚úÖ Including: scen10_6rep3\n",
            "   ‚ùå Filtering out: scen10_6rep4\n",
            "   ‚ùå Filtering out: scen10_6rep5\n",
            "   ‚ùå Filtering out: scen10_6rep6\n",
            "   ‚ùå Filtering out: scen10_6rep7\n",
            "   ‚ùå Filtering out: scen10_6rep8\n",
            "   ‚ùå Filtering out: scen10_6rep9\n",
            "   ‚úÖ Including: scen10_7rep1\n",
            "   ‚ùå Filtering out: scen10_7rep10\n",
            "   ‚úÖ Including: scen10_7rep2\n",
            "   ‚úÖ Including: scen10_7rep3\n",
            "   ‚ùå Filtering out: scen10_7rep4\n",
            "   ‚ùå Filtering out: scen10_7rep5\n",
            "   ‚ùå Filtering out: scen10_7rep6\n",
            "   ‚ùå Filtering out: scen10_7rep7\n",
            "   ‚ùå Filtering out: scen10_7rep8\n",
            "   ‚ùå Filtering out: scen10_7rep9\n",
            "   ‚úÖ Including: scen11_2rep1\n",
            "   ‚ùå Filtering out: scen11_2rep10\n",
            "   ‚úÖ Including: scen11_2rep2\n",
            "   ‚úÖ Including: scen11_2rep3\n",
            "   ‚ùå Filtering out: scen11_2rep4\n",
            "   ‚ùå Filtering out: scen11_2rep5\n",
            "   ‚ùå Filtering out: scen11_2rep6\n",
            "   ‚ùå Filtering out: scen11_2rep7\n",
            "   ‚ùå Filtering out: scen11_2rep8\n",
            "   ‚ùå Filtering out: scen11_2rep9\n",
            "   ‚úÖ Including: scen11_3rep1\n",
            "   ‚ùå Filtering out: scen11_3rep10\n",
            "   ‚úÖ Including: scen11_3rep2\n",
            "   ‚úÖ Including: scen11_3rep3\n",
            "   ‚ùå Filtering out: scen11_3rep4\n",
            "   ‚ùå Filtering out: scen11_3rep5\n",
            "   ‚ùå Filtering out: scen11_3rep6\n",
            "   ‚ùå Filtering out: scen11_3rep7\n",
            "   ‚ùå Filtering out: scen11_3rep8\n",
            "   ‚ùå Filtering out: scen11_3rep9\n",
            "   ‚úÖ Including: scen11_4rep1\n",
            "   ‚ùå Filtering out: scen11_4rep10\n",
            "   ‚úÖ Including: scen11_4rep2\n",
            "   ‚úÖ Including: scen11_4rep3\n",
            "   ‚ùå Filtering out: scen11_4rep4\n",
            "   ‚ùå Filtering out: scen11_4rep5\n",
            "   ‚ùå Filtering out: scen11_4rep6\n",
            "   ‚ùå Filtering out: scen11_4rep7\n",
            "   ‚ùå Filtering out: scen11_4rep8\n",
            "   ‚ùå Filtering out: scen11_4rep9\n",
            "   ‚úÖ Including: scen11_5rep1\n",
            "   ‚ùå Filtering out: scen11_5rep10\n",
            "   ‚úÖ Including: scen11_5rep2\n",
            "   ‚úÖ Including: scen11_5rep3\n",
            "   ‚ùå Filtering out: scen11_5rep4\n",
            "   ‚ùå Filtering out: scen11_5rep5\n",
            "   ‚ùå Filtering out: scen11_5rep6\n",
            "   ‚ùå Filtering out: scen11_5rep7\n",
            "   ‚ùå Filtering out: scen11_5rep8\n",
            "   ‚ùå Filtering out: scen11_5rep9\n",
            "   ‚úÖ Including: scen11_6rep1\n",
            "   ‚ùå Filtering out: scen11_6rep10\n",
            "   ‚úÖ Including: scen11_6rep2\n",
            "   ‚úÖ Including: scen11_6rep3\n",
            "   ‚ùå Filtering out: scen11_6rep4\n",
            "   ‚ùå Filtering out: scen11_6rep5\n",
            "   ‚ùå Filtering out: scen11_6rep6\n",
            "   ‚ùå Filtering out: scen11_6rep7\n",
            "   ‚ùå Filtering out: scen11_6rep8\n",
            "   ‚ùå Filtering out: scen11_6rep9\n",
            "   ‚úÖ Including: scen11_7rep1\n",
            "   ‚ùå Filtering out: scen11_7rep10\n",
            "   ‚úÖ Including: scen11_7rep2\n",
            "   ‚úÖ Including: scen11_7rep3\n",
            "   ‚ùå Filtering out: scen11_7rep4\n",
            "   ‚ùå Filtering out: scen11_7rep5\n",
            "   ‚ùå Filtering out: scen11_7rep6\n",
            "   ‚ùå Filtering out: scen11_7rep7\n",
            "   ‚ùå Filtering out: scen11_7rep8\n",
            "   ‚ùå Filtering out: scen11_7rep9\n",
            "   ‚úÖ Including: scen12_1rep1\n",
            "   ‚ùå Filtering out: scen12_1rep10\n",
            "   ‚úÖ Including: scen12_1rep2\n",
            "   ‚úÖ Including: scen12_1rep3\n",
            "   ‚ùå Filtering out: scen12_1rep4\n",
            "   ‚ùå Filtering out: scen12_1rep5\n",
            "   ‚ùå Filtering out: scen12_1rep6\n",
            "   ‚ùå Filtering out: scen12_1rep7\n",
            "   ‚ùå Filtering out: scen12_1rep8\n",
            "   ‚ùå Filtering out: scen12_1rep9\n",
            "   ‚úÖ Including: scen12_2rep1\n",
            "   ‚ùå Filtering out: scen12_2rep10\n",
            "   ‚úÖ Including: scen12_2rep2\n",
            "   ‚úÖ Including: scen12_2rep3\n",
            "   ‚ùå Filtering out: scen12_2rep4\n",
            "   ‚ùå Filtering out: scen12_2rep5\n",
            "   ‚ùå Filtering out: scen12_2rep6\n",
            "   ‚ùå Filtering out: scen12_2rep7\n",
            "   ‚ùå Filtering out: scen12_2rep8\n",
            "   ‚ùå Filtering out: scen12_2rep9\n",
            "   ‚úÖ Including: scen12_3rep1\n",
            "   ‚ùå Filtering out: scen12_3rep10\n",
            "   ‚úÖ Including: scen12_3rep2\n",
            "   ‚úÖ Including: scen12_3rep3\n",
            "   ‚ùå Filtering out: scen12_3rep4\n",
            "   ‚ùå Filtering out: scen12_3rep5\n",
            "   ‚ùå Filtering out: scen12_3rep6\n",
            "   ‚ùå Filtering out: scen12_3rep7\n",
            "   ‚ùå Filtering out: scen12_3rep8\n",
            "   ‚ùå Filtering out: scen12_3rep9\n",
            "   ‚úÖ Including: scen12_4rep1\n",
            "   ‚ùå Filtering out: scen12_4rep10\n",
            "   ‚úÖ Including: scen12_4rep2\n",
            "   ‚úÖ Including: scen12_4rep3\n",
            "   ‚ùå Filtering out: scen12_4rep4\n",
            "   ‚ùå Filtering out: scen12_4rep5\n",
            "   ‚ùå Filtering out: scen12_4rep6\n",
            "   ‚ùå Filtering out: scen12_4rep7\n",
            "   ‚ùå Filtering out: scen12_4rep8\n",
            "   ‚ùå Filtering out: scen12_4rep9\n",
            "   ‚úÖ Including: scen12_5rep1\n",
            "   ‚ùå Filtering out: scen12_5rep10\n",
            "   ‚úÖ Including: scen12_5rep2\n",
            "   ‚úÖ Including: scen12_5rep3\n",
            "   ‚ùå Filtering out: scen12_5rep4\n",
            "   ‚ùå Filtering out: scen12_5rep5\n",
            "   ‚ùå Filtering out: scen12_5rep6\n",
            "   ‚ùå Filtering out: scen12_5rep7\n",
            "   ‚ùå Filtering out: scen12_5rep8\n",
            "   ‚ùå Filtering out: scen12_5rep9\n",
            "   ‚úÖ Including: scen12_6rep1\n",
            "   ‚ùå Filtering out: scen12_6rep10\n",
            "   ‚úÖ Including: scen12_6rep2\n",
            "   ‚úÖ Including: scen12_6rep3\n",
            "   ‚ùå Filtering out: scen12_6rep4\n",
            "   ‚ùå Filtering out: scen12_6rep5\n",
            "   ‚ùå Filtering out: scen12_6rep6\n",
            "   ‚ùå Filtering out: scen12_6rep7\n",
            "   ‚ùå Filtering out: scen12_6rep8\n",
            "   ‚ùå Filtering out: scen12_6rep9\n",
            "   ‚úÖ Including: scen13_1rep1\n",
            "   ‚ùå Filtering out: scen13_1rep10\n",
            "   ‚úÖ Including: scen13_1rep2\n",
            "   ‚úÖ Including: scen13_1rep3\n",
            "   ‚ùå Filtering out: scen13_1rep4\n",
            "   ‚ùå Filtering out: scen13_1rep5\n",
            "   ‚ùå Filtering out: scen13_1rep6\n",
            "   ‚ùå Filtering out: scen13_1rep7\n",
            "   ‚ùå Filtering out: scen13_1rep8\n",
            "   ‚ùå Filtering out: scen13_1rep9\n",
            "   ‚úÖ Including: scen13_2rep1\n",
            "   ‚ùå Filtering out: scen13_2rep10\n",
            "   ‚úÖ Including: scen13_2rep2\n",
            "   ‚úÖ Including: scen13_2rep3\n",
            "   ‚ùå Filtering out: scen13_2rep4\n",
            "   ‚ùå Filtering out: scen13_2rep5\n",
            "   ‚ùå Filtering out: scen13_2rep6\n",
            "   ‚ùå Filtering out: scen13_2rep7\n",
            "   ‚ùå Filtering out: scen13_2rep8\n",
            "   ‚ùå Filtering out: scen13_2rep9\n",
            "   ‚úÖ Including: scen13_3rep1\n",
            "   ‚ùå Filtering out: scen13_3rep10\n",
            "   ‚úÖ Including: scen13_3rep2\n",
            "   ‚úÖ Including: scen13_3rep3\n",
            "   ‚ùå Filtering out: scen13_3rep4\n",
            "   ‚ùå Filtering out: scen13_3rep5\n",
            "   ‚ùå Filtering out: scen13_3rep6\n",
            "   ‚ùå Filtering out: scen13_3rep7\n",
            "   ‚ùå Filtering out: scen13_3rep8\n",
            "   ‚ùå Filtering out: scen13_3rep9\n",
            "   ‚úÖ Including: scen13_4rep1\n",
            "   ‚ùå Filtering out: scen13_4rep10\n",
            "   ‚úÖ Including: scen13_4rep2\n",
            "   ‚úÖ Including: scen13_4rep3\n",
            "   ‚ùå Filtering out: scen13_4rep4\n",
            "   ‚ùå Filtering out: scen13_4rep5\n",
            "   ‚ùå Filtering out: scen13_4rep6\n",
            "   ‚ùå Filtering out: scen13_4rep7\n",
            "   ‚ùå Filtering out: scen13_4rep8\n",
            "   ‚ùå Filtering out: scen13_4rep9\n",
            "   ‚úÖ Including: scen13_5rep1\n",
            "   ‚ùå Filtering out: scen13_5rep10\n",
            "   ‚úÖ Including: scen13_5rep2\n",
            "   ‚úÖ Including: scen13_5rep3\n",
            "   ‚ùå Filtering out: scen13_5rep4\n",
            "   ‚ùå Filtering out: scen13_5rep5\n",
            "   ‚ùå Filtering out: scen13_5rep6\n",
            "   ‚ùå Filtering out: scen13_5rep7\n",
            "   ‚ùå Filtering out: scen13_5rep8\n",
            "   ‚ùå Filtering out: scen13_5rep9\n",
            "   ‚úÖ Including: scen13_6rep1\n",
            "   ‚ùå Filtering out: scen13_6rep10\n",
            "   ‚úÖ Including: scen13_6rep2\n",
            "   ‚úÖ Including: scen13_6rep3\n",
            "   ‚ùå Filtering out: scen13_6rep4\n",
            "   ‚ùå Filtering out: scen13_6rep5\n",
            "   ‚ùå Filtering out: scen13_6rep6\n",
            "   ‚ùå Filtering out: scen13_6rep7\n",
            "   ‚ùå Filtering out: scen13_6rep8\n",
            "   ‚ùå Filtering out: scen13_6rep9\n",
            "   ‚úÖ Including: scen14_2rep1\n",
            "   ‚ùå Filtering out: scen14_2rep10\n",
            "   ‚úÖ Including: scen14_2rep2\n",
            "   ‚úÖ Including: scen14_2rep3\n",
            "   ‚ùå Filtering out: scen14_2rep4\n",
            "   ‚ùå Filtering out: scen14_2rep5\n",
            "   ‚ùå Filtering out: scen14_2rep6\n",
            "   ‚ùå Filtering out: scen14_2rep7\n",
            "   ‚ùå Filtering out: scen14_2rep8\n",
            "   ‚ùå Filtering out: scen14_2rep9\n",
            "   ‚úÖ Including: scen14_3rep1\n",
            "   ‚ùå Filtering out: scen14_3rep10\n",
            "   ‚úÖ Including: scen14_3rep2\n",
            "   ‚úÖ Including: scen14_3rep3\n",
            "   ‚ùå Filtering out: scen14_3rep4\n",
            "   ‚ùå Filtering out: scen14_3rep5\n",
            "   ‚ùå Filtering out: scen14_3rep6\n",
            "   ‚ùå Filtering out: scen14_3rep7\n",
            "   ‚ùå Filtering out: scen14_3rep8\n",
            "   ‚ùå Filtering out: scen14_3rep9\n",
            "   ‚úÖ Including: scen14_4rep1\n",
            "   ‚ùå Filtering out: scen14_4rep10\n",
            "   ‚úÖ Including: scen14_4rep2\n",
            "   ‚úÖ Including: scen14_4rep3\n",
            "   ‚ùå Filtering out: scen14_4rep4\n",
            "   ‚ùå Filtering out: scen14_4rep5\n",
            "   ‚ùå Filtering out: scen14_4rep6\n",
            "   ‚ùå Filtering out: scen14_4rep7\n",
            "   ‚ùå Filtering out: scen14_4rep8\n",
            "   ‚ùå Filtering out: scen14_4rep9\n",
            "   ‚úÖ Including: scen14_5rep1\n",
            "   ‚ùå Filtering out: scen14_5rep10\n",
            "   ‚úÖ Including: scen14_5rep2\n",
            "   ‚úÖ Including: scen14_5rep3\n",
            "   ‚ùå Filtering out: scen14_5rep4\n",
            "   ‚ùå Filtering out: scen14_5rep5\n",
            "   ‚ùå Filtering out: scen14_5rep6\n",
            "   ‚ùå Filtering out: scen14_5rep7\n",
            "   ‚ùå Filtering out: scen14_5rep8\n",
            "   ‚ùå Filtering out: scen14_5rep9\n",
            "   ‚úÖ Including: scen14_6rep1\n",
            "   ‚ùå Filtering out: scen14_6rep10\n",
            "   ‚úÖ Including: scen14_6rep2\n",
            "   ‚úÖ Including: scen14_6rep3\n",
            "   ‚ùå Filtering out: scen14_6rep4\n",
            "   ‚ùå Filtering out: scen14_6rep5\n",
            "   ‚ùå Filtering out: scen14_6rep6\n",
            "   ‚ùå Filtering out: scen14_6rep7\n",
            "   ‚ùå Filtering out: scen14_6rep8\n",
            "   ‚ùå Filtering out: scen14_6rep9\n",
            "   ‚úÖ Including: scen15_1rep1\n",
            "   ‚ùå Filtering out: scen15_1rep10\n",
            "   ‚úÖ Including: scen15_1rep2\n",
            "   ‚úÖ Including: scen15_1rep3\n",
            "   ‚ùå Filtering out: scen15_1rep4\n",
            "   ‚ùå Filtering out: scen15_1rep5\n",
            "   ‚ùå Filtering out: scen15_1rep6\n",
            "   ‚ùå Filtering out: scen15_1rep7\n",
            "   ‚ùå Filtering out: scen15_1rep8\n",
            "   ‚ùå Filtering out: scen15_1rep9\n",
            "   ‚úÖ Including: scen15_2rep1\n",
            "   ‚ùå Filtering out: scen15_2rep10\n",
            "   ‚úÖ Including: scen15_2rep2\n",
            "   ‚úÖ Including: scen15_2rep3\n",
            "   ‚ùå Filtering out: scen15_2rep4\n",
            "   ‚ùå Filtering out: scen15_2rep5\n",
            "   ‚ùå Filtering out: scen15_2rep6\n",
            "   ‚ùå Filtering out: scen15_2rep7\n",
            "   ‚ùå Filtering out: scen15_2rep8\n",
            "   ‚ùå Filtering out: scen15_2rep9\n",
            "   ‚úÖ Including: scen15_3rep1\n",
            "   ‚ùå Filtering out: scen15_3rep10\n",
            "   ‚úÖ Including: scen15_3rep2\n",
            "   ‚úÖ Including: scen15_3rep3\n",
            "   ‚ùå Filtering out: scen15_3rep4\n",
            "   ‚ùå Filtering out: scen15_3rep5\n",
            "   ‚ùå Filtering out: scen15_3rep6\n",
            "   ‚ùå Filtering out: scen15_3rep7\n",
            "   ‚ùå Filtering out: scen15_3rep8\n",
            "   ‚ùå Filtering out: scen15_3rep9\n",
            "   ‚úÖ Including: scen15_4rep1\n",
            "   ‚ùå Filtering out: scen15_4rep10\n",
            "   ‚úÖ Including: scen15_4rep2\n",
            "   ‚úÖ Including: scen15_4rep3\n",
            "   ‚ùå Filtering out: scen15_4rep4\n",
            "   ‚ùå Filtering out: scen15_4rep5\n",
            "   ‚ùå Filtering out: scen15_4rep6\n",
            "   ‚ùå Filtering out: scen15_4rep7\n",
            "   ‚ùå Filtering out: scen15_4rep8\n",
            "   ‚ùå Filtering out: scen15_4rep9\n",
            "   ‚úÖ Including: scen15_5rep1\n",
            "   ‚ùå Filtering out: scen15_5rep10\n",
            "   ‚úÖ Including: scen15_5rep2\n",
            "   ‚úÖ Including: scen15_5rep3\n",
            "   ‚ùå Filtering out: scen15_5rep4\n",
            "   ‚ùå Filtering out: scen15_5rep5\n",
            "   ‚ùå Filtering out: scen15_5rep6\n",
            "   ‚ùå Filtering out: scen15_5rep7\n",
            "   ‚ùå Filtering out: scen15_5rep8\n",
            "   ‚ùå Filtering out: scen15_5rep9\n",
            "   ‚úÖ Including: scen16_1rep1\n",
            "   ‚ùå Filtering out: scen16_1rep10\n",
            "   ‚úÖ Including: scen16_1rep2\n",
            "   ‚úÖ Including: scen16_1rep3\n",
            "   ‚ùå Filtering out: scen16_1rep4\n",
            "   ‚ùå Filtering out: scen16_1rep5\n",
            "   ‚ùå Filtering out: scen16_1rep6\n",
            "   ‚ùå Filtering out: scen16_1rep7\n",
            "   ‚ùå Filtering out: scen16_1rep8\n",
            "   ‚ùå Filtering out: scen16_1rep9\n",
            "   ‚úÖ Including: scen16_2rep1\n",
            "   ‚ùå Filtering out: scen16_2rep10\n",
            "   ‚úÖ Including: scen16_2rep2\n",
            "   ‚úÖ Including: scen16_2rep3\n",
            "   ‚ùå Filtering out: scen16_2rep4\n",
            "   ‚ùå Filtering out: scen16_2rep5\n",
            "   ‚ùå Filtering out: scen16_2rep6\n",
            "   ‚ùå Filtering out: scen16_2rep7\n",
            "   ‚ùå Filtering out: scen16_2rep8\n",
            "   ‚ùå Filtering out: scen16_2rep9\n",
            "   ‚úÖ Including: scen16_3rep1\n",
            "   ‚ùå Filtering out: scen16_3rep10\n",
            "   ‚úÖ Including: scen16_3rep2\n",
            "   ‚úÖ Including: scen16_3rep3\n",
            "   ‚ùå Filtering out: scen16_3rep4\n",
            "   ‚ùå Filtering out: scen16_3rep5\n",
            "   ‚ùå Filtering out: scen16_3rep6\n",
            "   ‚ùå Filtering out: scen16_3rep7\n",
            "   ‚ùå Filtering out: scen16_3rep8\n",
            "   ‚ùå Filtering out: scen16_3rep9\n",
            "   ‚úÖ Including: scen16_4rep1\n",
            "   ‚ùå Filtering out: scen16_4rep10\n",
            "   ‚úÖ Including: scen16_4rep2\n",
            "   ‚úÖ Including: scen16_4rep3\n",
            "   ‚ùå Filtering out: scen16_4rep4\n",
            "   ‚ùå Filtering out: scen16_4rep5\n",
            "   ‚ùå Filtering out: scen16_4rep6\n",
            "   ‚ùå Filtering out: scen16_4rep7\n",
            "   ‚ùå Filtering out: scen16_4rep8\n",
            "   ‚ùå Filtering out: scen16_4rep9\n",
            "   ‚úÖ Including: scen16_5rep1\n",
            "   ‚ùå Filtering out: scen16_5rep10\n",
            "   ‚úÖ Including: scen16_5rep2\n",
            "   ‚úÖ Including: scen16_5rep3\n",
            "   ‚ùå Filtering out: scen16_5rep4\n",
            "   ‚ùå Filtering out: scen16_5rep5\n",
            "   ‚ùå Filtering out: scen16_5rep6\n",
            "   ‚ùå Filtering out: scen16_5rep7\n",
            "   ‚ùå Filtering out: scen16_5rep8\n",
            "   ‚ùå Filtering out: scen16_5rep9\n",
            "   ‚úÖ Including: scen17_2rep1\n",
            "   ‚ùå Filtering out: scen17_2rep10\n",
            "   ‚úÖ Including: scen17_2rep2\n",
            "   ‚úÖ Including: scen17_2rep3\n",
            "   ‚ùå Filtering out: scen17_2rep4\n",
            "   ‚ùå Filtering out: scen17_2rep5\n",
            "   ‚ùå Filtering out: scen17_2rep6\n",
            "   ‚ùå Filtering out: scen17_2rep7\n",
            "   ‚ùå Filtering out: scen17_2rep8\n",
            "   ‚ùå Filtering out: scen17_2rep9\n",
            "   ‚úÖ Including: scen17_3rep1\n",
            "   ‚ùå Filtering out: scen17_3rep10\n",
            "   ‚úÖ Including: scen17_3rep2\n",
            "   ‚úÖ Including: scen17_3rep3\n",
            "   ‚ùå Filtering out: scen17_3rep4\n",
            "   ‚ùå Filtering out: scen17_3rep5\n",
            "   ‚ùå Filtering out: scen17_3rep6\n",
            "   ‚ùå Filtering out: scen17_3rep7\n",
            "   ‚ùå Filtering out: scen17_3rep8\n",
            "   ‚ùå Filtering out: scen17_3rep9\n",
            "   ‚úÖ Including: scen17_4rep1\n",
            "   ‚ùå Filtering out: scen17_4rep10\n",
            "   ‚úÖ Including: scen17_4rep2\n",
            "   ‚úÖ Including: scen17_4rep3\n",
            "   ‚ùå Filtering out: scen17_4rep4\n",
            "   ‚ùå Filtering out: scen17_4rep5\n",
            "   ‚ùå Filtering out: scen17_4rep6\n",
            "   ‚ùå Filtering out: scen17_4rep7\n",
            "   ‚ùå Filtering out: scen17_4rep8\n",
            "   ‚ùå Filtering out: scen17_4rep9\n",
            "   ‚úÖ Including: scen17_5rep1\n",
            "   ‚ùå Filtering out: scen17_5rep10\n",
            "   ‚úÖ Including: scen17_5rep2\n",
            "   ‚úÖ Including: scen17_5rep3\n",
            "   ‚ùå Filtering out: scen17_5rep4\n",
            "   ‚ùå Filtering out: scen17_5rep5\n",
            "   ‚ùå Filtering out: scen17_5rep6\n",
            "   ‚ùå Filtering out: scen17_5rep7\n",
            "   ‚ùå Filtering out: scen17_5rep8\n",
            "   ‚ùå Filtering out: scen17_5rep9\n",
            "   ‚úÖ Including: scen18_1rep1\n",
            "   ‚ùå Filtering out: scen18_1rep10\n",
            "   ‚úÖ Including: scen18_1rep2\n",
            "   ‚úÖ Including: scen18_1rep3\n",
            "   ‚ùå Filtering out: scen18_1rep4\n",
            "   ‚ùå Filtering out: scen18_1rep5\n",
            "   ‚ùå Filtering out: scen18_1rep6\n",
            "   ‚ùå Filtering out: scen18_1rep7\n",
            "   ‚ùå Filtering out: scen18_1rep8\n",
            "   ‚ùå Filtering out: scen18_1rep9\n",
            "   ‚úÖ Including: scen18_2rep1\n",
            "   ‚ùå Filtering out: scen18_2rep10\n",
            "   ‚úÖ Including: scen18_2rep2\n",
            "   ‚úÖ Including: scen18_2rep3\n",
            "   ‚ùå Filtering out: scen18_2rep4\n",
            "   ‚ùå Filtering out: scen18_2rep5\n",
            "   ‚ùå Filtering out: scen18_2rep6\n",
            "   ‚ùå Filtering out: scen18_2rep7\n",
            "   ‚ùå Filtering out: scen18_2rep8\n",
            "   ‚ùå Filtering out: scen18_2rep9\n",
            "   ‚úÖ Including: scen18_3rep1\n",
            "   ‚ùå Filtering out: scen18_3rep10\n",
            "   ‚úÖ Including: scen18_3rep2\n",
            "   ‚úÖ Including: scen18_3rep3\n",
            "   ‚ùå Filtering out: scen18_3rep4\n",
            "   ‚ùå Filtering out: scen18_3rep5\n",
            "   ‚ùå Filtering out: scen18_3rep6\n",
            "   ‚ùå Filtering out: scen18_3rep7\n",
            "   ‚ùå Filtering out: scen18_3rep8\n",
            "   ‚ùå Filtering out: scen18_3rep9\n",
            "   ‚úÖ Including: scen18_4rep1\n",
            "   ‚ùå Filtering out: scen18_4rep10\n",
            "   ‚úÖ Including: scen18_4rep2\n",
            "   ‚úÖ Including: scen18_4rep3\n",
            "   ‚ùå Filtering out: scen18_4rep4\n",
            "   ‚ùå Filtering out: scen18_4rep5\n",
            "   ‚ùå Filtering out: scen18_4rep6\n",
            "   ‚ùå Filtering out: scen18_4rep7\n",
            "   ‚ùå Filtering out: scen18_4rep8\n",
            "   ‚ùå Filtering out: scen18_4rep9\n",
            "   ‚úÖ Including: scen19_1rep1\n",
            "   ‚ùå Filtering out: scen19_1rep10\n",
            "   ‚úÖ Including: scen19_1rep2\n",
            "   ‚úÖ Including: scen19_1rep3\n",
            "   ‚ùå Filtering out: scen19_1rep4\n",
            "   ‚ùå Filtering out: scen19_1rep5\n",
            "   ‚ùå Filtering out: scen19_1rep6\n",
            "   ‚ùå Filtering out: scen19_1rep7\n",
            "   ‚ùå Filtering out: scen19_1rep8\n",
            "   ‚ùå Filtering out: scen19_1rep9\n",
            "   ‚úÖ Including: scen19_2rep1\n",
            "   ‚ùå Filtering out: scen19_2rep10\n",
            "   ‚úÖ Including: scen19_2rep2\n",
            "   ‚úÖ Including: scen19_2rep3\n",
            "   ‚ùå Filtering out: scen19_2rep4\n",
            "   ‚ùå Filtering out: scen19_2rep5\n",
            "   ‚ùå Filtering out: scen19_2rep6\n",
            "   ‚ùå Filtering out: scen19_2rep7\n",
            "   ‚ùå Filtering out: scen19_2rep8\n",
            "   ‚ùå Filtering out: scen19_2rep9\n",
            "   ‚úÖ Including: scen19_3rep1\n",
            "   ‚ùå Filtering out: scen19_3rep10\n",
            "   ‚úÖ Including: scen19_3rep2\n",
            "   ‚úÖ Including: scen19_3rep3\n",
            "   ‚ùå Filtering out: scen19_3rep4\n",
            "   ‚ùå Filtering out: scen19_3rep5\n",
            "   ‚ùå Filtering out: scen19_3rep6\n",
            "   ‚ùå Filtering out: scen19_3rep7\n",
            "   ‚ùå Filtering out: scen19_3rep8\n",
            "   ‚ùå Filtering out: scen19_3rep9\n",
            "   ‚úÖ Including: scen19_4rep1\n",
            "   ‚ùå Filtering out: scen19_4rep10\n",
            "   ‚úÖ Including: scen19_4rep2\n",
            "   ‚úÖ Including: scen19_4rep3\n",
            "   ‚ùå Filtering out: scen19_4rep4\n",
            "   ‚ùå Filtering out: scen19_4rep5\n",
            "   ‚ùå Filtering out: scen19_4rep6\n",
            "   ‚ùå Filtering out: scen19_4rep7\n",
            "   ‚ùå Filtering out: scen19_4rep8\n",
            "   ‚ùå Filtering out: scen19_4rep9\n",
            "   ‚úÖ Including: scen19_5rep1\n",
            "   ‚ùå Filtering out: scen19_5rep10\n",
            "   ‚úÖ Including: scen19_5rep2\n",
            "   ‚úÖ Including: scen19_5rep3\n",
            "   ‚ùå Filtering out: scen19_5rep4\n",
            "   ‚ùå Filtering out: scen19_5rep5\n",
            "   ‚ùå Filtering out: scen19_5rep6\n",
            "   ‚ùå Filtering out: scen19_5rep7\n",
            "   ‚ùå Filtering out: scen19_5rep8\n",
            "   ‚ùå Filtering out: scen19_5rep9\n",
            "   ‚úÖ Including: scen1_10rep1\n",
            "   ‚ùå Filtering out: scen1_10rep10\n",
            "   ‚úÖ Including: scen1_10rep2\n",
            "   ‚úÖ Including: scen1_10rep3\n",
            "   ‚ùå Filtering out: scen1_10rep4\n",
            "   ‚ùå Filtering out: scen1_10rep5\n",
            "   ‚ùå Filtering out: scen1_10rep6\n",
            "   ‚ùå Filtering out: scen1_10rep7\n",
            "   ‚ùå Filtering out: scen1_10rep8\n",
            "   ‚ùå Filtering out: scen1_10rep9\n",
            "   ‚úÖ Including: scen1_1rep1\n",
            "   ‚ùå Filtering out: scen1_1rep10\n",
            "   ‚úÖ Including: scen1_1rep2\n",
            "   ‚úÖ Including: scen1_1rep3\n",
            "   ‚ùå Filtering out: scen1_1rep4\n",
            "   ‚ùå Filtering out: scen1_1rep5\n",
            "   ‚ùå Filtering out: scen1_1rep6\n",
            "   ‚ùå Filtering out: scen1_1rep7\n",
            "   ‚ùå Filtering out: scen1_1rep8\n",
            "   ‚ùå Filtering out: scen1_1rep9\n",
            "   ‚úÖ Including: scen1_2rep1\n",
            "   ‚ùå Filtering out: scen1_2rep10\n",
            "   ‚úÖ Including: scen1_2rep2\n",
            "   ‚úÖ Including: scen1_2rep3\n",
            "   ‚ùå Filtering out: scen1_2rep4\n",
            "   ‚ùå Filtering out: scen1_2rep5\n",
            "   ‚ùå Filtering out: scen1_2rep6\n",
            "   ‚ùå Filtering out: scen1_2rep7\n",
            "   ‚ùå Filtering out: scen1_2rep8\n",
            "   ‚ùå Filtering out: scen1_2rep9\n",
            "   ‚úÖ Including: scen1_3rep1\n",
            "   ‚ùå Filtering out: scen1_3rep10\n",
            "   ‚úÖ Including: scen1_3rep2\n",
            "   ‚úÖ Including: scen1_3rep3\n",
            "   ‚ùå Filtering out: scen1_3rep4\n",
            "   ‚ùå Filtering out: scen1_3rep5\n",
            "   ‚ùå Filtering out: scen1_3rep6\n",
            "   ‚ùå Filtering out: scen1_3rep7\n",
            "   ‚ùå Filtering out: scen1_3rep8\n",
            "   ‚ùå Filtering out: scen1_3rep9\n",
            "   ‚úÖ Including: scen1_4rep1\n",
            "   ‚ùå Filtering out: scen1_4rep10\n",
            "   ‚úÖ Including: scen1_4rep2\n",
            "   ‚úÖ Including: scen1_4rep3\n",
            "   ‚ùå Filtering out: scen1_4rep4\n",
            "   ‚ùå Filtering out: scen1_4rep5\n",
            "   ‚ùå Filtering out: scen1_4rep6\n",
            "   ‚ùå Filtering out: scen1_4rep7\n",
            "   ‚ùå Filtering out: scen1_4rep8\n",
            "   ‚ùå Filtering out: scen1_4rep9\n",
            "   ‚úÖ Including: scen1_5rep1\n",
            "   ‚ùå Filtering out: scen1_5rep10\n",
            "   ‚úÖ Including: scen1_5rep2\n",
            "   ‚úÖ Including: scen1_5rep3\n",
            "   ‚ùå Filtering out: scen1_5rep4\n",
            "   ‚ùå Filtering out: scen1_5rep5\n",
            "   ‚ùå Filtering out: scen1_5rep6\n",
            "   ‚ùå Filtering out: scen1_5rep7\n",
            "   ‚ùå Filtering out: scen1_5rep8\n",
            "   ‚ùå Filtering out: scen1_5rep9\n",
            "   ‚úÖ Including: scen1_6rep1\n",
            "   ‚ùå Filtering out: scen1_6rep10\n",
            "   ‚úÖ Including: scen1_6rep2\n",
            "   ‚úÖ Including: scen1_6rep3\n",
            "   ‚ùå Filtering out: scen1_6rep4\n",
            "   ‚ùå Filtering out: scen1_6rep5\n",
            "   ‚ùå Filtering out: scen1_6rep6\n",
            "   ‚ùå Filtering out: scen1_6rep7\n",
            "   ‚ùå Filtering out: scen1_6rep8\n",
            "   ‚ùå Filtering out: scen1_6rep9\n",
            "   ‚úÖ Including: scen1_7rep1\n",
            "   ‚ùå Filtering out: scen1_7rep10\n",
            "   ‚úÖ Including: scen1_7rep2\n",
            "   ‚úÖ Including: scen1_7rep3\n",
            "   ‚ùå Filtering out: scen1_7rep4\n",
            "   ‚ùå Filtering out: scen1_7rep5\n",
            "   ‚ùå Filtering out: scen1_7rep6\n",
            "   ‚ùå Filtering out: scen1_7rep7\n",
            "   ‚ùå Filtering out: scen1_7rep8\n",
            "   ‚ùå Filtering out: scen1_7rep9\n",
            "   ‚úÖ Including: scen1_8rep1\n",
            "   ‚ùå Filtering out: scen1_8rep10\n",
            "   ‚úÖ Including: scen1_8rep2\n",
            "   ‚úÖ Including: scen1_8rep3\n",
            "   ‚ùå Filtering out: scen1_8rep4\n",
            "   ‚ùå Filtering out: scen1_8rep5\n",
            "   ‚ùå Filtering out: scen1_8rep6\n",
            "   ‚ùå Filtering out: scen1_8rep7\n",
            "   ‚ùå Filtering out: scen1_8rep8\n",
            "   ‚ùå Filtering out: scen1_8rep9\n",
            "   ‚úÖ Including: scen1_9rep1\n",
            "   ‚ùå Filtering out: scen1_9rep10\n",
            "   ‚úÖ Including: scen1_9rep2\n",
            "   ‚úÖ Including: scen1_9rep3\n",
            "   ‚ùå Filtering out: scen1_9rep4\n",
            "   ‚ùå Filtering out: scen1_9rep5\n",
            "   ‚ùå Filtering out: scen1_9rep6\n",
            "   ‚ùå Filtering out: scen1_9rep7\n",
            "   ‚ùå Filtering out: scen1_9rep8\n",
            "   ‚ùå Filtering out: scen1_9rep9\n",
            "   ‚úÖ Including: scen20_2rep1\n",
            "   ‚ùå Filtering out: scen20_2rep10\n",
            "   ‚úÖ Including: scen20_2rep2\n",
            "   ‚úÖ Including: scen20_2rep3\n",
            "   ‚ùå Filtering out: scen20_2rep4\n",
            "   ‚ùå Filtering out: scen20_2rep5\n",
            "   ‚ùå Filtering out: scen20_2rep6\n",
            "   ‚ùå Filtering out: scen20_2rep7\n",
            "   ‚ùå Filtering out: scen20_2rep8\n",
            "   ‚ùå Filtering out: scen20_2rep9\n",
            "   ‚úÖ Including: scen20_3rep1\n",
            "   ‚ùå Filtering out: scen20_3rep10\n",
            "   ‚úÖ Including: scen20_3rep2\n",
            "   ‚úÖ Including: scen20_3rep3\n",
            "   ‚ùå Filtering out: scen20_3rep4\n",
            "   ‚ùå Filtering out: scen20_3rep5\n",
            "   ‚ùå Filtering out: scen20_3rep6\n",
            "   ‚ùå Filtering out: scen20_3rep7\n",
            "   ‚ùå Filtering out: scen20_3rep8\n",
            "   ‚ùå Filtering out: scen20_3rep9\n",
            "   ‚úÖ Including: scen20_4rep1\n",
            "   ‚ùå Filtering out: scen20_4rep10\n",
            "   ‚úÖ Including: scen20_4rep2\n",
            "   ‚úÖ Including: scen20_4rep3\n",
            "   ‚ùå Filtering out: scen20_4rep4\n",
            "   ‚ùå Filtering out: scen20_4rep5\n",
            "   ‚ùå Filtering out: scen20_4rep6\n",
            "   ‚ùå Filtering out: scen20_4rep7\n",
            "   ‚ùå Filtering out: scen20_4rep8\n",
            "   ‚ùå Filtering out: scen20_4rep9\n",
            "   ‚úÖ Including: scen20_5rep1\n",
            "   ‚ùå Filtering out: scen20_5rep10\n",
            "   ‚úÖ Including: scen20_5rep2\n",
            "   ‚úÖ Including: scen20_5rep3\n",
            "   ‚ùå Filtering out: scen20_5rep4\n",
            "   ‚ùå Filtering out: scen20_5rep5\n",
            "   ‚ùå Filtering out: scen20_5rep6\n",
            "   ‚ùå Filtering out: scen20_5rep7\n",
            "   ‚ùå Filtering out: scen20_5rep8\n",
            "   ‚ùå Filtering out: scen20_5rep9\n",
            "   ‚úÖ Including: scen21_1rep1\n",
            "   ‚ùå Filtering out: scen21_1rep10\n",
            "   ‚úÖ Including: scen21_1rep2\n",
            "   ‚úÖ Including: scen21_1rep3\n",
            "   ‚ùå Filtering out: scen21_1rep4\n",
            "   ‚ùå Filtering out: scen21_1rep5\n",
            "   ‚ùå Filtering out: scen21_1rep6\n",
            "   ‚ùå Filtering out: scen21_1rep7\n",
            "   ‚ùå Filtering out: scen21_1rep8\n",
            "   ‚ùå Filtering out: scen21_1rep9\n",
            "   ‚úÖ Including: scen21_2rep1\n",
            "   ‚ùå Filtering out: scen21_2rep10\n",
            "   ‚úÖ Including: scen21_2rep2\n",
            "   ‚úÖ Including: scen21_2rep3\n",
            "   ‚ùå Filtering out: scen21_2rep4\n",
            "   ‚ùå Filtering out: scen21_2rep5\n",
            "   ‚ùå Filtering out: scen21_2rep6\n",
            "   ‚ùå Filtering out: scen21_2rep7\n",
            "   ‚ùå Filtering out: scen21_2rep8\n",
            "   ‚ùå Filtering out: scen21_2rep9\n",
            "   ‚úÖ Including: scen21_3rep1\n",
            "   ‚ùå Filtering out: scen21_3rep10\n",
            "   ‚úÖ Including: scen21_3rep2\n",
            "   ‚úÖ Including: scen21_3rep3\n",
            "   ‚ùå Filtering out: scen21_3rep4\n",
            "   ‚ùå Filtering out: scen21_3rep5\n",
            "   ‚ùå Filtering out: scen21_3rep6\n",
            "   ‚ùå Filtering out: scen21_3rep7\n",
            "   ‚ùå Filtering out: scen21_3rep8\n",
            "   ‚ùå Filtering out: scen21_3rep9\n",
            "   ‚úÖ Including: scen21_4rep1\n",
            "   ‚ùå Filtering out: scen21_4rep10\n",
            "   ‚úÖ Including: scen21_4rep2\n",
            "   ‚úÖ Including: scen21_4rep3\n",
            "   ‚ùå Filtering out: scen21_4rep4\n",
            "   ‚ùå Filtering out: scen21_4rep5\n",
            "   ‚ùå Filtering out: scen21_4rep6\n",
            "   ‚ùå Filtering out: scen21_4rep7\n",
            "   ‚ùå Filtering out: scen21_4rep8\n",
            "   ‚ùå Filtering out: scen21_4rep9\n",
            "   ‚úÖ Including: scen2_10rep1\n",
            "   ‚ùå Filtering out: scen2_10rep10\n",
            "   ‚úÖ Including: scen2_10rep2\n",
            "   ‚úÖ Including: scen2_10rep3\n",
            "   ‚ùå Filtering out: scen2_10rep4\n",
            "   ‚ùå Filtering out: scen2_10rep5\n",
            "   ‚ùå Filtering out: scen2_10rep6\n",
            "   ‚ùå Filtering out: scen2_10rep7\n",
            "   ‚ùå Filtering out: scen2_10rep8\n",
            "   ‚ùå Filtering out: scen2_10rep9\n",
            "   ‚úÖ Including: scen2_2rep1\n",
            "   ‚ùå Filtering out: scen2_2rep10\n",
            "   ‚úÖ Including: scen2_2rep2\n",
            "   ‚úÖ Including: scen2_2rep3\n",
            "   ‚ùå Filtering out: scen2_2rep4\n",
            "   ‚ùå Filtering out: scen2_2rep5\n",
            "   ‚ùå Filtering out: scen2_2rep6\n",
            "   ‚ùå Filtering out: scen2_2rep7\n",
            "   ‚ùå Filtering out: scen2_2rep8\n",
            "   ‚ùå Filtering out: scen2_2rep9\n",
            "   ‚úÖ Including: scen2_3rep1\n",
            "   ‚ùå Filtering out: scen2_3rep10\n",
            "   ‚úÖ Including: scen2_3rep2\n",
            "   ‚úÖ Including: scen2_3rep3\n",
            "   ‚ùå Filtering out: scen2_3rep4\n",
            "   ‚ùå Filtering out: scen2_3rep5\n",
            "   ‚ùå Filtering out: scen2_3rep6\n",
            "   ‚ùå Filtering out: scen2_3rep7\n",
            "   ‚ùå Filtering out: scen2_3rep8\n",
            "   ‚ùå Filtering out: scen2_3rep9\n",
            "   ‚úÖ Including: scen2_4rep1\n",
            "   ‚ùå Filtering out: scen2_4rep10\n",
            "   ‚úÖ Including: scen2_4rep2\n",
            "   ‚úÖ Including: scen2_4rep3\n",
            "   ‚ùå Filtering out: scen2_4rep4\n",
            "   ‚ùå Filtering out: scen2_4rep5\n",
            "   ‚ùå Filtering out: scen2_4rep6\n",
            "   ‚ùå Filtering out: scen2_4rep7\n",
            "   ‚ùå Filtering out: scen2_4rep8\n",
            "   ‚ùå Filtering out: scen2_4rep9\n",
            "   ‚úÖ Including: scen2_5rep1\n",
            "   ‚ùå Filtering out: scen2_5rep10\n",
            "   ‚úÖ Including: scen2_5rep2\n",
            "   ‚úÖ Including: scen2_5rep3\n",
            "   ‚ùå Filtering out: scen2_5rep4\n",
            "   ‚ùå Filtering out: scen2_5rep5\n",
            "   ‚ùå Filtering out: scen2_5rep6\n",
            "   ‚ùå Filtering out: scen2_5rep7\n",
            "   ‚ùå Filtering out: scen2_5rep8\n",
            "   ‚ùå Filtering out: scen2_5rep9\n",
            "   ‚úÖ Including: scen2_6rep1\n",
            "   ‚ùå Filtering out: scen2_6rep10\n",
            "   ‚úÖ Including: scen2_6rep2\n",
            "   ‚úÖ Including: scen2_6rep3\n",
            "   ‚ùå Filtering out: scen2_6rep4\n",
            "   ‚ùå Filtering out: scen2_6rep5\n",
            "   ‚ùå Filtering out: scen2_6rep6\n",
            "   ‚ùå Filtering out: scen2_6rep7\n",
            "   ‚ùå Filtering out: scen2_6rep8\n",
            "   ‚ùå Filtering out: scen2_6rep9\n",
            "   ‚úÖ Including: scen2_7rep1\n",
            "   ‚ùå Filtering out: scen2_7rep10\n",
            "   ‚úÖ Including: scen2_7rep2\n",
            "   ‚úÖ Including: scen2_7rep3\n",
            "   ‚ùå Filtering out: scen2_7rep4\n",
            "   ‚ùå Filtering out: scen2_7rep5\n",
            "   ‚ùå Filtering out: scen2_7rep6\n",
            "   ‚ùå Filtering out: scen2_7rep7\n",
            "   ‚ùå Filtering out: scen2_7rep8\n",
            "   ‚ùå Filtering out: scen2_7rep9\n",
            "   ‚úÖ Including: scen2_8rep1\n",
            "   ‚ùå Filtering out: scen2_8rep10\n",
            "   ‚úÖ Including: scen2_8rep2\n",
            "   ‚úÖ Including: scen2_8rep3\n",
            "   ‚ùå Filtering out: scen2_8rep4\n",
            "   ‚ùå Filtering out: scen2_8rep5\n",
            "   ‚ùå Filtering out: scen2_8rep6\n",
            "   ‚ùå Filtering out: scen2_8rep7\n",
            "   ‚ùå Filtering out: scen2_8rep8\n",
            "   ‚ùå Filtering out: scen2_8rep9\n",
            "   ‚úÖ Including: scen2_9rep1\n",
            "   ‚ùå Filtering out: scen2_9rep10\n",
            "   ‚úÖ Including: scen2_9rep2\n",
            "   ‚úÖ Including: scen2_9rep3\n",
            "   ‚ùå Filtering out: scen2_9rep4\n",
            "   ‚ùå Filtering out: scen2_9rep5\n",
            "   ‚ùå Filtering out: scen2_9rep6\n",
            "   ‚ùå Filtering out: scen2_9rep7\n",
            "   ‚ùå Filtering out: scen2_9rep8\n",
            "   ‚ùå Filtering out: scen2_9rep9\n",
            "   ‚úÖ Including: scen3_1rep1\n",
            "   ‚ùå Filtering out: scen3_1rep10\n",
            "   ‚úÖ Including: scen3_1rep2\n",
            "   ‚úÖ Including: scen3_1rep3\n",
            "   ‚ùå Filtering out: scen3_1rep4\n",
            "   ‚ùå Filtering out: scen3_1rep5\n",
            "   ‚ùå Filtering out: scen3_1rep6\n",
            "   ‚ùå Filtering out: scen3_1rep7\n",
            "   ‚ùå Filtering out: scen3_1rep8\n",
            "   ‚ùå Filtering out: scen3_1rep9\n",
            "   ‚úÖ Including: scen3_2rep1\n",
            "   ‚ùå Filtering out: scen3_2rep10\n",
            "   ‚úÖ Including: scen3_2rep2\n",
            "   ‚úÖ Including: scen3_2rep3\n",
            "   ‚ùå Filtering out: scen3_2rep4\n",
            "   ‚ùå Filtering out: scen3_2rep5\n",
            "   ‚ùå Filtering out: scen3_2rep6\n",
            "   ‚ùå Filtering out: scen3_2rep7\n",
            "   ‚ùå Filtering out: scen3_2rep8\n",
            "   ‚ùå Filtering out: scen3_2rep9\n",
            "   ‚úÖ Including: scen3_3rep1\n",
            "   ‚ùå Filtering out: scen3_3rep10\n",
            "   ‚úÖ Including: scen3_3rep2\n",
            "   ‚úÖ Including: scen3_3rep3\n",
            "   ‚ùå Filtering out: scen3_3rep4\n",
            "   ‚ùå Filtering out: scen3_3rep5\n",
            "   ‚ùå Filtering out: scen3_3rep6\n",
            "   ‚ùå Filtering out: scen3_3rep7\n",
            "   ‚ùå Filtering out: scen3_3rep8\n",
            "   ‚ùå Filtering out: scen3_3rep9\n",
            "   ‚úÖ Including: scen3_4rep1\n",
            "   ‚ùå Filtering out: scen3_4rep10\n",
            "   ‚úÖ Including: scen3_4rep2\n",
            "   ‚úÖ Including: scen3_4rep3\n",
            "   ‚ùå Filtering out: scen3_4rep4\n",
            "   ‚ùå Filtering out: scen3_4rep5\n",
            "   ‚ùå Filtering out: scen3_4rep6\n",
            "   ‚ùå Filtering out: scen3_4rep7\n",
            "   ‚ùå Filtering out: scen3_4rep8\n",
            "   ‚ùå Filtering out: scen3_4rep9\n",
            "   ‚úÖ Including: scen3_5rep1\n",
            "   ‚ùå Filtering out: scen3_5rep10\n",
            "   ‚úÖ Including: scen3_5rep2\n",
            "   ‚úÖ Including: scen3_5rep3\n",
            "   ‚ùå Filtering out: scen3_5rep4\n",
            "   ‚ùå Filtering out: scen3_5rep5\n",
            "   ‚ùå Filtering out: scen3_5rep6\n",
            "   ‚ùå Filtering out: scen3_5rep7\n",
            "   ‚ùå Filtering out: scen3_5rep8\n",
            "   ‚ùå Filtering out: scen3_5rep9\n",
            "   ‚úÖ Including: scen3_6rep1\n",
            "   ‚ùå Filtering out: scen3_6rep10\n",
            "   ‚úÖ Including: scen3_6rep2\n",
            "   ‚úÖ Including: scen3_6rep3\n",
            "   ‚ùå Filtering out: scen3_6rep4\n",
            "   ‚ùå Filtering out: scen3_6rep5\n",
            "   ‚ùå Filtering out: scen3_6rep6\n",
            "   ‚ùå Filtering out: scen3_6rep7\n",
            "   ‚ùå Filtering out: scen3_6rep8\n",
            "   ‚ùå Filtering out: scen3_6rep9\n",
            "   ‚úÖ Including: scen3_7rep1\n",
            "   ‚ùå Filtering out: scen3_7rep10\n",
            "   ‚úÖ Including: scen3_7rep2\n",
            "   ‚úÖ Including: scen3_7rep3\n",
            "   ‚ùå Filtering out: scen3_7rep4\n",
            "   ‚ùå Filtering out: scen3_7rep5\n",
            "   ‚ùå Filtering out: scen3_7rep6\n",
            "   ‚ùå Filtering out: scen3_7rep7\n",
            "   ‚ùå Filtering out: scen3_7rep8\n",
            "   ‚ùå Filtering out: scen3_7rep9\n",
            "   ‚úÖ Including: scen3_8rep1\n",
            "   ‚ùå Filtering out: scen3_8rep10\n",
            "   ‚úÖ Including: scen3_8rep2\n",
            "   ‚úÖ Including: scen3_8rep3\n",
            "   ‚ùå Filtering out: scen3_8rep4\n",
            "   ‚ùå Filtering out: scen3_8rep5\n",
            "   ‚ùå Filtering out: scen3_8rep6\n",
            "   ‚ùå Filtering out: scen3_8rep7\n",
            "   ‚ùå Filtering out: scen3_8rep8\n",
            "   ‚ùå Filtering out: scen3_8rep9\n",
            "   ‚úÖ Including: scen3_9rep1\n",
            "   ‚ùå Filtering out: scen3_9rep10\n",
            "   ‚úÖ Including: scen3_9rep2\n",
            "   ‚úÖ Including: scen3_9rep3\n",
            "   ‚ùå Filtering out: scen3_9rep4\n",
            "   ‚ùå Filtering out: scen3_9rep5\n",
            "   ‚ùå Filtering out: scen3_9rep6\n",
            "   ‚ùå Filtering out: scen3_9rep7\n",
            "   ‚ùå Filtering out: scen3_9rep8\n",
            "   ‚ùå Filtering out: scen3_9rep9\n",
            "   ‚úÖ Including: scen4_1rep1\n",
            "   ‚ùå Filtering out: scen4_1rep10\n",
            "   ‚úÖ Including: scen4_1rep2\n",
            "   ‚úÖ Including: scen4_1rep3\n",
            "   ‚ùå Filtering out: scen4_1rep4\n",
            "   ‚ùå Filtering out: scen4_1rep5\n",
            "   ‚ùå Filtering out: scen4_1rep6\n",
            "   ‚ùå Filtering out: scen4_1rep7\n",
            "   ‚ùå Filtering out: scen4_1rep8\n",
            "   ‚ùå Filtering out: scen4_1rep9\n",
            "   ‚úÖ Including: scen4_2rep1\n",
            "   ‚ùå Filtering out: scen4_2rep10\n",
            "   ‚úÖ Including: scen4_2rep2\n",
            "   ‚úÖ Including: scen4_2rep3\n",
            "   ‚ùå Filtering out: scen4_2rep4\n",
            "   ‚ùå Filtering out: scen4_2rep5\n",
            "   ‚ùå Filtering out: scen4_2rep6\n",
            "   ‚ùå Filtering out: scen4_2rep7\n",
            "   ‚ùå Filtering out: scen4_2rep8\n",
            "   ‚ùå Filtering out: scen4_2rep9\n",
            "   ‚úÖ Including: scen4_3rep1\n",
            "   ‚ùå Filtering out: scen4_3rep10\n",
            "   ‚úÖ Including: scen4_3rep2\n",
            "   ‚úÖ Including: scen4_3rep3\n",
            "   ‚ùå Filtering out: scen4_3rep4\n",
            "   ‚ùå Filtering out: scen4_3rep5\n",
            "   ‚ùå Filtering out: scen4_3rep6\n",
            "   ‚ùå Filtering out: scen4_3rep7\n",
            "   ‚ùå Filtering out: scen4_3rep8\n",
            "   ‚ùå Filtering out: scen4_3rep9\n",
            "   ‚úÖ Including: scen4_4rep1\n",
            "   ‚ùå Filtering out: scen4_4rep10\n",
            "   ‚úÖ Including: scen4_4rep2\n",
            "   ‚úÖ Including: scen4_4rep3\n",
            "   ‚ùå Filtering out: scen4_4rep4\n",
            "   ‚ùå Filtering out: scen4_4rep5\n",
            "   ‚ùå Filtering out: scen4_4rep6\n",
            "   ‚ùå Filtering out: scen4_4rep7\n",
            "   ‚ùå Filtering out: scen4_4rep8\n",
            "   ‚ùå Filtering out: scen4_4rep9\n",
            "   ‚úÖ Including: scen4_5rep1\n",
            "   ‚ùå Filtering out: scen4_5rep10\n",
            "   ‚úÖ Including: scen4_5rep2\n",
            "   ‚úÖ Including: scen4_5rep3\n",
            "   ‚ùå Filtering out: scen4_5rep4\n",
            "   ‚ùå Filtering out: scen4_5rep5\n",
            "   ‚ùå Filtering out: scen4_5rep6\n",
            "   ‚ùå Filtering out: scen4_5rep7\n",
            "   ‚ùå Filtering out: scen4_5rep8\n",
            "   ‚ùå Filtering out: scen4_5rep9\n",
            "   ‚úÖ Including: scen4_6rep1\n",
            "   ‚ùå Filtering out: scen4_6rep10\n",
            "   ‚úÖ Including: scen4_6rep2\n",
            "   ‚úÖ Including: scen4_6rep3\n",
            "   ‚ùå Filtering out: scen4_6rep4\n",
            "   ‚ùå Filtering out: scen4_6rep5\n",
            "   ‚ùå Filtering out: scen4_6rep6\n",
            "   ‚ùå Filtering out: scen4_6rep7\n",
            "   ‚ùå Filtering out: scen4_6rep8\n",
            "   ‚ùå Filtering out: scen4_6rep9\n",
            "   ‚úÖ Including: scen4_7rep1\n",
            "   ‚ùå Filtering out: scen4_7rep10\n",
            "   ‚úÖ Including: scen4_7rep2\n",
            "   ‚úÖ Including: scen4_7rep3\n",
            "   ‚ùå Filtering out: scen4_7rep4\n",
            "   ‚ùå Filtering out: scen4_7rep5\n",
            "   ‚ùå Filtering out: scen4_7rep6\n",
            "   ‚ùå Filtering out: scen4_7rep7\n",
            "   ‚ùå Filtering out: scen4_7rep8\n",
            "   ‚ùå Filtering out: scen4_7rep9\n",
            "   ‚úÖ Including: scen4_8rep1\n",
            "   ‚ùå Filtering out: scen4_8rep10\n",
            "   ‚úÖ Including: scen4_8rep2\n",
            "   ‚úÖ Including: scen4_8rep3\n",
            "   ‚ùå Filtering out: scen4_8rep4\n",
            "   ‚ùå Filtering out: scen4_8rep5\n",
            "   ‚ùå Filtering out: scen4_8rep6\n",
            "   ‚ùå Filtering out: scen4_8rep7\n",
            "   ‚ùå Filtering out: scen4_8rep8\n",
            "   ‚ùå Filtering out: scen4_8rep9\n",
            "   ‚úÖ Including: scen4_9rep1\n",
            "   ‚ùå Filtering out: scen4_9rep10\n",
            "   ‚úÖ Including: scen4_9rep2\n",
            "   ‚úÖ Including: scen4_9rep3\n",
            "   ‚ùå Filtering out: scen4_9rep4\n",
            "   ‚ùå Filtering out: scen4_9rep5\n",
            "   ‚ùå Filtering out: scen4_9rep6\n",
            "   ‚ùå Filtering out: scen4_9rep7\n",
            "   ‚ùå Filtering out: scen4_9rep8\n",
            "   ‚ùå Filtering out: scen4_9rep9\n",
            "   ‚úÖ Including: scen5_2rep1\n",
            "   ‚ùå Filtering out: scen5_2rep10\n",
            "   ‚úÖ Including: scen5_2rep2\n",
            "   ‚úÖ Including: scen5_2rep3\n",
            "   ‚ùå Filtering out: scen5_2rep4\n",
            "   ‚ùå Filtering out: scen5_2rep5\n",
            "   ‚ùå Filtering out: scen5_2rep6\n",
            "   ‚ùå Filtering out: scen5_2rep7\n",
            "   ‚ùå Filtering out: scen5_2rep8\n",
            "   ‚ùå Filtering out: scen5_2rep9\n",
            "   ‚úÖ Including: scen5_3rep1\n",
            "   ‚ùå Filtering out: scen5_3rep10\n",
            "   ‚úÖ Including: scen5_3rep2\n",
            "   ‚úÖ Including: scen5_3rep3\n",
            "   ‚ùå Filtering out: scen5_3rep4\n",
            "   ‚ùå Filtering out: scen5_3rep5\n",
            "   ‚ùå Filtering out: scen5_3rep6\n",
            "   ‚ùå Filtering out: scen5_3rep7\n",
            "   ‚ùå Filtering out: scen5_3rep8\n",
            "   ‚ùå Filtering out: scen5_3rep9\n",
            "   ‚úÖ Including: scen5_4rep1\n",
            "   ‚ùå Filtering out: scen5_4rep10\n",
            "   ‚úÖ Including: scen5_4rep2\n",
            "   ‚úÖ Including: scen5_4rep3\n",
            "   ‚ùå Filtering out: scen5_4rep4\n",
            "   ‚ùå Filtering out: scen5_4rep5\n",
            "   ‚ùå Filtering out: scen5_4rep6\n",
            "   ‚ùå Filtering out: scen5_4rep7\n",
            "   ‚ùå Filtering out: scen5_4rep8\n",
            "   ‚ùå Filtering out: scen5_4rep9\n",
            "   ‚úÖ Including: scen5_5rep1\n",
            "   ‚ùå Filtering out: scen5_5rep10\n",
            "   ‚úÖ Including: scen5_5rep2\n",
            "   ‚úÖ Including: scen5_5rep3\n",
            "   ‚ùå Filtering out: scen5_5rep4\n",
            "   ‚ùå Filtering out: scen5_5rep5\n",
            "   ‚ùå Filtering out: scen5_5rep6\n",
            "   ‚ùå Filtering out: scen5_5rep7\n",
            "   ‚ùå Filtering out: scen5_5rep8\n",
            "   ‚ùå Filtering out: scen5_5rep9\n",
            "   ‚úÖ Including: scen5_6rep1\n",
            "   ‚ùå Filtering out: scen5_6rep10\n",
            "   ‚úÖ Including: scen5_6rep2\n",
            "   ‚úÖ Including: scen5_6rep3\n",
            "   ‚ùå Filtering out: scen5_6rep4\n",
            "   ‚ùå Filtering out: scen5_6rep5\n",
            "   ‚ùå Filtering out: scen5_6rep6\n",
            "   ‚ùå Filtering out: scen5_6rep7\n",
            "   ‚ùå Filtering out: scen5_6rep8\n",
            "   ‚ùå Filtering out: scen5_6rep9\n",
            "   ‚úÖ Including: scen5_7rep1\n",
            "   ‚ùå Filtering out: scen5_7rep10\n",
            "   ‚úÖ Including: scen5_7rep2\n",
            "   ‚úÖ Including: scen5_7rep3\n",
            "   ‚ùå Filtering out: scen5_7rep4\n",
            "   ‚ùå Filtering out: scen5_7rep5\n",
            "   ‚ùå Filtering out: scen5_7rep6\n",
            "   ‚ùå Filtering out: scen5_7rep7\n",
            "   ‚ùå Filtering out: scen5_7rep8\n",
            "   ‚ùå Filtering out: scen5_7rep9\n",
            "   ‚úÖ Including: scen5_8rep1\n",
            "   ‚ùå Filtering out: scen5_8rep10\n",
            "   ‚úÖ Including: scen5_8rep2\n",
            "   ‚úÖ Including: scen5_8rep3\n",
            "   ‚ùå Filtering out: scen5_8rep4\n",
            "   ‚ùå Filtering out: scen5_8rep5\n",
            "   ‚ùå Filtering out: scen5_8rep6\n",
            "   ‚ùå Filtering out: scen5_8rep7\n",
            "   ‚ùå Filtering out: scen5_8rep8\n",
            "   ‚ùå Filtering out: scen5_8rep9\n",
            "   ‚úÖ Including: scen5_9rep1\n",
            "   ‚ùå Filtering out: scen5_9rep10\n",
            "   ‚úÖ Including: scen5_9rep2\n",
            "   ‚úÖ Including: scen5_9rep3\n",
            "   ‚ùå Filtering out: scen5_9rep4\n",
            "   ‚ùå Filtering out: scen5_9rep5\n",
            "   ‚ùå Filtering out: scen5_9rep6\n",
            "   ‚ùå Filtering out: scen5_9rep7\n",
            "   ‚ùå Filtering out: scen5_9rep8\n",
            "   ‚ùå Filtering out: scen5_9rep9\n",
            "   ‚úÖ Including: scen6_1rep1\n",
            "   ‚ùå Filtering out: scen6_1rep10\n",
            "   ‚úÖ Including: scen6_1rep2\n",
            "   ‚úÖ Including: scen6_1rep3\n",
            "   ‚ùå Filtering out: scen6_1rep4\n",
            "   ‚ùå Filtering out: scen6_1rep5\n",
            "   ‚ùå Filtering out: scen6_1rep6\n",
            "   ‚ùå Filtering out: scen6_1rep7\n",
            "   ‚ùå Filtering out: scen6_1rep8\n",
            "   ‚ùå Filtering out: scen6_1rep9\n",
            "   ‚úÖ Including: scen6_2rep1\n",
            "   ‚ùå Filtering out: scen6_2rep10\n",
            "   ‚úÖ Including: scen6_2rep2\n",
            "   ‚úÖ Including: scen6_2rep3\n",
            "   ‚ùå Filtering out: scen6_2rep4\n",
            "   ‚ùå Filtering out: scen6_2rep5\n",
            "   ‚ùå Filtering out: scen6_2rep6\n",
            "   ‚ùå Filtering out: scen6_2rep7\n",
            "   ‚ùå Filtering out: scen6_2rep8\n",
            "   ‚ùå Filtering out: scen6_2rep9\n",
            "   ‚úÖ Including: scen6_3rep1\n",
            "   ‚ùå Filtering out: scen6_3rep10\n",
            "   ‚úÖ Including: scen6_3rep2\n",
            "   ‚úÖ Including: scen6_3rep3\n",
            "   ‚ùå Filtering out: scen6_3rep4\n",
            "   ‚ùå Filtering out: scen6_3rep5\n",
            "   ‚ùå Filtering out: scen6_3rep6\n",
            "   ‚ùå Filtering out: scen6_3rep7\n",
            "   ‚ùå Filtering out: scen6_3rep8\n",
            "   ‚ùå Filtering out: scen6_3rep9\n",
            "   ‚úÖ Including: scen6_4rep1\n",
            "   ‚ùå Filtering out: scen6_4rep10\n",
            "   ‚úÖ Including: scen6_4rep2\n",
            "   ‚úÖ Including: scen6_4rep3\n",
            "   ‚ùå Filtering out: scen6_4rep4\n",
            "   ‚ùå Filtering out: scen6_4rep5\n",
            "   ‚ùå Filtering out: scen6_4rep6\n",
            "   ‚ùå Filtering out: scen6_4rep7\n",
            "   ‚ùå Filtering out: scen6_4rep8\n",
            "   ‚ùå Filtering out: scen6_4rep9\n",
            "   ‚úÖ Including: scen6_5rep1\n",
            "   ‚ùå Filtering out: scen6_5rep10\n",
            "   ‚úÖ Including: scen6_5rep2\n",
            "   ‚úÖ Including: scen6_5rep3\n",
            "   ‚ùå Filtering out: scen6_5rep4\n",
            "   ‚ùå Filtering out: scen6_5rep5\n",
            "   ‚ùå Filtering out: scen6_5rep6\n",
            "   ‚ùå Filtering out: scen6_5rep7\n",
            "   ‚ùå Filtering out: scen6_5rep8\n",
            "   ‚ùå Filtering out: scen6_5rep9\n",
            "   ‚úÖ Including: scen6_6rep1\n",
            "   ‚ùå Filtering out: scen6_6rep10\n",
            "   ‚úÖ Including: scen6_6rep2\n",
            "   ‚úÖ Including: scen6_6rep3\n",
            "   ‚ùå Filtering out: scen6_6rep4\n",
            "   ‚ùå Filtering out: scen6_6rep5\n",
            "   ‚ùå Filtering out: scen6_6rep6\n",
            "   ‚ùå Filtering out: scen6_6rep7\n",
            "   ‚ùå Filtering out: scen6_6rep8\n",
            "   ‚ùå Filtering out: scen6_6rep9\n",
            "   ‚úÖ Including: scen6_7rep1\n",
            "   ‚ùå Filtering out: scen6_7rep10\n",
            "   ‚úÖ Including: scen6_7rep2\n",
            "   ‚úÖ Including: scen6_7rep3\n",
            "   ‚ùå Filtering out: scen6_7rep4\n",
            "   ‚ùå Filtering out: scen6_7rep5\n",
            "   ‚ùå Filtering out: scen6_7rep6\n",
            "   ‚ùå Filtering out: scen6_7rep7\n",
            "   ‚ùå Filtering out: scen6_7rep8\n",
            "   ‚ùå Filtering out: scen6_7rep9\n",
            "   ‚úÖ Including: scen6_8rep1\n",
            "   ‚ùå Filtering out: scen6_8rep10\n",
            "   ‚úÖ Including: scen6_8rep2\n",
            "   ‚úÖ Including: scen6_8rep3\n",
            "   ‚ùå Filtering out: scen6_8rep4\n",
            "   ‚ùå Filtering out: scen6_8rep5\n",
            "   ‚ùå Filtering out: scen6_8rep6\n",
            "   ‚ùå Filtering out: scen6_8rep7\n",
            "   ‚ùå Filtering out: scen6_8rep8\n",
            "   ‚ùå Filtering out: scen6_8rep9\n",
            "   ‚úÖ Including: scen7_1rep1\n",
            "   ‚ùå Filtering out: scen7_1rep10\n",
            "   ‚úÖ Including: scen7_1rep2\n",
            "   ‚úÖ Including: scen7_1rep3\n",
            "   ‚ùå Filtering out: scen7_1rep4\n",
            "   ‚ùå Filtering out: scen7_1rep5\n",
            "   ‚ùå Filtering out: scen7_1rep6\n",
            "   ‚ùå Filtering out: scen7_1rep7\n",
            "   ‚ùå Filtering out: scen7_1rep8\n",
            "   ‚ùå Filtering out: scen7_1rep9\n",
            "   ‚úÖ Including: scen7_2rep1\n",
            "   ‚ùå Filtering out: scen7_2rep10\n",
            "   ‚úÖ Including: scen7_2rep2\n",
            "   ‚úÖ Including: scen7_2rep3\n",
            "   ‚ùå Filtering out: scen7_2rep4\n",
            "   ‚ùå Filtering out: scen7_2rep5\n",
            "   ‚ùå Filtering out: scen7_2rep6\n",
            "   ‚ùå Filtering out: scen7_2rep7\n",
            "   ‚ùå Filtering out: scen7_2rep8\n",
            "   ‚ùå Filtering out: scen7_2rep9\n",
            "   ‚úÖ Including: scen7_3rep1\n",
            "   ‚ùå Filtering out: scen7_3rep10\n",
            "   ‚úÖ Including: scen7_3rep2\n",
            "   ‚úÖ Including: scen7_3rep3\n",
            "   ‚ùå Filtering out: scen7_3rep4\n",
            "   ‚ùå Filtering out: scen7_3rep5\n",
            "   ‚ùå Filtering out: scen7_3rep6\n",
            "   ‚ùå Filtering out: scen7_3rep7\n",
            "   ‚ùå Filtering out: scen7_3rep8\n",
            "   ‚ùå Filtering out: scen7_3rep9\n",
            "   ‚úÖ Including: scen7_4rep1\n",
            "   ‚ùå Filtering out: scen7_4rep10\n",
            "   ‚úÖ Including: scen7_4rep2\n",
            "   ‚úÖ Including: scen7_4rep3\n",
            "   ‚ùå Filtering out: scen7_4rep4\n",
            "   ‚ùå Filtering out: scen7_4rep5\n",
            "   ‚ùå Filtering out: scen7_4rep6\n",
            "   ‚ùå Filtering out: scen7_4rep7\n",
            "   ‚ùå Filtering out: scen7_4rep8\n",
            "   ‚ùå Filtering out: scen7_4rep9\n",
            "   ‚úÖ Including: scen7_5rep1\n",
            "   ‚ùå Filtering out: scen7_5rep10\n",
            "   ‚úÖ Including: scen7_5rep2\n",
            "   ‚úÖ Including: scen7_5rep3\n",
            "   ‚ùå Filtering out: scen7_5rep4\n",
            "   ‚ùå Filtering out: scen7_5rep5\n",
            "   ‚ùå Filtering out: scen7_5rep6\n",
            "   ‚ùå Filtering out: scen7_5rep7\n",
            "   ‚ùå Filtering out: scen7_5rep8\n",
            "   ‚ùå Filtering out: scen7_5rep9\n",
            "   ‚úÖ Including: scen7_6rep1\n",
            "   ‚ùå Filtering out: scen7_6rep10\n",
            "   ‚úÖ Including: scen7_6rep2\n",
            "   ‚úÖ Including: scen7_6rep3\n",
            "   ‚ùå Filtering out: scen7_6rep4\n",
            "   ‚ùå Filtering out: scen7_6rep5\n",
            "   ‚ùå Filtering out: scen7_6rep6\n",
            "   ‚ùå Filtering out: scen7_6rep7\n",
            "   ‚ùå Filtering out: scen7_6rep8\n",
            "   ‚ùå Filtering out: scen7_6rep9\n",
            "   ‚úÖ Including: scen7_7rep1\n",
            "   ‚ùå Filtering out: scen7_7rep10\n",
            "   ‚úÖ Including: scen7_7rep2\n",
            "   ‚úÖ Including: scen7_7rep3\n",
            "   ‚ùå Filtering out: scen7_7rep4\n",
            "   ‚ùå Filtering out: scen7_7rep5\n",
            "   ‚ùå Filtering out: scen7_7rep6\n",
            "   ‚ùå Filtering out: scen7_7rep7\n",
            "   ‚ùå Filtering out: scen7_7rep8\n",
            "   ‚ùå Filtering out: scen7_7rep9\n",
            "   ‚úÖ Including: scen7_8rep1\n",
            "   ‚ùå Filtering out: scen7_8rep10\n",
            "   ‚úÖ Including: scen7_8rep2\n",
            "   ‚úÖ Including: scen7_8rep3\n",
            "   ‚ùå Filtering out: scen7_8rep4\n",
            "   ‚ùå Filtering out: scen7_8rep5\n",
            "   ‚ùå Filtering out: scen7_8rep6\n",
            "   ‚ùå Filtering out: scen7_8rep7\n",
            "   ‚ùå Filtering out: scen7_8rep8\n",
            "   ‚ùå Filtering out: scen7_8rep9\n",
            "   ‚úÖ Including: scen8_2rep1\n",
            "   ‚ùå Filtering out: scen8_2rep10\n",
            "   ‚úÖ Including: scen8_2rep2\n",
            "   ‚úÖ Including: scen8_2rep3\n",
            "   ‚ùå Filtering out: scen8_2rep4\n",
            "   ‚ùå Filtering out: scen8_2rep5\n",
            "   ‚ùå Filtering out: scen8_2rep6\n",
            "   ‚ùå Filtering out: scen8_2rep7\n",
            "   ‚ùå Filtering out: scen8_2rep8\n",
            "   ‚ùå Filtering out: scen8_2rep9\n",
            "   ‚úÖ Including: scen8_3rep1\n",
            "   ‚ùå Filtering out: scen8_3rep10\n",
            "   ‚úÖ Including: scen8_3rep2\n",
            "   ‚úÖ Including: scen8_3rep3\n",
            "   ‚ùå Filtering out: scen8_3rep4\n",
            "   ‚ùå Filtering out: scen8_3rep5\n",
            "   ‚ùå Filtering out: scen8_3rep6\n",
            "   ‚ùå Filtering out: scen8_3rep7\n",
            "   ‚ùå Filtering out: scen8_3rep8\n",
            "   ‚ùå Filtering out: scen8_3rep9\n",
            "   ‚úÖ Including: scen8_4rep1\n",
            "   ‚ùå Filtering out: scen8_4rep10\n",
            "   ‚úÖ Including: scen8_4rep2\n",
            "   ‚úÖ Including: scen8_4rep3\n",
            "   ‚ùå Filtering out: scen8_4rep4\n",
            "   ‚ùå Filtering out: scen8_4rep5\n",
            "   ‚ùå Filtering out: scen8_4rep6\n",
            "   ‚ùå Filtering out: scen8_4rep7\n",
            "   ‚ùå Filtering out: scen8_4rep8\n",
            "   ‚ùå Filtering out: scen8_4rep9\n",
            "   ‚úÖ Including: scen8_5rep1\n",
            "   ‚ùå Filtering out: scen8_5rep10\n",
            "   ‚úÖ Including: scen8_5rep2\n",
            "   ‚úÖ Including: scen8_5rep3\n",
            "   ‚ùå Filtering out: scen8_5rep4\n",
            "   ‚ùå Filtering out: scen8_5rep5\n",
            "   ‚ùå Filtering out: scen8_5rep6\n",
            "   ‚ùå Filtering out: scen8_5rep7\n",
            "   ‚ùå Filtering out: scen8_5rep8\n",
            "   ‚ùå Filtering out: scen8_5rep9\n",
            "   ‚úÖ Including: scen8_6rep1\n",
            "   ‚ùå Filtering out: scen8_6rep10\n",
            "   ‚úÖ Including: scen8_6rep2\n",
            "   ‚úÖ Including: scen8_6rep3\n",
            "   ‚ùå Filtering out: scen8_6rep4\n",
            "   ‚ùå Filtering out: scen8_6rep5\n",
            "   ‚ùå Filtering out: scen8_6rep6\n",
            "   ‚ùå Filtering out: scen8_6rep7\n",
            "   ‚ùå Filtering out: scen8_6rep8\n",
            "   ‚ùå Filtering out: scen8_6rep9\n",
            "   ‚úÖ Including: scen8_7rep1\n",
            "   ‚ùå Filtering out: scen8_7rep10\n",
            "   ‚úÖ Including: scen8_7rep2\n",
            "   ‚úÖ Including: scen8_7rep3\n",
            "   ‚ùå Filtering out: scen8_7rep4\n",
            "   ‚ùå Filtering out: scen8_7rep5\n",
            "   ‚ùå Filtering out: scen8_7rep6\n",
            "   ‚ùå Filtering out: scen8_7rep7\n",
            "   ‚ùå Filtering out: scen8_7rep8\n",
            "   ‚ùå Filtering out: scen8_7rep9\n",
            "   ‚úÖ Including: scen8_8rep1\n",
            "   ‚ùå Filtering out: scen8_8rep10\n",
            "   ‚úÖ Including: scen8_8rep2\n",
            "   ‚úÖ Including: scen8_8rep3\n",
            "   ‚ùå Filtering out: scen8_8rep4\n",
            "   ‚ùå Filtering out: scen8_8rep5\n",
            "   ‚ùå Filtering out: scen8_8rep6\n",
            "   ‚ùå Filtering out: scen8_8rep7\n",
            "   ‚ùå Filtering out: scen8_8rep8\n",
            "   ‚ùå Filtering out: scen8_8rep9\n",
            "   ‚úÖ Including: scen9_1rep1\n",
            "   ‚ùå Filtering out: scen9_1rep10\n",
            "   ‚úÖ Including: scen9_1rep2\n",
            "   ‚úÖ Including: scen9_1rep3\n",
            "   ‚ùå Filtering out: scen9_1rep4\n",
            "   ‚ùå Filtering out: scen9_1rep5\n",
            "   ‚ùå Filtering out: scen9_1rep6\n",
            "   ‚ùå Filtering out: scen9_1rep7\n",
            "   ‚ùå Filtering out: scen9_1rep8\n",
            "   ‚ùå Filtering out: scen9_1rep9\n",
            "   ‚úÖ Including: scen9_2rep1\n",
            "   ‚ùå Filtering out: scen9_2rep10\n",
            "   ‚úÖ Including: scen9_2rep2\n",
            "   ‚úÖ Including: scen9_2rep3\n",
            "   ‚ùå Filtering out: scen9_2rep4\n",
            "   ‚ùå Filtering out: scen9_2rep5\n",
            "   ‚ùå Filtering out: scen9_2rep6\n",
            "   ‚ùå Filtering out: scen9_2rep7\n",
            "   ‚ùå Filtering out: scen9_2rep8\n",
            "   ‚ùå Filtering out: scen9_2rep9\n",
            "   ‚úÖ Including: scen9_3rep1\n",
            "   ‚ùå Filtering out: scen9_3rep10\n",
            "   ‚úÖ Including: scen9_3rep2\n",
            "   ‚úÖ Including: scen9_3rep3\n",
            "   ‚ùå Filtering out: scen9_3rep4\n",
            "   ‚ùå Filtering out: scen9_3rep5\n",
            "   ‚ùå Filtering out: scen9_3rep6\n",
            "   ‚ùå Filtering out: scen9_3rep7\n",
            "   ‚ùå Filtering out: scen9_3rep8\n",
            "   ‚ùå Filtering out: scen9_3rep9\n",
            "   ‚úÖ Including: scen9_4rep1\n",
            "   ‚ùå Filtering out: scen9_4rep10\n",
            "   ‚úÖ Including: scen9_4rep2\n",
            "   ‚úÖ Including: scen9_4rep3\n",
            "   ‚ùå Filtering out: scen9_4rep4\n",
            "   ‚ùå Filtering out: scen9_4rep5\n",
            "   ‚ùå Filtering out: scen9_4rep6\n",
            "   ‚ùå Filtering out: scen9_4rep7\n",
            "   ‚ùå Filtering out: scen9_4rep8\n",
            "   ‚ùå Filtering out: scen9_4rep9\n",
            "   ‚úÖ Including: scen9_5rep1\n",
            "   ‚ùå Filtering out: scen9_5rep10\n",
            "   ‚úÖ Including: scen9_5rep2\n",
            "   ‚úÖ Including: scen9_5rep3\n",
            "   ‚ùå Filtering out: scen9_5rep4\n",
            "   ‚ùå Filtering out: scen9_5rep5\n",
            "   ‚ùå Filtering out: scen9_5rep6\n",
            "   ‚ùå Filtering out: scen9_5rep7\n",
            "   ‚ùå Filtering out: scen9_5rep8\n",
            "   ‚ùå Filtering out: scen9_5rep9\n",
            "   ‚úÖ Including: scen9_6rep1\n",
            "   ‚ùå Filtering out: scen9_6rep10\n",
            "   ‚úÖ Including: scen9_6rep2\n",
            "   ‚úÖ Including: scen9_6rep3\n",
            "   ‚ùå Filtering out: scen9_6rep4\n",
            "   ‚ùå Filtering out: scen9_6rep5\n",
            "   ‚ùå Filtering out: scen9_6rep6\n",
            "   ‚ùå Filtering out: scen9_6rep7\n",
            "   ‚ùå Filtering out: scen9_6rep8\n",
            "   ‚ùå Filtering out: scen9_6rep9\n",
            "   ‚úÖ Including: scen9_7rep1\n",
            "   ‚ùå Filtering out: scen9_7rep10\n",
            "   ‚úÖ Including: scen9_7rep2\n",
            "   ‚úÖ Including: scen9_7rep3\n",
            "   ‚ùå Filtering out: scen9_7rep4\n",
            "   ‚ùå Filtering out: scen9_7rep5\n",
            "   ‚ùå Filtering out: scen9_7rep6\n",
            "   ‚ùå Filtering out: scen9_7rep7\n",
            "   ‚ùå Filtering out: scen9_7rep8\n",
            "   ‚ùå Filtering out: scen9_7rep9\n",
            "\n",
            "üìä Filtering results:\n",
            "   ‚úÖ Included scenarios: 408\n",
            "   ‚ùå Filtered out scenarios: 952\n",
            "   üìã Filtered out scenarios:\n",
            "      - scen10_1rep10\n",
            "      - scen10_1rep4\n",
            "      - scen10_1rep5\n",
            "      - scen10_1rep6\n",
            "      - scen10_1rep7\n",
            "      - scen10_1rep8\n",
            "      - scen10_1rep9\n",
            "      - scen10_2rep10\n",
            "      - scen10_2rep4\n",
            "      - scen10_2rep5\n",
            "      ... and 942 more\n",
            "\n",
            "üìÑ Scanning 408 filtered folders for CSV files...\n",
            "   üìä scen10_1rep1: 61 CSV files\n",
            "   üìä scen10_1rep2: 61 CSV files\n",
            "   üìä scen10_1rep3: 61 CSV files\n",
            "   üìä scen10_2rep1: 61 CSV files\n",
            "   üìä scen10_2rep2: 61 CSV files\n",
            "   üìä scen10_2rep3: 61 CSV files\n",
            "   üìä scen10_3rep1: 61 CSV files\n",
            "   üìä scen10_3rep2: 61 CSV files\n",
            "   üìä scen10_3rep3: 61 CSV files\n",
            "   üìä scen10_4rep1: 61 CSV files\n",
            "   üìä scen10_4rep2: 61 CSV files\n",
            "   üìä scen10_4rep3: 61 CSV files\n",
            "   üìä scen10_5rep1: 61 CSV files\n",
            "   üìä scen10_5rep2: 61 CSV files\n",
            "   üìä scen10_5rep3: 61 CSV files\n",
            "   üìä scen10_6rep1: 61 CSV files\n",
            "   üìä scen10_6rep2: 61 CSV files\n",
            "   üìä scen10_6rep3: 61 CSV files\n",
            "   üìä scen10_7rep1: 61 CSV files\n",
            "   üìä scen10_7rep2: 61 CSV files\n",
            "   üìä scen10_7rep3: 61 CSV files\n",
            "   üìä scen11_2rep1: 61 CSV files\n",
            "   üìä scen11_2rep2: 61 CSV files\n",
            "   üìä scen11_2rep3: 61 CSV files\n",
            "   üìä scen11_3rep1: 61 CSV files\n",
            "   üìä scen11_3rep2: 61 CSV files\n",
            "   üìä scen11_3rep3: 61 CSV files\n",
            "   üìä scen11_4rep1: 61 CSV files\n",
            "   üìä scen11_4rep2: 61 CSV files\n",
            "   üìä scen11_4rep3: 61 CSV files\n",
            "   üìä scen11_5rep1: 61 CSV files\n",
            "   üìä scen11_5rep2: 61 CSV files\n",
            "   üìä scen11_5rep3: 61 CSV files\n",
            "   üìä scen11_6rep1: 61 CSV files\n",
            "   üìä scen11_6rep2: 61 CSV files\n",
            "   üìä scen11_6rep3: 61 CSV files\n",
            "   üìä scen11_7rep1: 61 CSV files\n",
            "   üìä scen11_7rep2: 61 CSV files\n",
            "   üìä scen11_7rep3: 61 CSV files\n",
            "   üìä scen12_1rep1: 61 CSV files\n",
            "   üìä scen12_1rep2: 61 CSV files\n",
            "   üìä scen12_1rep3: 61 CSV files\n",
            "   üìä scen12_2rep1: 61 CSV files\n",
            "   üìä scen12_2rep2: 61 CSV files\n",
            "   üìä scen12_2rep3: 61 CSV files\n",
            "   üìä scen12_3rep1: 61 CSV files\n",
            "   üìä scen12_3rep2: 61 CSV files\n",
            "   üìä scen12_3rep3: 61 CSV files\n",
            "   üìä scen12_4rep1: 61 CSV files\n",
            "   üìä scen12_4rep2: 61 CSV files\n",
            "   üìä scen12_4rep3: 61 CSV files\n",
            "   üìä scen12_5rep1: 61 CSV files\n",
            "   üìä scen12_5rep2: 61 CSV files\n",
            "   üìä scen12_5rep3: 61 CSV files\n",
            "   üìä scen12_6rep1: 61 CSV files\n",
            "   üìä scen12_6rep2: 61 CSV files\n",
            "   üìä scen12_6rep3: 61 CSV files\n",
            "   üìä scen13_1rep1: 61 CSV files\n",
            "   üìä scen13_1rep2: 61 CSV files\n",
            "   üìä scen13_1rep3: 61 CSV files\n",
            "   üìä scen13_2rep1: 61 CSV files\n",
            "   üìä scen13_2rep2: 61 CSV files\n",
            "   üìä scen13_2rep3: 61 CSV files\n",
            "   üìä scen13_3rep1: 61 CSV files\n",
            "   üìä scen13_3rep2: 61 CSV files\n",
            "   üìä scen13_3rep3: 61 CSV files\n",
            "   üìä scen13_4rep1: 61 CSV files\n",
            "   üìä scen13_4rep2: 61 CSV files\n",
            "   üìä scen13_4rep3: 61 CSV files\n",
            "   üìä scen13_5rep1: 61 CSV files\n",
            "   üìä scen13_5rep2: 61 CSV files\n",
            "   üìä scen13_5rep3: 61 CSV files\n",
            "   üìä scen13_6rep1: 61 CSV files\n",
            "   üìä scen13_6rep2: 61 CSV files\n",
            "   üìä scen13_6rep3: 61 CSV files\n",
            "   üìä scen14_2rep1: 61 CSV files\n",
            "   üìä scen14_2rep2: 61 CSV files\n",
            "   üìä scen14_2rep3: 61 CSV files\n",
            "   üìä scen14_3rep1: 61 CSV files\n",
            "   üìä scen14_3rep2: 61 CSV files\n",
            "   üìä scen14_3rep3: 61 CSV files\n",
            "   üìä scen14_4rep1: 61 CSV files\n",
            "   üìä scen14_4rep2: 61 CSV files\n",
            "   üìä scen14_4rep3: 61 CSV files\n",
            "   üìä scen14_5rep1: 61 CSV files\n",
            "   üìä scen14_5rep2: 61 CSV files\n",
            "   üìä scen14_5rep3: 61 CSV files\n",
            "   üìä scen14_6rep1: 61 CSV files\n",
            "   üìä scen14_6rep2: 61 CSV files\n",
            "   üìä scen14_6rep3: 61 CSV files\n",
            "   üìä scen15_1rep1: 61 CSV files\n",
            "   üìä scen15_1rep2: 61 CSV files\n",
            "   üìä scen15_1rep3: 61 CSV files\n",
            "   üìä scen15_2rep1: 61 CSV files\n",
            "   üìä scen15_2rep2: 61 CSV files\n",
            "   üìä scen15_2rep3: 61 CSV files\n",
            "   üìä scen15_3rep1: 61 CSV files\n",
            "   üìä scen15_3rep2: 61 CSV files\n",
            "   üìä scen15_3rep3: 61 CSV files\n",
            "   üìä scen15_4rep1: 61 CSV files\n",
            "   üìä scen15_4rep2: 61 CSV files\n",
            "   üìä scen15_4rep3: 61 CSV files\n",
            "   üìä scen15_5rep1: 61 CSV files\n",
            "   üìä scen15_5rep2: 61 CSV files\n",
            "   üìä scen15_5rep3: 61 CSV files\n",
            "   üìä scen16_1rep1: 61 CSV files\n",
            "   üìä scen16_1rep2: 61 CSV files\n",
            "   üìä scen16_1rep3: 61 CSV files\n",
            "   üìä scen16_2rep1: 61 CSV files\n",
            "   üìä scen16_2rep2: 61 CSV files\n",
            "   üìä scen16_2rep3: 61 CSV files\n",
            "   üìä scen16_3rep1: 61 CSV files\n",
            "   üìä scen16_3rep2: 61 CSV files\n",
            "   üìä scen16_3rep3: 61 CSV files\n",
            "   üìä scen16_4rep1: 61 CSV files\n",
            "   üìä scen16_4rep2: 61 CSV files\n",
            "   üìä scen16_4rep3: 61 CSV files\n",
            "   üìä scen16_5rep1: 61 CSV files\n",
            "   üìä scen16_5rep2: 61 CSV files\n",
            "   üìä scen16_5rep3: 61 CSV files\n",
            "   üìä scen17_2rep1: 61 CSV files\n",
            "   üìä scen17_2rep2: 61 CSV files\n",
            "   üìä scen17_2rep3: 61 CSV files\n",
            "   üìä scen17_3rep1: 61 CSV files\n",
            "   üìä scen17_3rep2: 61 CSV files\n",
            "   üìä scen17_3rep3: 61 CSV files\n",
            "   üìä scen17_4rep1: 61 CSV files\n",
            "   üìä scen17_4rep2: 61 CSV files\n",
            "   üìä scen17_4rep3: 61 CSV files\n",
            "   üìä scen17_5rep1: 61 CSV files\n",
            "   üìä scen17_5rep2: 61 CSV files\n",
            "   üìä scen17_5rep3: 61 CSV files\n",
            "   üìä scen18_1rep1: 61 CSV files\n",
            "   üìä scen18_1rep2: 61 CSV files\n",
            "   üìä scen18_1rep3: 61 CSV files\n",
            "   üìä scen18_2rep1: 61 CSV files\n",
            "   üìä scen18_2rep2: 61 CSV files\n",
            "   üìä scen18_2rep3: 61 CSV files\n",
            "   üìä scen18_3rep1: 61 CSV files\n",
            "   üìä scen18_3rep2: 61 CSV files\n",
            "   üìä scen18_3rep3: 61 CSV files\n",
            "   üìä scen18_4rep1: 61 CSV files\n",
            "   üìä scen18_4rep2: 61 CSV files\n",
            "   üìä scen18_4rep3: 61 CSV files\n",
            "   üìä scen19_1rep1: 61 CSV files\n",
            "   üìä scen19_1rep2: 61 CSV files\n",
            "   üìä scen19_1rep3: 61 CSV files\n",
            "   üìä scen19_2rep1: 61 CSV files\n",
            "   üìä scen19_2rep2: 61 CSV files\n",
            "   üìä scen19_2rep3: 61 CSV files\n",
            "   üìä scen19_3rep1: 61 CSV files\n",
            "   üìä scen19_3rep2: 61 CSV files\n",
            "   üìä scen19_3rep3: 61 CSV files\n",
            "   üìä scen19_4rep1: 61 CSV files\n",
            "   üìä scen19_4rep2: 61 CSV files\n",
            "   üìä scen19_4rep3: 61 CSV files\n",
            "   üìä scen19_5rep1: 61 CSV files\n",
            "   üìä scen19_5rep2: 61 CSV files\n",
            "   üìä scen19_5rep3: 61 CSV files\n",
            "   üìä scen1_10rep1: 61 CSV files\n",
            "   üìä scen1_10rep2: 61 CSV files\n",
            "   üìä scen1_10rep3: 61 CSV files\n",
            "   üìä scen1_1rep1: 61 CSV files\n",
            "   üìä scen1_1rep2: 61 CSV files\n",
            "   üìä scen1_1rep3: 61 CSV files\n",
            "   üìä scen1_2rep1: 61 CSV files\n",
            "   üìä scen1_2rep2: 61 CSV files\n",
            "   üìä scen1_2rep3: 61 CSV files\n",
            "   üìä scen1_3rep1: 61 CSV files\n",
            "   üìä scen1_3rep2: 61 CSV files\n",
            "   üìä scen1_3rep3: 61 CSV files\n",
            "   üìä scen1_4rep1: 61 CSV files\n",
            "   üìä scen1_4rep2: 61 CSV files\n",
            "   üìä scen1_4rep3: 61 CSV files\n",
            "   üìä scen1_5rep1: 61 CSV files\n",
            "   üìä scen1_5rep2: 61 CSV files\n",
            "   üìä scen1_5rep3: 61 CSV files\n",
            "   üìä scen1_6rep1: 61 CSV files\n",
            "   üìä scen1_6rep2: 61 CSV files\n",
            "   üìä scen1_6rep3: 61 CSV files\n",
            "   üìä scen1_7rep1: 61 CSV files\n",
            "   üìä scen1_7rep2: 61 CSV files\n",
            "   üìä scen1_7rep3: 61 CSV files\n",
            "   üìä scen1_8rep1: 61 CSV files\n",
            "   üìä scen1_8rep2: 61 CSV files\n",
            "   üìä scen1_8rep3: 61 CSV files\n",
            "   üìä scen1_9rep1: 61 CSV files\n",
            "   üìä scen1_9rep2: 61 CSV files\n",
            "   üìä scen1_9rep3: 61 CSV files\n",
            "   üìä scen20_2rep1: 61 CSV files\n",
            "   üìä scen20_2rep2: 61 CSV files\n",
            "   üìä scen20_2rep3: 61 CSV files\n",
            "   üìä scen20_3rep1: 61 CSV files\n",
            "   üìä scen20_3rep2: 61 CSV files\n",
            "   üìä scen20_3rep3: 61 CSV files\n",
            "   üìä scen20_4rep1: 61 CSV files\n",
            "   üìä scen20_4rep2: 61 CSV files\n",
            "   üìä scen20_4rep3: 61 CSV files\n",
            "   üìä scen20_5rep1: 61 CSV files\n",
            "   üìä scen20_5rep2: 61 CSV files\n",
            "   üìä scen20_5rep3: 61 CSV files\n",
            "   üìä scen21_1rep1: 61 CSV files\n",
            "   üìä scen21_1rep2: 61 CSV files\n",
            "   üìä scen21_1rep3: 61 CSV files\n",
            "   üìä scen21_2rep1: 61 CSV files\n",
            "   üìä scen21_2rep2: 61 CSV files\n",
            "   üìä scen21_2rep3: 61 CSV files\n",
            "   üìä scen21_3rep1: 61 CSV files\n",
            "   üìä scen21_3rep2: 61 CSV files\n",
            "   üìä scen21_3rep3: 61 CSV files\n",
            "   üìä scen21_4rep1: 61 CSV files\n",
            "   üìä scen21_4rep2: 61 CSV files\n",
            "   üìä scen21_4rep3: 61 CSV files\n",
            "   üìä scen2_10rep1: 61 CSV files\n",
            "   üìä scen2_10rep2: 61 CSV files\n",
            "   üìä scen2_10rep3: 61 CSV files\n",
            "   üìä scen2_2rep1: 61 CSV files\n",
            "   üìä scen2_2rep2: 61 CSV files\n",
            "   üìä scen2_2rep3: 61 CSV files\n",
            "   üìä scen2_3rep1: 61 CSV files\n",
            "   üìä scen2_3rep2: 61 CSV files\n",
            "   üìä scen2_3rep3: 61 CSV files\n",
            "   üìä scen2_4rep1: 61 CSV files\n",
            "   üìä scen2_4rep2: 61 CSV files\n",
            "   üìä scen2_4rep3: 61 CSV files\n",
            "   üìä scen2_5rep1: 61 CSV files\n",
            "   üìä scen2_5rep2: 61 CSV files\n",
            "   üìä scen2_5rep3: 61 CSV files\n",
            "   üìä scen2_6rep1: 61 CSV files\n",
            "   üìä scen2_6rep2: 61 CSV files\n",
            "   üìä scen2_6rep3: 61 CSV files\n",
            "   üìä scen2_7rep1: 61 CSV files\n",
            "   üìä scen2_7rep2: 61 CSV files\n",
            "   üìä scen2_7rep3: 61 CSV files\n",
            "   üìä scen2_8rep1: 61 CSV files\n",
            "   üìä scen2_8rep2: 61 CSV files\n",
            "   üìä scen2_8rep3: 61 CSV files\n",
            "   üìä scen2_9rep1: 61 CSV files\n",
            "   üìä scen2_9rep2: 61 CSV files\n",
            "   üìä scen2_9rep3: 61 CSV files\n",
            "   üìä scen3_1rep1: 61 CSV files\n",
            "   üìä scen3_1rep2: 61 CSV files\n",
            "   üìä scen3_1rep3: 61 CSV files\n",
            "   üìä scen3_2rep1: 61 CSV files\n",
            "   üìä scen3_2rep2: 61 CSV files\n",
            "   üìä scen3_2rep3: 61 CSV files\n",
            "   üìä scen3_3rep1: 61 CSV files\n",
            "   üìä scen3_3rep2: 61 CSV files\n",
            "   üìä scen3_3rep3: 61 CSV files\n",
            "   üìä scen3_4rep1: 61 CSV files\n",
            "   üìä scen3_4rep2: 61 CSV files\n",
            "   üìä scen3_4rep3: 61 CSV files\n",
            "   üìä scen3_5rep1: 61 CSV files\n",
            "   üìä scen3_5rep2: 61 CSV files\n",
            "   üìä scen3_5rep3: 61 CSV files\n",
            "   üìä scen3_6rep1: 61 CSV files\n",
            "   üìä scen3_6rep2: 61 CSV files\n",
            "   üìä scen3_6rep3: 61 CSV files\n",
            "   üìä scen3_7rep1: 61 CSV files\n",
            "   üìä scen3_7rep2: 61 CSV files\n",
            "   üìä scen3_7rep3: 61 CSV files\n",
            "   üìä scen3_8rep1: 61 CSV files\n",
            "   üìä scen3_8rep2: 61 CSV files\n",
            "   üìä scen3_8rep3: 61 CSV files\n",
            "   üìä scen3_9rep1: 61 CSV files\n",
            "   üìä scen3_9rep2: 61 CSV files\n",
            "   üìä scen3_9rep3: 61 CSV files\n",
            "   üìä scen4_1rep1: 61 CSV files\n",
            "   üìä scen4_1rep2: 61 CSV files\n",
            "   üìä scen4_1rep3: 61 CSV files\n",
            "   üìä scen4_2rep1: 61 CSV files\n",
            "   üìä scen4_2rep2: 61 CSV files\n",
            "   üìä scen4_2rep3: 61 CSV files\n",
            "   üìä scen4_3rep1: 61 CSV files\n",
            "   üìä scen4_3rep2: 61 CSV files\n",
            "   üìä scen4_3rep3: 61 CSV files\n",
            "   üìä scen4_4rep1: 61 CSV files\n",
            "   üìä scen4_4rep2: 61 CSV files\n",
            "   üìä scen4_4rep3: 61 CSV files\n",
            "   üìä scen4_5rep1: 61 CSV files\n",
            "   üìä scen4_5rep2: 61 CSV files\n",
            "   üìä scen4_5rep3: 61 CSV files\n",
            "   üìä scen4_6rep1: 61 CSV files\n",
            "   üìä scen4_6rep2: 61 CSV files\n",
            "   üìä scen4_6rep3: 61 CSV files\n",
            "   üìä scen4_7rep1: 61 CSV files\n",
            "   üìä scen4_7rep2: 61 CSV files\n",
            "   üìä scen4_7rep3: 61 CSV files\n",
            "   üìä scen4_8rep1: 61 CSV files\n",
            "   üìä scen4_8rep2: 61 CSV files\n",
            "   üìä scen4_8rep3: 61 CSV files\n",
            "   üìä scen4_9rep1: 61 CSV files\n",
            "   üìä scen4_9rep2: 61 CSV files\n",
            "   üìä scen4_9rep3: 61 CSV files\n",
            "   üìä scen5_2rep1: 61 CSV files\n",
            "   üìä scen5_2rep2: 61 CSV files\n",
            "   üìä scen5_2rep3: 61 CSV files\n",
            "   üìä scen5_3rep1: 61 CSV files\n",
            "   üìä scen5_3rep2: 61 CSV files\n",
            "   üìä scen5_3rep3: 61 CSV files\n",
            "   üìä scen5_4rep1: 61 CSV files\n",
            "   üìä scen5_4rep2: 61 CSV files\n",
            "   üìä scen5_4rep3: 61 CSV files\n",
            "   üìä scen5_5rep1: 61 CSV files\n",
            "   üìä scen5_5rep2: 61 CSV files\n",
            "   üìä scen5_5rep3: 61 CSV files\n",
            "   üìä scen5_6rep1: 61 CSV files\n",
            "   üìä scen5_6rep2: 61 CSV files\n",
            "   üìä scen5_6rep3: 61 CSV files\n",
            "   üìä scen5_7rep1: 61 CSV files\n",
            "   üìä scen5_7rep2: 61 CSV files\n",
            "   üìä scen5_7rep3: 61 CSV files\n",
            "   üìä scen5_8rep1: 61 CSV files\n",
            "   üìä scen5_8rep2: 61 CSV files\n",
            "   üìä scen5_8rep3: 61 CSV files\n",
            "   üìä scen5_9rep1: 61 CSV files\n",
            "   üìä scen5_9rep2: 61 CSV files\n",
            "   üìä scen5_9rep3: 61 CSV files\n",
            "   üìä scen6_1rep1: 61 CSV files\n",
            "   üìä scen6_1rep2: 61 CSV files\n",
            "   üìä scen6_1rep3: 61 CSV files\n",
            "   üìä scen6_2rep1: 61 CSV files\n",
            "   üìä scen6_2rep2: 61 CSV files\n",
            "   üìä scen6_2rep3: 61 CSV files\n",
            "   üìä scen6_3rep1: 61 CSV files\n",
            "   üìä scen6_3rep2: 61 CSV files\n",
            "   üìä scen6_3rep3: 61 CSV files\n",
            "   üìä scen6_4rep1: 61 CSV files\n",
            "   üìä scen6_4rep2: 61 CSV files\n",
            "   üìä scen6_4rep3: 61 CSV files\n",
            "   üìä scen6_5rep1: 61 CSV files\n",
            "   üìä scen6_5rep2: 61 CSV files\n",
            "   üìä scen6_5rep3: 61 CSV files\n",
            "   üìä scen6_6rep1: 61 CSV files\n",
            "   üìä scen6_6rep2: 61 CSV files\n",
            "   üìä scen6_6rep3: 61 CSV files\n",
            "   üìä scen6_7rep1: 61 CSV files\n",
            "   üìä scen6_7rep2: 61 CSV files\n",
            "   üìä scen6_7rep3: 61 CSV files\n",
            "   üìä scen6_8rep1: 61 CSV files\n",
            "   üìä scen6_8rep2: 61 CSV files\n",
            "   üìä scen6_8rep3: 61 CSV files\n",
            "   üìä scen7_1rep1: 61 CSV files\n",
            "   üìä scen7_1rep2: 61 CSV files\n",
            "   üìä scen7_1rep3: 61 CSV files\n",
            "   üìä scen7_2rep1: 61 CSV files\n",
            "   üìä scen7_2rep2: 61 CSV files\n",
            "   üìä scen7_2rep3: 61 CSV files\n",
            "   üìä scen7_3rep1: 61 CSV files\n",
            "   üìä scen7_3rep2: 61 CSV files\n",
            "   üìä scen7_3rep3: 61 CSV files\n",
            "   üìä scen7_4rep1: 61 CSV files\n",
            "   üìä scen7_4rep2: 61 CSV files\n",
            "   üìä scen7_4rep3: 61 CSV files\n",
            "   üìä scen7_5rep1: 61 CSV files\n",
            "   üìä scen7_5rep2: 61 CSV files\n",
            "   üìä scen7_5rep3: 61 CSV files\n",
            "   üìä scen7_6rep1: 61 CSV files\n",
            "   üìä scen7_6rep2: 61 CSV files\n",
            "   üìä scen7_6rep3: 61 CSV files\n",
            "   üìä scen7_7rep1: 61 CSV files\n",
            "   üìä scen7_7rep2: 61 CSV files\n",
            "   üìä scen7_7rep3: 61 CSV files\n",
            "   üìä scen7_8rep1: 61 CSV files\n",
            "   üìä scen7_8rep2: 61 CSV files\n",
            "   üìä scen7_8rep3: 61 CSV files\n",
            "   üìä scen8_2rep1: 61 CSV files\n",
            "   üìä scen8_2rep2: 61 CSV files\n",
            "   üìä scen8_2rep3: 61 CSV files\n",
            "   üìä scen8_3rep1: 61 CSV files\n",
            "   üìä scen8_3rep2: 61 CSV files\n",
            "   üìä scen8_3rep3: 61 CSV files\n",
            "   üìä scen8_4rep1: 61 CSV files\n",
            "   üìä scen8_4rep2: 61 CSV files\n",
            "   üìä scen8_4rep3: 61 CSV files\n",
            "   üìä scen8_5rep1: 61 CSV files\n",
            "   üìä scen8_5rep2: 61 CSV files\n",
            "   üìä scen8_5rep3: 61 CSV files\n",
            "   üìä scen8_6rep1: 61 CSV files\n",
            "   üìä scen8_6rep2: 61 CSV files\n",
            "   üìä scen8_6rep3: 61 CSV files\n",
            "   üìä scen8_7rep1: 61 CSV files\n",
            "   üìä scen8_7rep2: 61 CSV files\n",
            "   üìä scen8_7rep3: 61 CSV files\n",
            "   üìä scen8_8rep1: 61 CSV files\n",
            "   üìä scen8_8rep2: 61 CSV files\n",
            "   üìä scen8_8rep3: 61 CSV files\n",
            "   üìä scen9_1rep1: 61 CSV files\n",
            "   üìä scen9_1rep2: 61 CSV files\n",
            "   üìä scen9_1rep3: 61 CSV files\n",
            "   üìä scen9_2rep1: 61 CSV files\n",
            "   üìä scen9_2rep2: 61 CSV files\n",
            "   üìä scen9_2rep3: 61 CSV files\n",
            "   üìä scen9_3rep1: 61 CSV files\n",
            "   üìä scen9_3rep2: 61 CSV files\n",
            "   üìä scen9_3rep3: 61 CSV files\n",
            "   üìä scen9_4rep1: 61 CSV files\n",
            "   üìä scen9_4rep2: 61 CSV files\n",
            "   üìä scen9_4rep3: 61 CSV files\n",
            "   üìä scen9_5rep1: 61 CSV files\n",
            "   üìä scen9_5rep2: 61 CSV files\n",
            "   üìä scen9_5rep3: 61 CSV files\n",
            "   üìä scen9_6rep1: 61 CSV files\n",
            "   üìä scen9_6rep2: 61 CSV files\n",
            "   üìä scen9_6rep3: 61 CSV files\n",
            "   üìä scen9_7rep1: 61 CSV files\n",
            "   üìä scen9_7rep2: 61 CSV files\n",
            "   üìä scen9_7rep3: 61 CSV files\n",
            "\n",
            "üìä CSV files by scenario (rep1/rep2/rep3 only):\n",
            "   scen10_1rep1: 61 files\n",
            "   scen10_1rep2: 61 files\n",
            "   scen10_1rep3: 61 files\n",
            "   scen10_2rep1: 61 files\n",
            "   scen10_2rep2: 61 files\n",
            "   scen10_2rep3: 61 files\n",
            "   scen10_3rep1: 61 files\n",
            "   scen10_3rep2: 61 files\n",
            "   scen10_3rep3: 61 files\n",
            "   scen10_4rep1: 61 files\n",
            "   scen10_4rep2: 61 files\n",
            "   scen10_4rep3: 61 files\n",
            "   scen10_5rep1: 61 files\n",
            "   scen10_5rep2: 61 files\n",
            "   scen10_5rep3: 61 files\n",
            "   scen10_6rep1: 61 files\n",
            "   scen10_6rep2: 61 files\n",
            "   scen10_6rep3: 61 files\n",
            "   scen10_7rep1: 61 files\n",
            "   scen10_7rep2: 61 files\n",
            "   scen10_7rep3: 61 files\n",
            "   scen11_2rep1: 61 files\n",
            "   scen11_2rep2: 61 files\n",
            "   scen11_2rep3: 61 files\n",
            "   scen11_3rep1: 61 files\n",
            "   scen11_3rep2: 61 files\n",
            "   scen11_3rep3: 61 files\n",
            "   scen11_4rep1: 61 files\n",
            "   scen11_4rep2: 61 files\n",
            "   scen11_4rep3: 61 files\n",
            "   scen11_5rep1: 61 files\n",
            "   scen11_5rep2: 61 files\n",
            "   scen11_5rep3: 61 files\n",
            "   scen11_6rep1: 61 files\n",
            "   scen11_6rep2: 61 files\n",
            "   scen11_6rep3: 61 files\n",
            "   scen11_7rep1: 61 files\n",
            "   scen11_7rep2: 61 files\n",
            "   scen11_7rep3: 61 files\n",
            "   scen12_1rep1: 61 files\n",
            "   scen12_1rep2: 61 files\n",
            "   scen12_1rep3: 61 files\n",
            "   scen12_2rep1: 61 files\n",
            "   scen12_2rep2: 61 files\n",
            "   scen12_2rep3: 61 files\n",
            "   scen12_3rep1: 61 files\n",
            "   scen12_3rep2: 61 files\n",
            "   scen12_3rep3: 61 files\n",
            "   scen12_4rep1: 61 files\n",
            "   scen12_4rep2: 61 files\n",
            "   scen12_4rep3: 61 files\n",
            "   scen12_5rep1: 61 files\n",
            "   scen12_5rep2: 61 files\n",
            "   scen12_5rep3: 61 files\n",
            "   scen12_6rep1: 61 files\n",
            "   scen12_6rep2: 61 files\n",
            "   scen12_6rep3: 61 files\n",
            "   scen13_1rep1: 61 files\n",
            "   scen13_1rep2: 61 files\n",
            "   scen13_1rep3: 61 files\n",
            "   scen13_2rep1: 61 files\n",
            "   scen13_2rep2: 61 files\n",
            "   scen13_2rep3: 61 files\n",
            "   scen13_3rep1: 61 files\n",
            "   scen13_3rep2: 61 files\n",
            "   scen13_3rep3: 61 files\n",
            "   scen13_4rep1: 61 files\n",
            "   scen13_4rep2: 61 files\n",
            "   scen13_4rep3: 61 files\n",
            "   scen13_5rep1: 61 files\n",
            "   scen13_5rep2: 61 files\n",
            "   scen13_5rep3: 61 files\n",
            "   scen13_6rep1: 61 files\n",
            "   scen13_6rep2: 61 files\n",
            "   scen13_6rep3: 61 files\n",
            "   scen14_2rep1: 61 files\n",
            "   scen14_2rep2: 61 files\n",
            "   scen14_2rep3: 61 files\n",
            "   scen14_3rep1: 61 files\n",
            "   scen14_3rep2: 61 files\n",
            "   scen14_3rep3: 61 files\n",
            "   scen14_4rep1: 61 files\n",
            "   scen14_4rep2: 61 files\n",
            "   scen14_4rep3: 61 files\n",
            "   scen14_5rep1: 61 files\n",
            "   scen14_5rep2: 61 files\n",
            "   scen14_5rep3: 61 files\n",
            "   scen14_6rep1: 61 files\n",
            "   scen14_6rep2: 61 files\n",
            "   scen14_6rep3: 61 files\n",
            "   scen15_1rep1: 61 files\n",
            "   scen15_1rep2: 61 files\n",
            "   scen15_1rep3: 61 files\n",
            "   scen15_2rep1: 61 files\n",
            "   scen15_2rep2: 61 files\n",
            "   scen15_2rep3: 61 files\n",
            "   scen15_3rep1: 61 files\n",
            "   scen15_3rep2: 61 files\n",
            "   scen15_3rep3: 61 files\n",
            "   scen15_4rep1: 61 files\n",
            "   scen15_4rep2: 61 files\n",
            "   scen15_4rep3: 61 files\n",
            "   scen15_5rep1: 61 files\n",
            "   scen15_5rep2: 61 files\n",
            "   scen15_5rep3: 61 files\n",
            "   scen16_1rep1: 61 files\n",
            "   scen16_1rep2: 61 files\n",
            "   scen16_1rep3: 61 files\n",
            "   scen16_2rep1: 61 files\n",
            "   scen16_2rep2: 61 files\n",
            "   scen16_2rep3: 61 files\n",
            "   scen16_3rep1: 61 files\n",
            "   scen16_3rep2: 61 files\n",
            "   scen16_3rep3: 61 files\n",
            "   scen16_4rep1: 61 files\n",
            "   scen16_4rep2: 61 files\n",
            "   scen16_4rep3: 61 files\n",
            "   scen16_5rep1: 61 files\n",
            "   scen16_5rep2: 61 files\n",
            "   scen16_5rep3: 61 files\n",
            "   scen17_2rep1: 61 files\n",
            "   scen17_2rep2: 61 files\n",
            "   scen17_2rep3: 61 files\n",
            "   scen17_3rep1: 61 files\n",
            "   scen17_3rep2: 61 files\n",
            "   scen17_3rep3: 61 files\n",
            "   scen17_4rep1: 61 files\n",
            "   scen17_4rep2: 61 files\n",
            "   scen17_4rep3: 61 files\n",
            "   scen17_5rep1: 61 files\n",
            "   scen17_5rep2: 61 files\n",
            "   scen17_5rep3: 61 files\n",
            "   scen18_1rep1: 61 files\n",
            "   scen18_1rep2: 61 files\n",
            "   scen18_1rep3: 61 files\n",
            "   scen18_2rep1: 61 files\n",
            "   scen18_2rep2: 61 files\n",
            "   scen18_2rep3: 61 files\n",
            "   scen18_3rep1: 61 files\n",
            "   scen18_3rep2: 61 files\n",
            "   scen18_3rep3: 61 files\n",
            "   scen18_4rep1: 61 files\n",
            "   scen18_4rep2: 61 files\n",
            "   scen18_4rep3: 61 files\n",
            "   scen19_1rep1: 61 files\n",
            "   scen19_1rep2: 61 files\n",
            "   scen19_1rep3: 61 files\n",
            "   scen19_2rep1: 61 files\n",
            "   scen19_2rep2: 61 files\n",
            "   scen19_2rep3: 61 files\n",
            "   scen19_3rep1: 61 files\n",
            "   scen19_3rep2: 61 files\n",
            "   scen19_3rep3: 61 files\n",
            "   scen19_4rep1: 61 files\n",
            "   scen19_4rep2: 61 files\n",
            "   scen19_4rep3: 61 files\n",
            "   scen19_5rep1: 61 files\n",
            "   scen19_5rep2: 61 files\n",
            "   scen19_5rep3: 61 files\n",
            "   scen1_10rep1: 61 files\n",
            "   scen1_10rep2: 61 files\n",
            "   scen1_10rep3: 61 files\n",
            "   scen1_1rep1: 61 files\n",
            "   scen1_1rep2: 61 files\n",
            "   scen1_1rep3: 61 files\n",
            "   scen1_2rep1: 61 files\n",
            "   scen1_2rep2: 61 files\n",
            "   scen1_2rep3: 61 files\n",
            "   scen1_3rep1: 61 files\n",
            "   scen1_3rep2: 61 files\n",
            "   scen1_3rep3: 61 files\n",
            "   scen1_4rep1: 61 files\n",
            "   scen1_4rep2: 61 files\n",
            "   scen1_4rep3: 61 files\n",
            "   scen1_5rep1: 61 files\n",
            "   scen1_5rep2: 61 files\n",
            "   scen1_5rep3: 61 files\n",
            "   scen1_6rep1: 61 files\n",
            "   scen1_6rep2: 61 files\n",
            "   scen1_6rep3: 61 files\n",
            "   scen1_7rep1: 61 files\n",
            "   scen1_7rep2: 61 files\n",
            "   scen1_7rep3: 61 files\n",
            "   scen1_8rep1: 61 files\n",
            "   scen1_8rep2: 61 files\n",
            "   scen1_8rep3: 61 files\n",
            "   scen1_9rep1: 61 files\n",
            "   scen1_9rep2: 61 files\n",
            "   scen1_9rep3: 61 files\n",
            "   scen20_2rep1: 61 files\n",
            "   scen20_2rep2: 61 files\n",
            "   scen20_2rep3: 61 files\n",
            "   scen20_3rep1: 61 files\n",
            "   scen20_3rep2: 61 files\n",
            "   scen20_3rep3: 61 files\n",
            "   scen20_4rep1: 61 files\n",
            "   scen20_4rep2: 61 files\n",
            "   scen20_4rep3: 61 files\n",
            "   scen20_5rep1: 61 files\n",
            "   scen20_5rep2: 61 files\n",
            "   scen20_5rep3: 61 files\n",
            "   scen21_1rep1: 61 files\n",
            "   scen21_1rep2: 61 files\n",
            "   scen21_1rep3: 61 files\n",
            "   scen21_2rep1: 61 files\n",
            "   scen21_2rep2: 61 files\n",
            "   scen21_2rep3: 61 files\n",
            "   scen21_3rep1: 61 files\n",
            "   scen21_3rep2: 61 files\n",
            "   scen21_3rep3: 61 files\n",
            "   scen21_4rep1: 61 files\n",
            "   scen21_4rep2: 61 files\n",
            "   scen21_4rep3: 61 files\n",
            "   scen2_10rep1: 61 files\n",
            "   scen2_10rep2: 61 files\n",
            "   scen2_10rep3: 61 files\n",
            "   scen2_2rep1: 61 files\n",
            "   scen2_2rep2: 61 files\n",
            "   scen2_2rep3: 61 files\n",
            "   scen2_3rep1: 61 files\n",
            "   scen2_3rep2: 61 files\n",
            "   scen2_3rep3: 61 files\n",
            "   scen2_4rep1: 61 files\n",
            "   scen2_4rep2: 61 files\n",
            "   scen2_4rep3: 61 files\n",
            "   scen2_5rep1: 61 files\n",
            "   scen2_5rep2: 61 files\n",
            "   scen2_5rep3: 61 files\n",
            "   scen2_6rep1: 61 files\n",
            "   scen2_6rep2: 61 files\n",
            "   scen2_6rep3: 61 files\n",
            "   scen2_7rep1: 61 files\n",
            "   scen2_7rep2: 61 files\n",
            "   scen2_7rep3: 61 files\n",
            "   scen2_8rep1: 61 files\n",
            "   scen2_8rep2: 61 files\n",
            "   scen2_8rep3: 61 files\n",
            "   scen2_9rep1: 61 files\n",
            "   scen2_9rep2: 61 files\n",
            "   scen2_9rep3: 61 files\n",
            "   scen3_1rep1: 61 files\n",
            "   scen3_1rep2: 61 files\n",
            "   scen3_1rep3: 61 files\n",
            "   scen3_2rep1: 61 files\n",
            "   scen3_2rep2: 61 files\n",
            "   scen3_2rep3: 61 files\n",
            "   scen3_3rep1: 61 files\n",
            "   scen3_3rep2: 61 files\n",
            "   scen3_3rep3: 61 files\n",
            "   scen3_4rep1: 61 files\n",
            "   scen3_4rep2: 61 files\n",
            "   scen3_4rep3: 61 files\n",
            "   scen3_5rep1: 61 files\n",
            "   scen3_5rep2: 61 files\n",
            "   scen3_5rep3: 61 files\n",
            "   scen3_6rep1: 61 files\n",
            "   scen3_6rep2: 61 files\n",
            "   scen3_6rep3: 61 files\n",
            "   scen3_7rep1: 61 files\n",
            "   scen3_7rep2: 61 files\n",
            "   scen3_7rep3: 61 files\n",
            "   scen3_8rep1: 61 files\n",
            "   scen3_8rep2: 61 files\n",
            "   scen3_8rep3: 61 files\n",
            "   scen3_9rep1: 61 files\n",
            "   scen3_9rep2: 61 files\n",
            "   scen3_9rep3: 61 files\n",
            "   scen4_1rep1: 61 files\n",
            "   scen4_1rep2: 61 files\n",
            "   scen4_1rep3: 61 files\n",
            "   scen4_2rep1: 61 files\n",
            "   scen4_2rep2: 61 files\n",
            "   scen4_2rep3: 61 files\n",
            "   scen4_3rep1: 61 files\n",
            "   scen4_3rep2: 61 files\n",
            "   scen4_3rep3: 61 files\n",
            "   scen4_4rep1: 61 files\n",
            "   scen4_4rep2: 61 files\n",
            "   scen4_4rep3: 61 files\n",
            "   scen4_5rep1: 61 files\n",
            "   scen4_5rep2: 61 files\n",
            "   scen4_5rep3: 61 files\n",
            "   scen4_6rep1: 61 files\n",
            "   scen4_6rep2: 61 files\n",
            "   scen4_6rep3: 61 files\n",
            "   scen4_7rep1: 61 files\n",
            "   scen4_7rep2: 61 files\n",
            "   scen4_7rep3: 61 files\n",
            "   scen4_8rep1: 61 files\n",
            "   scen4_8rep2: 61 files\n",
            "   scen4_8rep3: 61 files\n",
            "   scen4_9rep1: 61 files\n",
            "   scen4_9rep2: 61 files\n",
            "   scen4_9rep3: 61 files\n",
            "   scen5_2rep1: 61 files\n",
            "   scen5_2rep2: 61 files\n",
            "   scen5_2rep3: 61 files\n",
            "   scen5_3rep1: 61 files\n",
            "   scen5_3rep2: 61 files\n",
            "   scen5_3rep3: 61 files\n",
            "   scen5_4rep1: 61 files\n",
            "   scen5_4rep2: 61 files\n",
            "   scen5_4rep3: 61 files\n",
            "   scen5_5rep1: 61 files\n",
            "   scen5_5rep2: 61 files\n",
            "   scen5_5rep3: 61 files\n",
            "   scen5_6rep1: 61 files\n",
            "   scen5_6rep2: 61 files\n",
            "   scen5_6rep3: 61 files\n",
            "   scen5_7rep1: 61 files\n",
            "   scen5_7rep2: 61 files\n",
            "   scen5_7rep3: 61 files\n",
            "   scen5_8rep1: 61 files\n",
            "   scen5_8rep2: 61 files\n",
            "   scen5_8rep3: 61 files\n",
            "   scen5_9rep1: 61 files\n",
            "   scen5_9rep2: 61 files\n",
            "   scen5_9rep3: 61 files\n",
            "   scen6_1rep1: 61 files\n",
            "   scen6_1rep2: 61 files\n",
            "   scen6_1rep3: 61 files\n",
            "   scen6_2rep1: 61 files\n",
            "   scen6_2rep2: 61 files\n",
            "   scen6_2rep3: 61 files\n",
            "   scen6_3rep1: 61 files\n",
            "   scen6_3rep2: 61 files\n",
            "   scen6_3rep3: 61 files\n",
            "   scen6_4rep1: 61 files\n",
            "   scen6_4rep2: 61 files\n",
            "   scen6_4rep3: 61 files\n",
            "   scen6_5rep1: 61 files\n",
            "   scen6_5rep2: 61 files\n",
            "   scen6_5rep3: 61 files\n",
            "   scen6_6rep1: 61 files\n",
            "   scen6_6rep2: 61 files\n",
            "   scen6_6rep3: 61 files\n",
            "   scen6_7rep1: 61 files\n",
            "   scen6_7rep2: 61 files\n",
            "   scen6_7rep3: 61 files\n",
            "   scen6_8rep1: 61 files\n",
            "   scen6_8rep2: 61 files\n",
            "   scen6_8rep3: 61 files\n",
            "   scen7_1rep1: 61 files\n",
            "   scen7_1rep2: 61 files\n",
            "   scen7_1rep3: 61 files\n",
            "   scen7_2rep1: 61 files\n",
            "   scen7_2rep2: 61 files\n",
            "   scen7_2rep3: 61 files\n",
            "   scen7_3rep1: 61 files\n",
            "   scen7_3rep2: 61 files\n",
            "   scen7_3rep3: 61 files\n",
            "   scen7_4rep1: 61 files\n",
            "   scen7_4rep2: 61 files\n",
            "   scen7_4rep3: 61 files\n",
            "   scen7_5rep1: 61 files\n",
            "   scen7_5rep2: 61 files\n",
            "   scen7_5rep3: 61 files\n",
            "   scen7_6rep1: 61 files\n",
            "   scen7_6rep2: 61 files\n",
            "   scen7_6rep3: 61 files\n",
            "   scen7_7rep1: 61 files\n",
            "   scen7_7rep2: 61 files\n",
            "   scen7_7rep3: 61 files\n",
            "   scen7_8rep1: 61 files\n",
            "   scen7_8rep2: 61 files\n",
            "   scen7_8rep3: 61 files\n",
            "   scen8_2rep1: 61 files\n",
            "   scen8_2rep2: 61 files\n",
            "   scen8_2rep3: 61 files\n",
            "   scen8_3rep1: 61 files\n",
            "   scen8_3rep2: 61 files\n",
            "   scen8_3rep3: 61 files\n",
            "   scen8_4rep1: 61 files\n",
            "   scen8_4rep2: 61 files\n",
            "   scen8_4rep3: 61 files\n",
            "   scen8_5rep1: 61 files\n",
            "   scen8_5rep2: 61 files\n",
            "   scen8_5rep3: 61 files\n",
            "   scen8_6rep1: 61 files\n",
            "   scen8_6rep2: 61 files\n",
            "   scen8_6rep3: 61 files\n",
            "   scen8_7rep1: 61 files\n",
            "   scen8_7rep2: 61 files\n",
            "   scen8_7rep3: 61 files\n",
            "   scen8_8rep1: 61 files\n",
            "   scen8_8rep2: 61 files\n",
            "   scen8_8rep3: 61 files\n",
            "   scen9_1rep1: 61 files\n",
            "   scen9_1rep2: 61 files\n",
            "   scen9_1rep3: 61 files\n",
            "   scen9_2rep1: 61 files\n",
            "   scen9_2rep2: 61 files\n",
            "   scen9_2rep3: 61 files\n",
            "   scen9_3rep1: 61 files\n",
            "   scen9_3rep2: 61 files\n",
            "   scen9_3rep3: 61 files\n",
            "   scen9_4rep1: 61 files\n",
            "   scen9_4rep2: 61 files\n",
            "   scen9_4rep3: 61 files\n",
            "   scen9_5rep1: 61 files\n",
            "   scen9_5rep2: 61 files\n",
            "   scen9_5rep3: 61 files\n",
            "   scen9_6rep1: 61 files\n",
            "   scen9_6rep2: 61 files\n",
            "   scen9_6rep3: 61 files\n",
            "   scen9_7rep1: 61 files\n",
            "   scen9_7rep2: 61 files\n",
            "   scen9_7rep3: 61 files\n",
            "\n",
            "‚úÖ Filtered discovery complete:\n",
            "   Total CSV files: 24,888\n",
            "   Only rep1/rep2/rep3: ‚úÖ\n",
            "   Discovery time: 224.9s\n",
            "üéØ Will process 24,888 files with rate limiting\n",
            "\n",
            "üì° Step 2: Rate-limited processing...\n",
            "üì° RATE-LIMITED PROCESSING 24,888 files\n",
            "Workers: 4 (API-safe)\n",
            "============================================================\n",
            "üîÑ Checking for previous checkpoints...\n",
            "üìö Checkpoint loaded!\n",
            "   Files processed: 8,000\n",
            "   Samples: 50,000\n",
            "   Last batch: 16\n",
            "‚úÖ All files already processed!\n",
            "\n",
            "üìä Step 3: Analyzing dataset...\n",
            "RATE-LIMITED DATASET SUMMARY:\n",
            "   Files processed: 24,888\n",
            "   Total vehicles: 50,000\n",
            "   Label distribution:\n",
            "     aggressive: 32,708 (65.4%)\n",
            "     cooperative: 17,292 (34.6%)\n",
            "\n",
            "üîß Step 4: Data preparation...\n",
            "   Processed 0/50,000 samples\n",
            "   Processed 10,000/50,000 samples\n",
            "   Processed 20,000/50,000 samples\n",
            "   Processed 30,000/50,000 samples\n",
            "   Processed 40,000/50,000 samples\n",
            "‚úÖ Data prepared: 50,000 samples\n",
            "Train samples: 40,000\n",
            "Test samples: 10,000\n",
            "Label distribution in all_labels: Counter({'aggressive': 32708, 'cooperative': 17292})\n",
            "Unique labels: {'cooperative', 'aggressive'}\n",
            "Total samples: 50000\n",
            "Classes: ['aggressive' 'cooperative']\n",
            "\n",
            "üöÄ Step 5: Model training...\n",
            "üß† Fixed Model Architecture:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional_2\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_2\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "‚îè‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îì\n",
              "‚îÉ\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m‚îÉ\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m‚îÉ\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m‚îÉ\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m‚îÉ\n",
              "‚î°‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î©\n",
              "‚îÇ speed (\u001b[38;5;33mInputLayer\u001b[0m)  ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m1\u001b[0m)     ‚îÇ          \u001b[38;5;34m0\u001b[0m ‚îÇ -                 ‚îÇ\n",
              "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
              "‚îÇ position            ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m2\u001b[0m)     ‚îÇ          \u001b[38;5;34m0\u001b[0m ‚îÇ -                 ‚îÇ\n",
              "‚îÇ (\u001b[38;5;33mInputLayer\u001b[0m)        ‚îÇ                   ‚îÇ            ‚îÇ                   ‚îÇ\n",
              "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
              "‚îÇ acceleration        ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m1\u001b[0m)     ‚îÇ          \u001b[38;5;34m0\u001b[0m ‚îÇ speed[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       ‚îÇ\n",
              "‚îÇ (\u001b[38;5;33mLambda\u001b[0m)            ‚îÇ                   ‚îÇ            ‚îÇ                   ‚îÇ\n",
              "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
              "‚îÇ lateral (\u001b[38;5;33mLambda\u001b[0m)    ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m1\u001b[0m)     ‚îÇ          \u001b[38;5;34m0\u001b[0m ‚îÇ position[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    ‚îÇ\n",
              "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
              "‚îÇ concatenate_4       ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m3\u001b[0m)     ‚îÇ          \u001b[38;5;34m0\u001b[0m ‚îÇ speed[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],      ‚îÇ\n",
              "‚îÇ (\u001b[38;5;33mConcatenate\u001b[0m)       ‚îÇ                   ‚îÇ            ‚îÇ acceleration[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m‚Ä¶\u001b[0m ‚îÇ\n",
              "‚îÇ                     ‚îÇ                   ‚îÇ            ‚îÇ lateral[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     ‚îÇ\n",
              "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
              "‚îÇ conv1d_4 (\u001b[38;5;33mConv1D\u001b[0m)   ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m64\u001b[0m)    ‚îÇ        \u001b[38;5;34m640\u001b[0m ‚îÇ concatenate_4[\u001b[38;5;34m0\u001b[0m]‚Ä¶ ‚îÇ\n",
              "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
              "‚îÇ batch_normalizatio‚Ä¶ ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m64\u001b[0m)    ‚îÇ        \u001b[38;5;34m256\u001b[0m ‚îÇ conv1d_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    ‚îÇ\n",
              "‚îÇ (\u001b[38;5;33mBatchNormalizatio‚Ä¶\u001b[0m ‚îÇ                   ‚îÇ            ‚îÇ                   ‚îÇ\n",
              "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
              "‚îÇ dropout_4 (\u001b[38;5;33mDropout\u001b[0m) ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m64\u001b[0m)    ‚îÇ          \u001b[38;5;34m0\u001b[0m ‚îÇ batch_normalizat‚Ä¶ ‚îÇ\n",
              "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
              "‚îÇ conv1d_5 (\u001b[38;5;33mConv1D\u001b[0m)   ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m128\u001b[0m)   ‚îÇ     \u001b[38;5;34m24,704\u001b[0m ‚îÇ dropout_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   ‚îÇ\n",
              "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
              "‚îÇ lstm_4 (\u001b[38;5;33mLSTM\u001b[0m)       ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m128\u001b[0m)   ‚îÇ     \u001b[38;5;34m67,584\u001b[0m ‚îÇ concatenate_4[\u001b[38;5;34m0\u001b[0m]‚Ä¶ ‚îÇ\n",
              "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
              "‚îÇ batch_normalizatio‚Ä¶ ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m128\u001b[0m)   ‚îÇ        \u001b[38;5;34m512\u001b[0m ‚îÇ conv1d_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    ‚îÇ\n",
              "‚îÇ (\u001b[38;5;33mBatchNormalizatio‚Ä¶\u001b[0m ‚îÇ                   ‚îÇ            ‚îÇ                   ‚îÇ\n",
              "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
              "‚îÇ batch_normalizatio‚Ä¶ ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m128\u001b[0m)   ‚îÇ        \u001b[38;5;34m512\u001b[0m ‚îÇ lstm_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      ‚îÇ\n",
              "‚îÇ (\u001b[38;5;33mBatchNormalizatio‚Ä¶\u001b[0m ‚îÇ                   ‚îÇ            ‚îÇ                   ‚îÇ\n",
              "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
              "‚îÇ global_max_pooling‚Ä¶ ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       ‚îÇ          \u001b[38;5;34m0\u001b[0m ‚îÇ batch_normalizat‚Ä¶ ‚îÇ\n",
              "‚îÇ (\u001b[38;5;33mGlobalMaxPooling1‚Ä¶\u001b[0m ‚îÇ                   ‚îÇ            ‚îÇ                   ‚îÇ\n",
              "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
              "‚îÇ lstm_5 (\u001b[38;5;33mLSTM\u001b[0m)       ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        ‚îÇ     \u001b[38;5;34m49,408\u001b[0m ‚îÇ batch_normalizat‚Ä¶ ‚îÇ\n",
              "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
              "‚îÇ concatenate_5       ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m192\u001b[0m)       ‚îÇ          \u001b[38;5;34m0\u001b[0m ‚îÇ global_max_pooli‚Ä¶ ‚îÇ\n",
              "‚îÇ (\u001b[38;5;33mConcatenate\u001b[0m)       ‚îÇ                   ‚îÇ            ‚îÇ lstm_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      ‚îÇ\n",
              "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
              "‚îÇ dense_6 (\u001b[38;5;33mDense\u001b[0m)     ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       ‚îÇ     \u001b[38;5;34m49,408\u001b[0m ‚îÇ concatenate_5[\u001b[38;5;34m0\u001b[0m]‚Ä¶ ‚îÇ\n",
              "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
              "‚îÇ batch_normalizatio‚Ä¶ ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       ‚îÇ      \u001b[38;5;34m1,024\u001b[0m ‚îÇ dense_6[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     ‚îÇ\n",
              "‚îÇ (\u001b[38;5;33mBatchNormalizatio‚Ä¶\u001b[0m ‚îÇ                   ‚îÇ            ‚îÇ                   ‚îÇ\n",
              "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
              "‚îÇ dropout_5 (\u001b[38;5;33mDropout\u001b[0m) ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       ‚îÇ          \u001b[38;5;34m0\u001b[0m ‚îÇ batch_normalizat‚Ä¶ ‚îÇ\n",
              "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
              "‚îÇ dense_7 (\u001b[38;5;33mDense\u001b[0m)     ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       ‚îÇ     \u001b[38;5;34m32,896\u001b[0m ‚îÇ dropout_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   ‚îÇ\n",
              "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
              "‚îÇ dense_8 (\u001b[38;5;33mDense\u001b[0m)     ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)         ‚îÇ        \u001b[38;5;34m258\u001b[0m ‚îÇ dense_7[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     ‚îÇ\n",
              "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">‚îè‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îì\n",
              "‚îÉ<span style=\"font-weight: bold\"> Layer (type)        </span>‚îÉ<span style=\"font-weight: bold\"> Output Shape      </span>‚îÉ<span style=\"font-weight: bold\">    Param # </span>‚îÉ<span style=\"font-weight: bold\"> Connected to      </span>‚îÉ\n",
              "‚î°‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î©\n",
              "‚îÇ speed (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)  ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)     ‚îÇ          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> ‚îÇ -                 ‚îÇ\n",
              "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
              "‚îÇ position            ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)     ‚îÇ          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> ‚îÇ -                 ‚îÇ\n",
              "‚îÇ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        ‚îÇ                   ‚îÇ            ‚îÇ                   ‚îÇ\n",
              "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
              "‚îÇ acceleration        ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)     ‚îÇ          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> ‚îÇ speed[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       ‚îÇ\n",
              "‚îÇ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)            ‚îÇ                   ‚îÇ            ‚îÇ                   ‚îÇ\n",
              "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
              "‚îÇ lateral (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)    ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)     ‚îÇ          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> ‚îÇ position[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    ‚îÇ\n",
              "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
              "‚îÇ concatenate_4       ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)     ‚îÇ          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> ‚îÇ speed[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],      ‚îÇ\n",
              "‚îÇ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       ‚îÇ                   ‚îÇ            ‚îÇ acceleration[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">‚Ä¶</span> ‚îÇ\n",
              "‚îÇ                     ‚îÇ                   ‚îÇ            ‚îÇ lateral[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     ‚îÇ\n",
              "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
              "‚îÇ conv1d_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)   ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)    ‚îÇ        <span style=\"color: #00af00; text-decoration-color: #00af00\">640</span> ‚îÇ concatenate_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]‚Ä¶ ‚îÇ\n",
              "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
              "‚îÇ batch_normalizatio‚Ä¶ ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)    ‚îÇ        <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> ‚îÇ conv1d_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    ‚îÇ\n",
              "‚îÇ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio‚Ä¶</span> ‚îÇ                   ‚îÇ            ‚îÇ                   ‚îÇ\n",
              "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
              "‚îÇ dropout_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)    ‚îÇ          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> ‚îÇ batch_normalizat‚Ä¶ ‚îÇ\n",
              "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
              "‚îÇ conv1d_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)   ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)   ‚îÇ     <span style=\"color: #00af00; text-decoration-color: #00af00\">24,704</span> ‚îÇ dropout_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   ‚îÇ\n",
              "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
              "‚îÇ lstm_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)       ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)   ‚îÇ     <span style=\"color: #00af00; text-decoration-color: #00af00\">67,584</span> ‚îÇ concatenate_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]‚Ä¶ ‚îÇ\n",
              "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
              "‚îÇ batch_normalizatio‚Ä¶ ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)   ‚îÇ        <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> ‚îÇ conv1d_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    ‚îÇ\n",
              "‚îÇ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio‚Ä¶</span> ‚îÇ                   ‚îÇ            ‚îÇ                   ‚îÇ\n",
              "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
              "‚îÇ batch_normalizatio‚Ä¶ ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)   ‚îÇ        <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> ‚îÇ lstm_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      ‚îÇ\n",
              "‚îÇ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio‚Ä¶</span> ‚îÇ                   ‚îÇ            ‚îÇ                   ‚îÇ\n",
              "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
              "‚îÇ global_max_pooling‚Ä¶ ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       ‚îÇ          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> ‚îÇ batch_normalizat‚Ä¶ ‚îÇ\n",
              "‚îÇ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalMaxPooling1‚Ä¶</span> ‚îÇ                   ‚îÇ            ‚îÇ                   ‚îÇ\n",
              "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
              "‚îÇ lstm_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)       ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        ‚îÇ     <span style=\"color: #00af00; text-decoration-color: #00af00\">49,408</span> ‚îÇ batch_normalizat‚Ä¶ ‚îÇ\n",
              "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
              "‚îÇ concatenate_5       ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>)       ‚îÇ          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> ‚îÇ global_max_pooli‚Ä¶ ‚îÇ\n",
              "‚îÇ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       ‚îÇ                   ‚îÇ            ‚îÇ lstm_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      ‚îÇ\n",
              "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
              "‚îÇ dense_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       ‚îÇ     <span style=\"color: #00af00; text-decoration-color: #00af00\">49,408</span> ‚îÇ concatenate_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]‚Ä¶ ‚îÇ\n",
              "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
              "‚îÇ batch_normalizatio‚Ä¶ ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       ‚îÇ      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> ‚îÇ dense_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     ‚îÇ\n",
              "‚îÇ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio‚Ä¶</span> ‚îÇ                   ‚îÇ            ‚îÇ                   ‚îÇ\n",
              "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
              "‚îÇ dropout_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       ‚îÇ          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> ‚îÇ batch_normalizat‚Ä¶ ‚îÇ\n",
              "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
              "‚îÇ dense_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       ‚îÇ     <span style=\"color: #00af00; text-decoration-color: #00af00\">32,896</span> ‚îÇ dropout_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   ‚îÇ\n",
              "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
              "‚îÇ dense_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)         ‚îÇ        <span style=\"color: #00af00; text-decoration-color: #00af00\">258</span> ‚îÇ dense_7[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     ‚îÇ\n",
              "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m227,202\u001b[0m (887.51 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">227,202</span> (887.51 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m226,050\u001b[0m (883.01 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">226,050</span> (883.01 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m1,152\u001b[0m (4.50 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,152</span> (4.50 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training with rate-limited dataset...\n",
            "Training samples: 40,000\n",
            "Epoch 1/20\n",
            "\u001b[1m133/133\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 68ms/step - accuracy: 0.6714 - loss: 0.7032 - val_accuracy: 0.4883 - val_loss: 0.7472 - learning_rate: 0.0010\n",
            "Epoch 2/20\n",
            "\u001b[1m133/133\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - accuracy: 0.7131 - loss: 0.6099 - val_accuracy: 0.5175 - val_loss: 0.7290 - learning_rate: 0.0010\n",
            "Epoch 3/20\n",
            "\u001b[1m133/133\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - accuracy: 0.7229 - loss: 0.5888 - val_accuracy: 0.5318 - val_loss: 0.7673 - learning_rate: 0.0010\n",
            "Epoch 4/20\n",
            "\u001b[1m133/133\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - accuracy: 0.7330 - loss: 0.5744 - val_accuracy: 0.5347 - val_loss: 0.7629 - learning_rate: 0.0010\n",
            "Epoch 5/20\n",
            "\u001b[1m133/133\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - accuracy: 0.7357 - loss: 0.5718 - val_accuracy: 0.5360 - val_loss: 0.7784 - learning_rate: 0.0010\n",
            "Epoch 6/20\n",
            "\u001b[1m133/133\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - accuracy: 0.7443 - loss: 0.5634 - val_accuracy: 0.5485 - val_loss: 0.7517 - learning_rate: 5.0000e-04\n",
            "Epoch 7/20\n",
            "\u001b[1m133/133\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - accuracy: 0.7446 - loss: 0.5588 - val_accuracy: 0.5465 - val_loss: 0.7646 - learning_rate: 5.0000e-04\n",
            "‚úÖ Training completed!\n",
            "\n",
            "üìä Evaluation...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:5 out of the last 29 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x7a12782302c0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RATE-LIMITED RESULTS:\n",
            "   Test Accuracy: 0.510\n",
            "   F1 Score (macro): 0.463\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "  aggressive       0.50      0.82      0.62      4912\n",
            " cooperative       0.55      0.21      0.30      5088\n",
            "\n",
            "    accuracy                           0.51     10000\n",
            "   macro avg       0.53      0.52      0.46     10000\n",
            "weighted avg       0.53      0.51      0.46     10000\n",
            "\n",
            "\n",
            "üíæ Saving model...\n",
            "‚ùå Training failed: name 'cl_system' is not defined\n",
            "\n",
            "‚ùå Training failed - run resume_training() to continue\n",
            "\n",
            "üíæ All checkpoints saved to: /content/drive/MyDrive/Training_Checkpoints\n",
            "üì° API-safe trainer - no more 429 errors!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-48-2476193913.py\", line 817, in train_with_rate_limits\n",
            "    cl_system.model = model\n",
            "    ^^^^^^^^^\n",
            "NameError: name 'cl_system' is not defined\n"
          ]
        }
      ]
    }
  ]
}